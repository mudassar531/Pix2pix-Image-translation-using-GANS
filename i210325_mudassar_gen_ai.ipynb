{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c69da",
   "metadata": {
    "id": "f87c69da"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "JfPUvTUnr2wJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfPUvTUnr2wJ",
    "outputId": "9fdc8cdc-a412-4d1f-fb38-9d0c1032ed95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3c316",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcb3c316",
    "outputId": "f06ee867-80e9-438a-e970-71f926103abe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3eb2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19c3eb2b",
    "outputId": "e81b0740-8357-49ec-a75a-2115213d0695"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd418503",
   "metadata": {
    "id": "dd418503"
   },
   "source": [
    "## Queestion 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9WbcrrtqD5J",
   "metadata": {
    "id": "y9WbcrrtqD5J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IFgzcjBCqD8B",
   "metadata": {
    "id": "IFgzcjBCqD8B"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "train_folder = 'person-face-sketches/train'\n",
    "train_sketches_folder = os.path.join(train_folder, 'sketches')\n",
    "train_photos_folder = os.path.join(train_folder, 'photos')\n",
    "\n",
    "test_folder = 'person-face-sketches/test'\n",
    "test_sketches_folder = os.path.join(test_folder, 'sketches')\n",
    "test_photos_folder = os.path.join(test_folder, 'photos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dLkDXmAcqD-j",
   "metadata": {
    "id": "dLkDXmAcqD-j"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, sketches_dir, photos_dir, transform=None):\n",
    "        self.sketches_dir = Path(sketches_dir)\n",
    "        self.photos_dir = Path(photos_dir)\n",
    "        self.sketches = sorted(os.listdir(sketches_dir))\n",
    "        self.photos = sorted(os.listdir(photos_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.sketches), len(self.photos))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch_path = self.sketches_dir / self.sketches[idx % len(self.sketches)]\n",
    "        photo_path = self.photos_dir / self.photos[idx % len(self.photos)]\n",
    "        sketch = Image.open(sketch_path).convert('RGB')\n",
    "        photo = Image.open(photo_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            sketch = self.transform(sketch)\n",
    "            photo = self.transform(photo)\n",
    "\n",
    "        return {'sketch': sketch, 'photo': photo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qk_osTXtqLDS",
   "metadata": {
    "id": "qk_osTXtqLDS"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_dataset = ImageDataset(train_sketches_folder, train_photos_folder, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataset = ImageDataset(test_sketches_folder, test_photos_folder, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v1gTcHXKqLGQ",
   "metadata": {
    "id": "v1gTcHXKqLGQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = nn.Conv2d(filters, filters, kernel_size=3, padding=1)(x)\n",
    "    x = nn.BatchNorm2d(filters)(x)\n",
    "    x = nn.ReLU(inplace=True)(x)\n",
    "    x = nn.Conv2d(filters, filters, kernel_size=3, padding=1)(x)\n",
    "    x = nn.BatchNorm2d(filters)(x)\n",
    "    x = torch.add(x,shortcut)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ngf, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(ngf, ngf * 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 2, ngf * 4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ngf * 4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(*[residual_block(ngf*4) for _ in range(n_blocks)])\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(ngf * 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(ngf, output_nc, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.final(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=3, ndf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, kernel_size=4, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PRLOiwzMqLIH",
   "metadata": {
    "id": "PRLOiwzMqLIH"
   },
   "outputs": [],
   "source": [
    "G_A2B = Generator().to(device)\n",
    "G_B2A = Generator().to(device)\n",
    "D_A = Discriminator().to(device)\n",
    "D_B = Discriminator().to(device)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizer_G = optim.Adam(itertools.chain(G_A2B.parameters(), G_B2A.parameters()), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "n_epochs = 10\n",
    "decay_start_epoch = 100\n",
    "scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda epoch: 1.0)\n",
    "scheduler_D_A = optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda epoch: 1.0)\n",
    "scheduler_D_B = optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda epoch: 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eItNL_uQqECK",
   "metadata": {
    "id": "eItNL_uQqECK"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if np.random.uniform(0, 1) > 0.5:\n",
    "                    idx = np.random.randint(0, self.max_size)\n",
    "                    to_return.append(self.data[idx].clone())\n",
    "                    self.data[idx] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GRqmXWhOqolB",
   "metadata": {
    "id": "GRqmXWhOqolB"
   },
   "outputs": [],
   "source": [
    "fake_A_buffer = ReplayBuffer()\n",
    "fake_B_buffer = ReplayBuffer()\n",
    "\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OFuGE0Zdqons",
   "metadata": {
    "id": "OFuGE0Zdqons"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch):\n",
    "    torch.save({\n",
    "        'G_A2B_state_dict': G_A2B.state_dict(),\n",
    "        'G_B2A_state_dict': G_B2A.state_dict(),\n",
    "        'D_A_state_dict': D_A.state_dict(),\n",
    "        'D_B_state_dict': D_B.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_A_state_dict': optimizer_D_A.state_dict(),\n",
    "        'optimizer_D_B_state_dict': optimizer_D_B.state_dict(),\n",
    "        'scheduler_G_state_dict': scheduler_G.state_dict(),\n",
    "        'scheduler_D_A_state_dict': scheduler_D_A.state_dict(),\n",
    "        'scheduler_D_B_state_dict': scheduler_D_B.state_dict(),\n",
    "    }, f'checkpoint_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49aa9094",
   "metadata": {
    "id": "49aa9094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Batch [0/20655] Loss_D_A: 0.0 Loss_D_B: 0.0 Loss_G: 0.0\n",
      "Epoch [1/10] Batch [0/20655] Loss_D_A: 0.756834089756012 Loss_D_B: 0.5830504894256592 Loss_G: 0.2680385112762451\n",
      "Epoch [1/10] Batch [100/20655] Loss_D_A: 0.8872256278991699 Loss_D_B: 0.6495956182479858 Loss_G: 0.341248095035553\n",
      "Epoch [1/10] Batch [200/20655] Loss_D_A: 0.3654525876045227 Loss_D_B: 0.8645493984222412 Loss_G: 0.15156441926956177\n",
      "Epoch [1/10] Batch [300/20655] Loss_D_A: 0.6661231517791748 Loss_D_B: 0.5995445251464844 Loss_G: 0.3765715956687927\n",
      "Epoch [1/10] Batch [400/20655] Loss_D_A: 0.3590942621231079 Loss_D_B: 0.8491266965866089 Loss_G: 0.583227813243866\n",
      "Epoch [1/10] Batch [500/20655] Loss_D_A: 0.6020272970199585 Loss_D_B: 0.9163699746131897 Loss_G: 0.6042278409004211\n",
      "Epoch [1/10] Batch [600/20655] Loss_D_A: 0.5524541139602661 Loss_D_B: 0.8228917121887207 Loss_G: 0.1271839737892151\n",
      "Epoch [1/10] Batch [700/20655] Loss_D_A: 0.17287975549697876 Loss_D_B: 0.48835867643356323 Loss_G: 0.23472917079925537\n",
      "Epoch [1/10] Batch [800/20655] Loss_D_A: 0.639789879322052 Loss_D_B: 0.6163650751113892 Loss_G: 0.27555447816848755\n",
      "Epoch [1/10] Batch [900/20655] Loss_D_A: 0.7847387194633484 Loss_D_B: 0.4037821292877197 Loss_G: 0.42948561906814575\n",
      "Epoch [1/10] Batch [1000/20655] Loss_D_A: 0.3864824175834656 Loss_D_B: 0.7511886954307556 Loss_G: 0.3878365159034729\n",
      "Epoch [1/10] Batch [1100/20655] Loss_D_A: 0.3995206952095032 Loss_D_B: 0.016122519969940186 Loss_G: 0.7660582065582275\n",
      "Epoch [1/10] Batch [1200/20655] Loss_D_A: 0.8167261481285095 Loss_D_B: 0.8633966445922852 Loss_G: 0.12608742713928223\n",
      "Epoch [1/10] Batch [1300/20655] Loss_D_A: 0.1845199465751648 Loss_D_B: 0.5325173735618591 Loss_G: 0.193831205368042\n",
      "Epoch [1/10] Batch [1400/20655] Loss_D_A: 0.10913950204849243 Loss_D_B: 0.7394603490829468 Loss_G: 0.983719527721405\n",
      "Epoch [1/10] Batch [1500/20655] Loss_D_A: 0.4852457046508789 Loss_D_B: 0.03945201635360718 Loss_G: 0.6837351322174072\n",
      "Epoch [1/10] Batch [1600/20655] Loss_D_A: 0.7677916288375854 Loss_D_B: 0.056893885135650635 Loss_G: 0.7759971618652344\n",
      "Epoch [1/10] Batch [1700/20655] Loss_D_A: 0.5918056964874268 Loss_D_B: 0.6684308648109436 Loss_G: 0.051327288150787354\n",
      "Epoch [1/10] Batch [1800/20655] Loss_D_A: 0.5136544704437256 Loss_D_B: 0.8871914148330688 Loss_G: 0.0650363564491272\n",
      "Epoch [1/10] Batch [1900/20655] Loss_D_A: 0.08315867185592651 Loss_D_B: 0.36913472414016724 Loss_G: 0.9066911339759827\n",
      "Epoch [1/10] Batch [2000/20655] Loss_D_A: 0.47523754835128784 Loss_D_B: 0.6639846563339233 Loss_G: 0.7502068877220154\n",
      "Epoch [1/10] Batch [2100/20655] Loss_D_A: 0.44717246294021606 Loss_D_B: 0.15594899654388428 Loss_G: 0.9904107451438904\n",
      "Epoch [1/10] Batch [2200/20655] Loss_D_A: 0.8027506470680237 Loss_D_B: 0.37601590156555176 Loss_G: 0.9851588606834412\n",
      "Epoch [1/10] Batch [2300/20655] Loss_D_A: 0.08626663684844971 Loss_D_B: 0.08605676889419556 Loss_G: 0.6832984685897827\n",
      "Epoch [1/10] Batch [2400/20655] Loss_D_A: 0.8106611371040344 Loss_D_B: 0.025386691093444824 Loss_G: 0.32170021533966064\n",
      "Epoch [1/10] Batch [2500/20655] Loss_D_A: 0.43096500635147095 Loss_D_B: 0.3466039299964905 Loss_G: 0.996634840965271\n",
      "Epoch [1/10] Batch [2600/20655] Loss_D_A: 0.9592366814613342 Loss_D_B: 0.7971769571304321 Loss_G: 0.9954484105110168\n",
      "Epoch [1/10] Batch [2700/20655] Loss_D_A: 0.7001221179962158 Loss_D_B: 0.7723751664161682 Loss_G: 0.2705304026603699\n",
      "Epoch [1/10] Batch [2800/20655] Loss_D_A: 0.5616567730903625 Loss_D_B: 0.7597090601921082 Loss_G: 0.4585193991661072\n",
      "Epoch [1/10] Batch [2900/20655] Loss_D_A: 0.48569750785827637 Loss_D_B: 0.6899360418319702 Loss_G: 0.2705766558647156\n",
      "Epoch [1/10] Batch [3000/20655] Loss_D_A: 0.9136843681335449 Loss_D_B: 0.5069491267204285 Loss_G: 0.7840218544006348\n",
      "Epoch [1/10] Batch [3100/20655] Loss_D_A: 0.744563102722168 Loss_D_B: 0.18833953142166138 Loss_G: 0.7179012894630432\n",
      "Epoch [1/10] Batch [3200/20655] Loss_D_A: 0.6957194209098816 Loss_D_B: 0.3712841868400574 Loss_G: 0.5894530415534973\n",
      "Epoch [1/10] Batch [3300/20655] Loss_D_A: 0.32775336503982544 Loss_D_B: 0.4245729446411133 Loss_G: 0.012010633945465088\n",
      "Epoch [1/10] Batch [3400/20655] Loss_D_A: 0.0689697265625 Loss_D_B: 0.32813453674316406 Loss_G: 0.6043945550918579\n",
      "Epoch [1/10] Batch [3500/20655] Loss_D_A: 0.5363945960998535 Loss_D_B: 0.1415145993232727 Loss_G: 0.525687575340271\n",
      "Epoch [1/10] Batch [3600/20655] Loss_D_A: 0.09317612648010254 Loss_D_B: 0.23351776599884033 Loss_G: 0.5166327357292175\n",
      "Epoch [1/10] Batch [3700/20655] Loss_D_A: 0.2605324387550354 Loss_D_B: 0.0535581111907959 Loss_G: 0.9333349466323853\n",
      "Epoch [1/10] Batch [3800/20655] Loss_D_A: 0.5608993768692017 Loss_D_B: 0.5120533108711243 Loss_G: 0.21567291021347046\n",
      "Epoch [1/10] Batch [3900/20655] Loss_D_A: 0.4487738013267517 Loss_D_B: 0.5038536190986633 Loss_G: 0.5997254252433777\n",
      "Epoch [1/10] Batch [4000/20655] Loss_D_A: 0.7978784441947937 Loss_D_B: 0.41622787714004517 Loss_G: 0.8522687554359436\n",
      "Epoch [1/10] Batch [4100/20655] Loss_D_A: 0.8692455887794495 Loss_D_B: 0.34138017892837524 Loss_G: 0.8221758604049683\n",
      "Epoch [1/10] Batch [4200/20655] Loss_D_A: 0.7505428791046143 Loss_D_B: 0.5618951320648193 Loss_G: 0.6294613480567932\n",
      "Epoch [1/10] Batch [4300/20655] Loss_D_A: 0.042976438999176025 Loss_D_B: 0.8597564101219177 Loss_G: 0.7265751957893372\n",
      "Epoch [1/10] Batch [4400/20655] Loss_D_A: 0.8728967905044556 Loss_D_B: 0.7228077054023743 Loss_G: 0.4363129734992981\n",
      "Epoch [1/10] Batch [4500/20655] Loss_D_A: 0.21179771423339844 Loss_D_B: 0.3145309090614319 Loss_G: 0.08141547441482544\n",
      "Epoch [1/10] Batch [4600/20655] Loss_D_A: 0.8769904971122742 Loss_D_B: 0.43436139822006226 Loss_G: 0.26012927293777466\n",
      "Epoch [1/10] Batch [4700/20655] Loss_D_A: 0.14665192365646362 Loss_D_B: 0.03983432054519653 Loss_G: 0.1024520993232727\n",
      "Epoch [1/10] Batch [4800/20655] Loss_D_A: 0.13758248090744019 Loss_D_B: 0.25614744424819946 Loss_G: 0.4700496196746826\n",
      "Epoch [1/10] Batch [4900/20655] Loss_D_A: 0.33177125453948975 Loss_D_B: 0.6419126391410828 Loss_G: 0.016442716121673584\n",
      "Epoch [1/10] Batch [5000/20655] Loss_D_A: 0.5593390464782715 Loss_D_B: 0.8684679865837097 Loss_G: 0.9919889569282532\n",
      "Epoch [1/10] Batch [5100/20655] Loss_D_A: 0.7252662181854248 Loss_D_B: 0.2809011936187744 Loss_G: 0.37608468532562256\n",
      "Epoch [1/10] Batch [5200/20655] Loss_D_A: 0.5581615567207336 Loss_D_B: 0.4880363941192627 Loss_G: 0.49280500411987305\n",
      "Epoch [1/10] Batch [5300/20655] Loss_D_A: 0.5396181344985962 Loss_D_B: 0.7668297290802002 Loss_G: 0.9220971465110779\n",
      "Epoch [1/10] Batch [5400/20655] Loss_D_A: 0.8193950057029724 Loss_D_B: 0.8052522540092468 Loss_G: 0.7913722991943359\n",
      "Epoch [1/10] Batch [5500/20655] Loss_D_A: 0.3813322186470032 Loss_D_B: 0.07776826620101929 Loss_G: 0.47858285903930664\n",
      "Epoch [1/10] Batch [5600/20655] Loss_D_A: 0.5707737803459167 Loss_D_B: 0.38705676794052124 Loss_G: 0.7603593468666077\n",
      "Epoch [1/10] Batch [5700/20655] Loss_D_A: 0.6985489130020142 Loss_D_B: 0.48089951276779175 Loss_G: 0.2801954746246338\n",
      "Epoch [1/10] Batch [5800/20655] Loss_D_A: 0.9568158388137817 Loss_D_B: 0.929807722568512 Loss_G: 0.4742565155029297\n",
      "Epoch [1/10] Batch [5900/20655] Loss_D_A: 0.6465055346488953 Loss_D_B: 0.7366811633110046 Loss_G: 0.4784427881240845\n",
      "Epoch [1/10] Batch [6000/20655] Loss_D_A: 0.8819847702980042 Loss_D_B: 0.3443880081176758 Loss_G: 0.9658603072166443\n",
      "Epoch [1/10] Batch [6100/20655] Loss_D_A: 0.4667726159095764 Loss_D_B: 0.5427592396736145 Loss_G: 0.22726070880889893\n",
      "Epoch [1/10] Batch [6200/20655] Loss_D_A: 0.9194689393043518 Loss_D_B: 0.3472819924354553 Loss_G: 0.44580453634262085\n",
      "Epoch [1/10] Batch [6300/20655] Loss_D_A: 0.05272597074508667 Loss_D_B: 0.5139912962913513 Loss_G: 0.8065066933631897\n",
      "Epoch [1/10] Batch [6400/20655] Loss_D_A: 0.48420989513397217 Loss_D_B: 0.479925274848938 Loss_G: 0.11552661657333374\n",
      "Epoch [1/10] Batch [6500/20655] Loss_D_A: 0.8929601907730103 Loss_D_B: 0.024904906749725342 Loss_G: 0.8425242900848389\n",
      "Epoch [1/10] Batch [6600/20655] Loss_D_A: 0.5658550262451172 Loss_D_B: 0.13031870126724243 Loss_G: 0.3816230893135071\n",
      "Epoch [1/10] Batch [6700/20655] Loss_D_A: 0.48452913761138916 Loss_D_B: 0.893191397190094 Loss_G: 0.023665964603424072\n",
      "Epoch [1/10] Batch [6800/20655] Loss_D_A: 0.5461848974227905 Loss_D_B: 0.324124276638031 Loss_G: 0.9360150694847107\n",
      "Epoch [1/10] Batch [6900/20655] Loss_D_A: 0.6405422687530518 Loss_D_B: 0.8980429768562317 Loss_G: 0.28910213708877563\n",
      "Epoch [1/10] Batch [7000/20655] Loss_D_A: 0.818793773651123 Loss_D_B: 0.4895665645599365 Loss_G: 0.02986443042755127\n",
      "Epoch [1/10] Batch [7100/20655] Loss_D_A: 0.11420762538909912 Loss_D_B: 0.8554480075836182 Loss_G: 0.564827561378479\n",
      "Epoch [1/10] Batch [7200/20655] Loss_D_A: 0.12233376502990723 Loss_D_B: 0.3860322833061218 Loss_G: 0.41055750846862793\n",
      "Epoch [1/10] Batch [7300/20655] Loss_D_A: 0.6719179749488831 Loss_D_B: 0.7690836191177368 Loss_G: 0.7048152685165405\n",
      "Epoch [1/10] Batch [7400/20655] Loss_D_A: 0.4416000247001648 Loss_D_B: 0.3526651859283447 Loss_G: 0.9465765357017517\n",
      "Epoch [1/10] Batch [7500/20655] Loss_D_A: 0.38808029890060425 Loss_D_B: 0.07386618852615356 Loss_G: 0.43672817945480347\n",
      "Epoch [1/10] Batch [7600/20655] Loss_D_A: 0.4489436149597168 Loss_D_B: 0.8620023131370544 Loss_G: 0.18726062774658203\n",
      "Epoch [1/10] Batch [7700/20655] Loss_D_A: 0.2255101203918457 Loss_D_B: 0.029061496257781982 Loss_G: 0.1464892029762268\n",
      "Epoch [1/10] Batch [7800/20655] Loss_D_A: 0.6504092216491699 Loss_D_B: 0.4203733205795288 Loss_G: 0.4380439519882202\n",
      "Epoch [1/10] Batch [7900/20655] Loss_D_A: 0.31968802213668823 Loss_D_B: 0.0740848183631897 Loss_G: 0.13549840450286865\n",
      "Epoch [1/10] Batch [8000/20655] Loss_D_A: 0.9256401658058167 Loss_D_B: 0.7100566029548645 Loss_G: 0.9088628888130188\n",
      "Epoch [1/10] Batch [8100/20655] Loss_D_A: 0.5195993185043335 Loss_D_B: 0.17013132572174072 Loss_G: 0.24561715126037598\n",
      "Epoch [1/10] Batch [8200/20655] Loss_D_A: 0.37600553035736084 Loss_D_B: 0.17520540952682495 Loss_G: 0.7343966960906982\n",
      "Epoch [1/10] Batch [8300/20655] Loss_D_A: 0.2965908646583557 Loss_D_B: 0.4065467119216919 Loss_G: 0.31913286447525024\n",
      "Epoch [1/10] Batch [8400/20655] Loss_D_A: 0.8521946668624878 Loss_D_B: 0.9548975825309753 Loss_G: 0.9729456305503845\n",
      "Epoch [1/10] Batch [8500/20655] Loss_D_A: 0.635811448097229 Loss_D_B: 0.7162941694259644 Loss_G: 0.1972694993019104\n",
      "Epoch [1/10] Batch [8600/20655] Loss_D_A: 0.5902391076087952 Loss_D_B: 0.0702972412109375 Loss_G: 0.7714879512786865\n",
      "Epoch [1/10] Batch [8700/20655] Loss_D_A: 0.8200488686561584 Loss_D_B: 0.19724059104919434 Loss_G: 0.5944266319274902\n",
      "Epoch [1/10] Batch [8800/20655] Loss_D_A: 0.9523672461509705 Loss_D_B: 0.40993648767471313 Loss_G: 0.7244771718978882\n",
      "Epoch [1/10] Batch [8900/20655] Loss_D_A: 0.9055019617080688 Loss_D_B: 0.9436602592468262 Loss_G: 0.3056122064590454\n",
      "Epoch [1/10] Batch [9000/20655] Loss_D_A: 0.8119134306907654 Loss_D_B: 0.40368038415908813 Loss_G: 0.22659099102020264\n",
      "Epoch [1/10] Batch [9100/20655] Loss_D_A: 0.1384785771369934 Loss_D_B: 0.3720160722732544 Loss_G: 0.6628274917602539\n",
      "Epoch [1/10] Batch [9200/20655] Loss_D_A: 0.4056696891784668 Loss_D_B: 0.5031798481941223 Loss_G: 0.12670516967773438\n",
      "Epoch [1/10] Batch [9300/20655] Loss_D_A: 0.7273207902908325 Loss_D_B: 0.3704667091369629 Loss_G: 0.421644389629364\n",
      "Epoch [1/10] Batch [9400/20655] Loss_D_A: 0.2576414942741394 Loss_D_B: 0.016045987606048584 Loss_G: 0.9349229335784912\n",
      "Epoch [1/10] Batch [9500/20655] Loss_D_A: 0.7315521240234375 Loss_D_B: 0.20417803525924683 Loss_G: 0.9589246511459351\n",
      "Epoch [1/10] Batch [9600/20655] Loss_D_A: 0.7209516167640686 Loss_D_B: 0.9919933676719666 Loss_G: 0.04991394281387329\n",
      "Epoch [1/10] Batch [9700/20655] Loss_D_A: 0.982926607131958 Loss_D_B: 0.6146317720413208 Loss_G: 0.3121442198753357\n",
      "Epoch [1/10] Batch [9800/20655] Loss_D_A: 0.40182578563690186 Loss_D_B: 0.18392640352249146 Loss_G: 0.5208227038383484\n",
      "Epoch [1/10] Batch [9900/20655] Loss_D_A: 0.3165469765663147 Loss_D_B: 0.1943531036376953 Loss_G: 0.7887369394302368\n",
      "Epoch [1/10] Batch [10000/20655] Loss_D_A: 0.6933765411376953 Loss_D_B: 0.5754246115684509 Loss_G: 0.17298632860183716\n",
      "Epoch [1/10] Batch [10100/20655] Loss_D_A: 0.693213164806366 Loss_D_B: 0.5128242373466492 Loss_G: 0.21982330083847046\n",
      "Epoch [1/10] Batch [10200/20655] Loss_D_A: 0.9337697625160217 Loss_D_B: 0.47181838750839233 Loss_G: 0.6866776943206787\n",
      "Epoch [1/10] Batch [10300/20655] Loss_D_A: 0.9427712559700012 Loss_D_B: 0.8651286363601685 Loss_G: 0.6054876446723938\n",
      "Epoch [1/10] Batch [10400/20655] Loss_D_A: 0.5970886945724487 Loss_D_B: 0.9025668501853943 Loss_G: 0.6515087485313416\n",
      "Epoch [1/10] Batch [10500/20655] Loss_D_A: 0.3483186364173889 Loss_D_B: 0.26567786931991577 Loss_G: 0.2737918496131897\n",
      "Epoch [1/10] Batch [10600/20655] Loss_D_A: 0.9644850492477417 Loss_D_B: 0.012639760971069336 Loss_G: 0.02785879373550415\n",
      "Epoch [1/10] Batch [10700/20655] Loss_D_A: 0.17228585481643677 Loss_D_B: 0.5269533395767212 Loss_G: 0.4138411283493042\n",
      "Epoch [1/10] Batch [10800/20655] Loss_D_A: 0.11091774702072144 Loss_D_B: 0.8447007536888123 Loss_G: 0.2890373468399048\n",
      "Epoch [1/10] Batch [10900/20655] Loss_D_A: 0.29213935136795044 Loss_D_B: 0.9163526296615601 Loss_G: 0.8275946378707886\n",
      "Epoch [1/10] Batch [11000/20655] Loss_D_A: 0.7646527886390686 Loss_D_B: 0.539709210395813 Loss_G: 0.637224555015564\n",
      "Epoch [1/10] Batch [11100/20655] Loss_D_A: 0.5496965646743774 Loss_D_B: 0.9631935954093933 Loss_G: 0.30639171600341797\n",
      "Epoch [1/10] Batch [11200/20655] Loss_D_A: 0.6797963380813599 Loss_D_B: 0.42346304655075073 Loss_G: 0.003714442253112793\n",
      "Epoch [1/10] Batch [11300/20655] Loss_D_A: 0.15461015701293945 Loss_D_B: 0.5298624634742737 Loss_G: 0.8036015033721924\n",
      "Epoch [1/10] Batch [11400/20655] Loss_D_A: 0.20769250392913818 Loss_D_B: 0.0993087887763977 Loss_G: 0.9294469356536865\n",
      "Epoch [1/10] Batch [11500/20655] Loss_D_A: 0.4380364418029785 Loss_D_B: 0.9400956630706787 Loss_G: 0.7088101506233215\n",
      "Epoch [1/10] Batch [11600/20655] Loss_D_A: 0.5319095849990845 Loss_D_B: 0.26216089725494385 Loss_G: 0.1529577374458313\n",
      "Epoch [1/10] Batch [11700/20655] Loss_D_A: 0.24748313426971436 Loss_D_B: 0.968886137008667 Loss_G: 0.5199496746063232\n",
      "Epoch [1/10] Batch [11800/20655] Loss_D_A: 0.07590043544769287 Loss_D_B: 0.21140003204345703 Loss_G: 0.8322818279266357\n",
      "Epoch [1/10] Batch [11900/20655] Loss_D_A: 0.3689038157463074 Loss_D_B: 0.4216434359550476 Loss_G: 0.8352548480033875\n",
      "Epoch [1/10] Batch [12000/20655] Loss_D_A: 0.6714446544647217 Loss_D_B: 0.8518035411834717 Loss_G: 0.6526262760162354\n",
      "Epoch [1/10] Batch [12100/20655] Loss_D_A: 0.4016363024711609 Loss_D_B: 0.7073908448219299 Loss_G: 0.3804394602775574\n",
      "Epoch [1/10] Batch [12200/20655] Loss_D_A: 0.27554428577423096 Loss_D_B: 0.29374682903289795 Loss_G: 0.40434080362319946\n",
      "Epoch [1/10] Batch [12300/20655] Loss_D_A: 0.47858428955078125 Loss_D_B: 0.7427467107772827 Loss_G: 0.3502463698387146\n",
      "Epoch [1/10] Batch [12400/20655] Loss_D_A: 0.2961543798446655 Loss_D_B: 0.6529416441917419 Loss_G: 0.8169485330581665\n",
      "Epoch [1/10] Batch [12500/20655] Loss_D_A: 0.837896466255188 Loss_D_B: 0.950098991394043 Loss_G: 0.3423249125480652\n",
      "Epoch [1/10] Batch [12600/20655] Loss_D_A: 0.8261510133743286 Loss_D_B: 0.90506511926651 Loss_G: 0.324654221534729\n",
      "Epoch [1/10] Batch [12700/20655] Loss_D_A: 0.07677078247070312 Loss_D_B: 0.44089263677597046 Loss_G: 0.7810555100440979\n",
      "Epoch [1/10] Batch [12800/20655] Loss_D_A: 0.00878453254699707 Loss_D_B: 0.8775246739387512 Loss_G: 0.2715175747871399\n",
      "Epoch [1/10] Batch [12900/20655] Loss_D_A: 0.5793570876121521 Loss_D_B: 0.14466851949691772 Loss_G: 0.7930879592895508\n",
      "Epoch [1/10] Batch [13000/20655] Loss_D_A: 0.487887442111969 Loss_D_B: 0.0661153793334961 Loss_G: 0.3238721489906311\n",
      "Epoch [1/10] Batch [13100/20655] Loss_D_A: 0.7030176520347595 Loss_D_B: 0.38411587476730347 Loss_G: 0.44246119260787964\n",
      "Epoch [1/10] Batch [13200/20655] Loss_D_A: 0.12292611598968506 Loss_D_B: 0.27746284008026123 Loss_G: 0.3164697289466858\n",
      "Epoch [1/10] Batch [13300/20655] Loss_D_A: 0.06864672899246216 Loss_D_B: 0.5731399059295654 Loss_G: 0.4257085919380188\n",
      "Epoch [1/10] Batch [13400/20655] Loss_D_A: 0.5352376103401184 Loss_D_B: 0.9051371216773987 Loss_G: 0.5452362298965454\n",
      "Epoch [1/10] Batch [13500/20655] Loss_D_A: 0.9501416087150574 Loss_D_B: 0.31334346532821655 Loss_G: 0.2062166929244995\n",
      "Epoch [1/10] Batch [13600/20655] Loss_D_A: 0.31487858295440674 Loss_D_B: 0.5310284495353699 Loss_G: 0.33556288480758667\n",
      "Epoch [1/10] Batch [13700/20655] Loss_D_A: 0.6037776470184326 Loss_D_B: 0.8508874773979187 Loss_G: 0.7822360396385193\n",
      "Epoch [1/10] Batch [13800/20655] Loss_D_A: 0.40412795543670654 Loss_D_B: 0.8814900517463684 Loss_G: 0.6146095991134644\n",
      "Epoch [1/10] Batch [13900/20655] Loss_D_A: 0.014905452728271484 Loss_D_B: 0.5305835008621216 Loss_G: 0.26789915561676025\n",
      "Epoch [1/10] Batch [14000/20655] Loss_D_A: 0.874894917011261 Loss_D_B: 0.15502792596817017 Loss_G: 0.36715251207351685\n",
      "Epoch [1/10] Batch [14100/20655] Loss_D_A: 0.5985425710678101 Loss_D_B: 0.13484549522399902 Loss_G: 0.752000093460083\n",
      "Epoch [1/10] Batch [14200/20655] Loss_D_A: 0.2642422318458557 Loss_D_B: 0.5521448850631714 Loss_G: 0.33302849531173706\n",
      "Epoch [1/10] Batch [14300/20655] Loss_D_A: 0.7354326248168945 Loss_D_B: 0.2701186537742615 Loss_G: 0.972251296043396\n",
      "Epoch [1/10] Batch [14400/20655] Loss_D_A: 0.2827625870704651 Loss_D_B: 0.5560992956161499 Loss_G: 0.02977651357650757\n",
      "Epoch [1/10] Batch [14500/20655] Loss_D_A: 0.3342664837837219 Loss_D_B: 0.11694508790969849 Loss_G: 0.33765387535095215\n",
      "Epoch [1/10] Batch [14600/20655] Loss_D_A: 0.9111288785934448 Loss_D_B: 0.015932679176330566 Loss_G: 0.023695051670074463\n",
      "Epoch [1/10] Batch [14700/20655] Loss_D_A: 0.6978784203529358 Loss_D_B: 0.24382233619689941 Loss_G: 0.7650604248046875\n",
      "Epoch [1/10] Batch [14800/20655] Loss_D_A: 0.7645527124404907 Loss_D_B: 0.4984204173088074 Loss_G: 0.42027097940444946\n",
      "Epoch [1/10] Batch [14900/20655] Loss_D_A: 0.00177764892578125 Loss_D_B: 0.06890225410461426 Loss_G: 0.3562432527542114\n",
      "Epoch [1/10] Batch [15000/20655] Loss_D_A: 0.37914860248565674 Loss_D_B: 0.1646817922592163 Loss_G: 0.1538463830947876\n",
      "Epoch [1/10] Batch [15100/20655] Loss_D_A: 0.21164792776107788 Loss_D_B: 0.07670420408248901 Loss_G: 0.04194670915603638\n",
      "Epoch [1/10] Batch [15200/20655] Loss_D_A: 0.3235616683959961 Loss_D_B: 0.9243763089179993 Loss_G: 0.04538774490356445\n",
      "Epoch [1/10] Batch [15300/20655] Loss_D_A: 0.4488416314125061 Loss_D_B: 0.3087886571884155 Loss_G: 0.7860116958618164\n",
      "Epoch [1/10] Batch [15400/20655] Loss_D_A: 0.5863248705863953 Loss_D_B: 0.8943661451339722 Loss_G: 0.556045413017273\n",
      "Epoch [1/10] Batch [15500/20655] Loss_D_A: 0.3730922341346741 Loss_D_B: 0.3963415026664734 Loss_G: 0.8340685367584229\n",
      "Epoch [1/10] Batch [15600/20655] Loss_D_A: 0.44452327489852905 Loss_D_B: 0.7953125238418579 Loss_G: 0.14386403560638428\n",
      "Epoch [1/10] Batch [15700/20655] Loss_D_A: 0.3549240231513977 Loss_D_B: 0.9188261032104492 Loss_G: 0.46479809284210205\n",
      "Epoch [1/10] Batch [15800/20655] Loss_D_A: 0.35942918062210083 Loss_D_B: 0.8011264801025391 Loss_G: 0.9040546417236328\n",
      "Epoch [1/10] Batch [15900/20655] Loss_D_A: 0.46960556507110596 Loss_D_B: 0.14888852834701538 Loss_G: 0.2620648145675659\n",
      "Epoch [1/10] Batch [16000/20655] Loss_D_A: 0.6420949697494507 Loss_D_B: 0.9547462463378906 Loss_G: 0.8029277324676514\n",
      "Epoch [1/10] Batch [16100/20655] Loss_D_A: 0.8293359279632568 Loss_D_B: 0.4014340043067932 Loss_G: 0.6631854772567749\n",
      "Epoch [1/10] Batch [16200/20655] Loss_D_A: 0.5999219417572021 Loss_D_B: 0.15375560522079468 Loss_G: 0.22465568780899048\n",
      "Epoch [1/10] Batch [16300/20655] Loss_D_A: 0.5860298871994019 Loss_D_B: 0.25683534145355225 Loss_G: 0.6149182915687561\n",
      "Epoch [1/10] Batch [16400/20655] Loss_D_A: 0.7915738821029663 Loss_D_B: 0.13378804922103882 Loss_G: 0.39109164476394653\n",
      "Epoch [1/10] Batch [16500/20655] Loss_D_A: 0.8297518491744995 Loss_D_B: 0.47013604640960693 Loss_G: 0.9609631896018982\n",
      "Epoch [1/10] Batch [16600/20655] Loss_D_A: 0.032170116901397705 Loss_D_B: 0.200161874294281 Loss_G: 0.006797850131988525\n",
      "Epoch [1/10] Batch [16700/20655] Loss_D_A: 0.3715108036994934 Loss_D_B: 0.3035046458244324 Loss_G: 0.1495404839515686\n",
      "Epoch [1/10] Batch [16800/20655] Loss_D_A: 0.9371617436408997 Loss_D_B: 0.648354709148407 Loss_G: 0.9053208231925964\n",
      "Epoch [1/10] Batch [16900/20655] Loss_D_A: 0.5305553674697876 Loss_D_B: 0.8076973557472229 Loss_G: 0.7676301598548889\n",
      "Epoch [1/10] Batch [17000/20655] Loss_D_A: 0.6413398385047913 Loss_D_B: 0.3736044764518738 Loss_G: 0.35852187871932983\n",
      "Epoch [1/10] Batch [17100/20655] Loss_D_A: 0.4194280505180359 Loss_D_B: 0.043053388595581055 Loss_G: 0.21819818019866943\n",
      "Epoch [1/10] Batch [17200/20655] Loss_D_A: 0.8116073608398438 Loss_D_B: 0.2258123755455017 Loss_G: 0.9789922833442688\n",
      "Epoch [1/10] Batch [17300/20655] Loss_D_A: 0.6225540637969971 Loss_D_B: 0.6083860993385315 Loss_G: 0.15332311391830444\n",
      "Epoch [1/10] Batch [17400/20655] Loss_D_A: 0.20215070247650146 Loss_D_B: 0.4638187885284424 Loss_G: 0.8371825218200684\n",
      "Epoch [1/10] Batch [17500/20655] Loss_D_A: 0.4226372241973877 Loss_D_B: 0.052594542503356934 Loss_G: 0.5249765515327454\n",
      "Epoch [1/10] Batch [17600/20655] Loss_D_A: 0.24272000789642334 Loss_D_B: 0.4146128296852112 Loss_G: 0.7305901050567627\n",
      "Epoch [1/10] Batch [17700/20655] Loss_D_A: 0.13084542751312256 Loss_D_B: 0.7339197993278503 Loss_G: 0.14684516191482544\n",
      "Epoch [1/10] Batch [17800/20655] Loss_D_A: 0.13395297527313232 Loss_D_B: 0.28447771072387695 Loss_G: 0.5005614161491394\n",
      "Epoch [1/10] Batch [17900/20655] Loss_D_A: 0.052814602851867676 Loss_D_B: 0.11212480068206787 Loss_G: 0.7914958000183105\n",
      "Epoch [1/10] Batch [18000/20655] Loss_D_A: 0.37733036279678345 Loss_D_B: 0.4404066205024719 Loss_G: 0.48469680547714233\n",
      "Epoch [1/10] Batch [18100/20655] Loss_D_A: 0.46392375230789185 Loss_D_B: 0.837871789932251 Loss_G: 0.597842276096344\n",
      "Epoch [1/10] Batch [18200/20655] Loss_D_A: 0.14839816093444824 Loss_D_B: 0.10400259494781494 Loss_G: 0.4158097505569458\n",
      "Epoch [1/10] Batch [18300/20655] Loss_D_A: 0.11332398653030396 Loss_D_B: 0.8423016667366028 Loss_G: 0.8101593852043152\n",
      "Epoch [1/10] Batch [18400/20655] Loss_D_A: 0.7843621969223022 Loss_D_B: 0.022253870964050293 Loss_G: 0.2953818440437317\n",
      "Epoch [1/10] Batch [18500/20655] Loss_D_A: 0.32460570335388184 Loss_D_B: 0.41766446828842163 Loss_G: 0.027913808822631836\n",
      "Epoch [1/10] Batch [18600/20655] Loss_D_A: 0.7626926898956299 Loss_D_B: 0.8349921703338623 Loss_G: 0.6262184381484985\n",
      "Epoch [1/10] Batch [18700/20655] Loss_D_A: 0.979997456073761 Loss_D_B: 0.010467708110809326 Loss_G: 0.7503501772880554\n",
      "Epoch [1/10] Batch [18800/20655] Loss_D_A: 0.7372815608978271 Loss_D_B: 0.7201558947563171 Loss_G: 0.7970807552337646\n",
      "Epoch [1/10] Batch [18900/20655] Loss_D_A: 0.8535627126693726 Loss_D_B: 0.518735945224762 Loss_G: 0.36545342206954956\n",
      "Epoch [1/10] Batch [19000/20655] Loss_D_A: 0.1487070918083191 Loss_D_B: 0.0524907112121582 Loss_G: 0.046923935413360596\n",
      "Epoch [1/10] Batch [19100/20655] Loss_D_A: 0.6682835817337036 Loss_D_B: 0.7142347097396851 Loss_G: 0.3673996925354004\n",
      "Epoch [1/10] Batch [19200/20655] Loss_D_A: 0.7418559789657593 Loss_D_B: 0.02023226022720337 Loss_G: 0.9655766487121582\n",
      "Epoch [1/10] Batch [19300/20655] Loss_D_A: 0.04496121406555176 Loss_D_B: 0.5157786011695862 Loss_G: 0.06208318471908569\n",
      "Epoch [1/10] Batch [19400/20655] Loss_D_A: 0.9247757196426392 Loss_D_B: 0.11836189031600952 Loss_G: 0.653294563293457\n",
      "Epoch [1/10] Batch [19500/20655] Loss_D_A: 0.2316502332687378 Loss_D_B: 0.5062548518180847 Loss_G: 0.2023468017578125\n",
      "Epoch [1/10] Batch [19600/20655] Loss_D_A: 0.10582977533340454 Loss_D_B: 0.7749487161636353 Loss_G: 0.9570602178573608\n",
      "Epoch [1/10] Batch [19700/20655] Loss_D_A: 0.8009990453720093 Loss_D_B: 0.46674293279647827 Loss_G: 0.49473243951797485\n",
      "Epoch [1/10] Batch [19800/20655] Loss_D_A: 0.6529310345649719 Loss_D_B: 0.10476607084274292 Loss_G: 0.18893611431121826\n",
      "Epoch [1/10] Batch [19900/20655] Loss_D_A: 0.45295244455337524 Loss_D_B: 0.07045358419418335 Loss_G: 0.5847432613372803\n",
      "Epoch [1/10] Batch [20000/20655] Loss_D_A: 0.03133350610733032 Loss_D_B: 0.09410369396209717 Loss_G: 0.9330154061317444\n",
      "Epoch [1/10] Batch [20100/20655] Loss_D_A: 0.36761707067489624 Loss_D_B: 0.2794228196144104 Loss_G: 0.2595357894897461\n",
      "Epoch [1/10] Batch [20200/20655] Loss_D_A: 0.6368487477302551 Loss_D_B: 0.9170328974723816 Loss_G: 0.5971852540969849\n",
      "Epoch [1/10] Batch [20300/20655] Loss_D_A: 0.07967394590377808 Loss_D_B: 0.3578031063079834 Loss_G: 0.572132408618927\n",
      "Epoch [1/10] Batch [20400/20655] Loss_D_A: 0.9828873872756958 Loss_D_B: 0.9528517127037048 Loss_G: 0.5958364009857178\n",
      "Epoch [1/10] Batch [20500/20655] Loss_D_A: 0.7718935012817383 Loss_D_B: 0.0886048674583435 Loss_G: 0.06386518478393555\n",
      "Epoch [1/10] Batch [20600/20655] Loss_D_A: 0.8795554041862488 Loss_D_B: 0.5486472845077515 Loss_G: 0.2502242922782898\n",
      "Epoch [2/10] Batch [0/20655] Loss_D_A: 0.756834089756012 Loss_D_B: 0.5830504894256592 Loss_G: 0.2680385112762451\n",
      "Epoch [2/10] Batch [0/20655] Loss_D_A: 0.4786849617958069 Loss_D_B: 0.38527894020080566 Loss_G: 0.1476733684539795\n",
      "Epoch [2/10] Batch [100/20655] Loss_D_A: 0.47179514169692993 Loss_D_B: 0.5763641595840454 Loss_G: 0.7905768752098083\n",
      "Epoch [2/10] Batch [200/20655] Loss_D_A: 0.8074857592582703 Loss_D_B: 0.2883586883544922 Loss_G: 0.472909152507782\n",
      "Epoch [2/10] Batch [300/20655] Loss_D_A: 0.2099819779396057 Loss_D_B: 0.9601244926452637 Loss_G: 0.2779442071914673\n",
      "Epoch [2/10] Batch [400/20655] Loss_D_A: 0.6348880529403687 Loss_D_B: 0.43904322385787964 Loss_G: 0.1916608214378357\n",
      "Epoch [2/10] Batch [500/20655] Loss_D_A: 0.45053452253341675 Loss_D_B: 0.9449462294578552 Loss_G: 0.14768552780151367\n",
      "Epoch [2/10] Batch [600/20655] Loss_D_A: 0.0937078595161438 Loss_D_B: 0.7081606984138489 Loss_G: 0.7753480672836304\n",
      "Epoch [2/10] Batch [700/20655] Loss_D_A: 0.6526472568511963 Loss_D_B: 0.08145523071289062 Loss_G: 0.059269607067108154\n",
      "Epoch [2/10] Batch [800/20655] Loss_D_A: 0.3159976601600647 Loss_D_B: 0.4532175064086914 Loss_G: 0.023253798484802246\n",
      "Epoch [2/10] Batch [900/20655] Loss_D_A: 0.17322885990142822 Loss_D_B: 0.011451363563537598 Loss_G: 0.06997239589691162\n",
      "Epoch [2/10] Batch [1000/20655] Loss_D_A: 0.2758089303970337 Loss_D_B: 0.3636300563812256 Loss_G: 0.7003929615020752\n",
      "Epoch [2/10] Batch [1100/20655] Loss_D_A: 0.3178136944770813 Loss_D_B: 0.2670940160751343 Loss_G: 0.2540082335472107\n",
      "Epoch [2/10] Batch [1200/20655] Loss_D_A: 0.10960197448730469 Loss_D_B: 0.5796565413475037 Loss_G: 0.44588160514831543\n",
      "Epoch [2/10] Batch [1300/20655] Loss_D_A: 0.3620031476020813 Loss_D_B: 0.7681229710578918 Loss_G: 0.9852516651153564\n",
      "Epoch [2/10] Batch [1400/20655] Loss_D_A: 0.3802756071090698 Loss_D_B: 0.5923910737037659 Loss_G: 0.8616828322410583\n",
      "Epoch [2/10] Batch [1500/20655] Loss_D_A: 0.8812098503112793 Loss_D_B: 0.049721598625183105 Loss_G: 0.4179415702819824\n",
      "Epoch [2/10] Batch [1600/20655] Loss_D_A: 0.6571195721626282 Loss_D_B: 0.34174007177352905 Loss_G: 0.42888110876083374\n",
      "Epoch [2/10] Batch [1700/20655] Loss_D_A: 0.9740177392959595 Loss_D_B: 0.4309770464897156 Loss_G: 0.08570629358291626\n",
      "Epoch [2/10] Batch [1800/20655] Loss_D_A: 0.8755424618721008 Loss_D_B: 0.5717962384223938 Loss_G: 0.6375192999839783\n",
      "Epoch [2/10] Batch [1900/20655] Loss_D_A: 0.808079719543457 Loss_D_B: 0.5218453407287598 Loss_G: 0.6599662899971008\n",
      "Epoch [2/10] Batch [2000/20655] Loss_D_A: 0.6534965634346008 Loss_D_B: 0.9736921191215515 Loss_G: 0.28195059299468994\n",
      "Epoch [2/10] Batch [2100/20655] Loss_D_A: 0.7555435299873352 Loss_D_B: 0.43192845582962036 Loss_G: 0.07875597476959229\n",
      "Epoch [2/10] Batch [2200/20655] Loss_D_A: 0.9414464831352234 Loss_D_B: 0.2988842725753784 Loss_G: 0.020421266555786133\n",
      "Epoch [2/10] Batch [2300/20655] Loss_D_A: 0.1064344048500061 Loss_D_B: 0.023701608180999756 Loss_G: 0.6186460852622986\n",
      "Epoch [2/10] Batch [2400/20655] Loss_D_A: 0.6086868047714233 Loss_D_B: 0.006545543670654297 Loss_G: 0.7582179307937622\n",
      "Epoch [2/10] Batch [2500/20655] Loss_D_A: 0.5524734854698181 Loss_D_B: 0.6999504566192627 Loss_G: 0.7263795733451843\n",
      "Epoch [2/10] Batch [2600/20655] Loss_D_A: 0.6489213109016418 Loss_D_B: 0.03976410627365112 Loss_G: 0.7249423265457153\n",
      "Epoch [2/10] Batch [2700/20655] Loss_D_A: 0.7106596827507019 Loss_D_B: 0.6560423374176025 Loss_G: 0.41150379180908203\n",
      "Epoch [2/10] Batch [2800/20655] Loss_D_A: 0.21796131134033203 Loss_D_B: 0.5359174609184265 Loss_G: 0.616827130317688\n",
      "Epoch [2/10] Batch [2900/20655] Loss_D_A: 0.2520604133605957 Loss_D_B: 0.6418832540512085 Loss_G: 0.44024115800857544\n",
      "Epoch [2/10] Batch [3000/20655] Loss_D_A: 0.12301445007324219 Loss_D_B: 0.8546072244644165 Loss_G: 0.312505304813385\n",
      "Epoch [2/10] Batch [3100/20655] Loss_D_A: 0.7753537893295288 Loss_D_B: 0.9549877643585205 Loss_G: 0.3896141052246094\n",
      "Epoch [2/10] Batch [3200/20655] Loss_D_A: 0.8055627942085266 Loss_D_B: 0.9761753678321838 Loss_G: 0.1742662787437439\n",
      "Epoch [2/10] Batch [3300/20655] Loss_D_A: 0.596520721912384 Loss_D_B: 0.3969675898551941 Loss_G: 0.6739115118980408\n",
      "Epoch [2/10] Batch [3400/20655] Loss_D_A: 0.2595452666282654 Loss_D_B: 0.3284003734588623 Loss_G: 0.7182213664054871\n",
      "Epoch [2/10] Batch [3500/20655] Loss_D_A: 0.3777250051498413 Loss_D_B: 0.05040019750595093 Loss_G: 0.9329804182052612\n",
      "Epoch [2/10] Batch [3600/20655] Loss_D_A: 0.5633447766304016 Loss_D_B: 0.6292823553085327 Loss_G: 0.5365102291107178\n",
      "Epoch [2/10] Batch [3700/20655] Loss_D_A: 0.50931715965271 Loss_D_B: 0.29952603578567505 Loss_G: 0.14005804061889648\n",
      "Epoch [2/10] Batch [3800/20655] Loss_D_A: 0.4792020916938782 Loss_D_B: 0.2640928030014038 Loss_G: 0.6908591389656067\n",
      "Epoch [2/10] Batch [3900/20655] Loss_D_A: 0.9260187149047852 Loss_D_B: 0.32510125637054443 Loss_G: 0.4545143246650696\n",
      "Epoch [2/10] Batch [4000/20655] Loss_D_A: 0.06636816263198853 Loss_D_B: 0.1803075671195984 Loss_G: 0.41939544677734375\n",
      "Epoch [2/10] Batch [4100/20655] Loss_D_A: 0.131494402885437 Loss_D_B: 0.48022300004959106 Loss_G: 0.3738529682159424\n",
      "Epoch [2/10] Batch [4200/20655] Loss_D_A: 0.26206326484680176 Loss_D_B: 0.6890451312065125 Loss_G: 0.740425169467926\n",
      "Epoch [2/10] Batch [4300/20655] Loss_D_A: 0.4030963182449341 Loss_D_B: 0.6665181517601013 Loss_G: 0.07718890905380249\n",
      "Epoch [2/10] Batch [4400/20655] Loss_D_A: 0.6038680672645569 Loss_D_B: 0.4724065065383911 Loss_G: 0.3698526620864868\n",
      "Epoch [2/10] Batch [4500/20655] Loss_D_A: 0.21739178895950317 Loss_D_B: 0.6612672209739685 Loss_G: 0.310721218585968\n",
      "Epoch [2/10] Batch [4600/20655] Loss_D_A: 0.8562097549438477 Loss_D_B: 0.8555140495300293 Loss_G: 0.6474020481109619\n",
      "Epoch [2/10] Batch [4700/20655] Loss_D_A: 0.6779414415359497 Loss_D_B: 0.028100252151489258 Loss_G: 0.8112056851387024\n",
      "Epoch [2/10] Batch [4800/20655] Loss_D_A: 0.41315335035324097 Loss_D_B: 0.0048506855964660645 Loss_G: 0.6594676971435547\n",
      "Epoch [2/10] Batch [4900/20655] Loss_D_A: 0.7957732081413269 Loss_D_B: 0.9907644987106323 Loss_G: 0.8251082897186279\n",
      "Epoch [2/10] Batch [5000/20655] Loss_D_A: 0.853302538394928 Loss_D_B: 0.6533377170562744 Loss_G: 0.5023837089538574\n",
      "Epoch [2/10] Batch [5100/20655] Loss_D_A: 0.8200551271438599 Loss_D_B: 0.3680065870285034 Loss_G: 0.03806447982788086\n",
      "Epoch [2/10] Batch [5200/20655] Loss_D_A: 0.5182029008865356 Loss_D_B: 0.9587852954864502 Loss_G: 0.3931630253791809\n",
      "Epoch [2/10] Batch [5300/20655] Loss_D_A: 0.5945485830307007 Loss_D_B: 0.12458735704421997 Loss_G: 0.5521209836006165\n",
      "Epoch [2/10] Batch [5400/20655] Loss_D_A: 0.5926702618598938 Loss_D_B: 0.9176536798477173 Loss_G: 0.06349050998687744\n",
      "Epoch [2/10] Batch [5500/20655] Loss_D_A: 0.3045765161514282 Loss_D_B: 0.1991979479789734 Loss_G: 0.6867507100105286\n",
      "Epoch [2/10] Batch [5600/20655] Loss_D_A: 0.9710402488708496 Loss_D_B: 0.2716636657714844 Loss_G: 0.2240896224975586\n",
      "Epoch [2/10] Batch [5700/20655] Loss_D_A: 0.34987008571624756 Loss_D_B: 0.7238691449165344 Loss_G: 0.3549540638923645\n",
      "Epoch [2/10] Batch [5800/20655] Loss_D_A: 0.3615594506263733 Loss_D_B: 0.4306047558784485 Loss_G: 0.6583795547485352\n",
      "Epoch [2/10] Batch [5900/20655] Loss_D_A: 0.9414650797843933 Loss_D_B: 0.8070619702339172 Loss_G: 0.05565530061721802\n",
      "Epoch [2/10] Batch [6000/20655] Loss_D_A: 0.6563290953636169 Loss_D_B: 0.6768388748168945 Loss_G: 0.5157476663589478\n",
      "Epoch [2/10] Batch [6100/20655] Loss_D_A: 0.03344231843948364 Loss_D_B: 0.002586543560028076 Loss_G: 0.32824885845184326\n",
      "Epoch [2/10] Batch [6200/20655] Loss_D_A: 0.6691844463348389 Loss_D_B: 0.3130241632461548 Loss_G: 0.9309216141700745\n",
      "Epoch [2/10] Batch [6300/20655] Loss_D_A: 0.4933367371559143 Loss_D_B: 0.7969089150428772 Loss_G: 0.2971939444541931\n",
      "Epoch [2/10] Batch [6400/20655] Loss_D_A: 0.11842352151870728 Loss_D_B: 0.5275717973709106 Loss_G: 0.2541811466217041\n",
      "Epoch [2/10] Batch [6500/20655] Loss_D_A: 0.23338645696640015 Loss_D_B: 0.8968995213508606 Loss_G: 0.08571130037307739\n",
      "Epoch [2/10] Batch [6600/20655] Loss_D_A: 0.23446232080459595 Loss_D_B: 0.42917054891586304 Loss_G: 0.4547528624534607\n",
      "Epoch [2/10] Batch [6700/20655] Loss_D_A: 0.3422384262084961 Loss_D_B: 0.9619802832603455 Loss_G: 0.417270302772522\n",
      "Epoch [2/10] Batch [6800/20655] Loss_D_A: 0.6050174832344055 Loss_D_B: 0.4142959713935852 Loss_G: 0.08951860666275024\n",
      "Epoch [2/10] Batch [6900/20655] Loss_D_A: 0.4612872004508972 Loss_D_B: 0.023164749145507812 Loss_G: 0.9597606658935547\n",
      "Epoch [2/10] Batch [7000/20655] Loss_D_A: 0.481489360332489 Loss_D_B: 0.21816885471343994 Loss_G: 0.838797926902771\n",
      "Epoch [2/10] Batch [7100/20655] Loss_D_A: 0.0675191879272461 Loss_D_B: 0.5967103838920593 Loss_G: 0.8434853553771973\n",
      "Epoch [2/10] Batch [7200/20655] Loss_D_A: 0.33349788188934326 Loss_D_B: 0.5360657572746277 Loss_G: 0.5670345425605774\n",
      "Epoch [2/10] Batch [7300/20655] Loss_D_A: 0.4960091710090637 Loss_D_B: 0.9016765356063843 Loss_G: 0.23533082008361816\n",
      "Epoch [2/10] Batch [7400/20655] Loss_D_A: 0.22408676147460938 Loss_D_B: 0.3598220944404602 Loss_G: 0.28793078660964966\n",
      "Epoch [2/10] Batch [7500/20655] Loss_D_A: 0.8715015649795532 Loss_D_B: 0.3315812349319458 Loss_G: 0.6569667458534241\n",
      "Epoch [2/10] Batch [7600/20655] Loss_D_A: 0.5955889225006104 Loss_D_B: 0.22781133651733398 Loss_G: 0.8917178511619568\n",
      "Epoch [2/10] Batch [7700/20655] Loss_D_A: 0.33350878953933716 Loss_D_B: 0.2541300058364868 Loss_G: 0.9968858361244202\n",
      "Epoch [2/10] Batch [7800/20655] Loss_D_A: 0.8295029401779175 Loss_D_B: 0.9814400672912598 Loss_G: 0.06573313474655151\n",
      "Epoch [2/10] Batch [7900/20655] Loss_D_A: 0.8621658682823181 Loss_D_B: 0.9969151020050049 Loss_G: 0.42465585470199585\n",
      "Epoch [2/10] Batch [8000/20655] Loss_D_A: 0.8936633467674255 Loss_D_B: 0.20975565910339355 Loss_G: 0.8973790407180786\n",
      "Epoch [2/10] Batch [8100/20655] Loss_D_A: 0.2521684765815735 Loss_D_B: 0.4188203811645508 Loss_G: 0.6448149681091309\n",
      "Epoch [2/10] Batch [8200/20655] Loss_D_A: 0.04578965902328491 Loss_D_B: 0.8365837335586548 Loss_G: 0.41096997261047363\n",
      "Epoch [2/10] Batch [8300/20655] Loss_D_A: 0.9370145797729492 Loss_D_B: 0.338096559047699 Loss_G: 0.8756743669509888\n",
      "Epoch [2/10] Batch [8400/20655] Loss_D_A: 0.5198708176612854 Loss_D_B: 0.10791212320327759 Loss_G: 0.43391215801239014\n",
      "Epoch [2/10] Batch [8500/20655] Loss_D_A: 0.5793270468711853 Loss_D_B: 0.19776123762130737 Loss_G: 0.9666501879692078\n",
      "Epoch [2/10] Batch [8600/20655] Loss_D_A: 0.9027924537658691 Loss_D_B: 0.43185198307037354 Loss_G: 0.1312142014503479\n",
      "Epoch [2/10] Batch [8700/20655] Loss_D_A: 0.155481219291687 Loss_D_B: 0.3373209834098816 Loss_G: 0.9872305393218994\n",
      "Epoch [2/10] Batch [8800/20655] Loss_D_A: 0.19953715801239014 Loss_D_B: 0.0988149642944336 Loss_G: 0.3846389651298523\n",
      "Epoch [2/10] Batch [8900/20655] Loss_D_A: 0.2572683095932007 Loss_D_B: 0.4383891224861145 Loss_G: 0.12569016218185425\n",
      "Epoch [2/10] Batch [9000/20655] Loss_D_A: 0.954971432685852 Loss_D_B: 0.42542022466659546 Loss_G: 0.7025378346443176\n",
      "Epoch [2/10] Batch [9100/20655] Loss_D_A: 0.554686963558197 Loss_D_B: 0.3230263590812683 Loss_G: 0.14184051752090454\n",
      "Epoch [2/10] Batch [9200/20655] Loss_D_A: 0.21789777278900146 Loss_D_B: 0.8401080965995789 Loss_G: 0.8826984167098999\n",
      "Epoch [2/10] Batch [9300/20655] Loss_D_A: 0.03744006156921387 Loss_D_B: 0.09771311283111572 Loss_G: 0.8303194642066956\n",
      "Epoch [2/10] Batch [9400/20655] Loss_D_A: 0.7001521587371826 Loss_D_B: 0.5337931513786316 Loss_G: 0.27003175020217896\n",
      "Epoch [2/10] Batch [9500/20655] Loss_D_A: 0.8597199320793152 Loss_D_B: 0.4513947367668152 Loss_G: 0.15083634853363037\n",
      "Epoch [2/10] Batch [9600/20655] Loss_D_A: 0.593471348285675 Loss_D_B: 0.14114093780517578 Loss_G: 0.5654383301734924\n",
      "Epoch [2/10] Batch [9700/20655] Loss_D_A: 0.10148811340332031 Loss_D_B: 0.6436237096786499 Loss_G: 0.8451864123344421\n",
      "Epoch [2/10] Batch [9800/20655] Loss_D_A: 0.40095800161361694 Loss_D_B: 0.09746658802032471 Loss_G: 0.19902735948562622\n",
      "Epoch [2/10] Batch [9900/20655] Loss_D_A: 0.931070864200592 Loss_D_B: 0.518868625164032 Loss_G: 0.5524783134460449\n",
      "Epoch [2/10] Batch [10000/20655] Loss_D_A: 0.41000622510910034 Loss_D_B: 0.10916447639465332 Loss_G: 0.1304534673690796\n",
      "Epoch [2/10] Batch [10100/20655] Loss_D_A: 0.9274507761001587 Loss_D_B: 0.5495023131370544 Loss_G: 0.08618676662445068\n",
      "Epoch [2/10] Batch [10200/20655] Loss_D_A: 0.22020882368087769 Loss_D_B: 0.49906325340270996 Loss_G: 0.7894163131713867\n",
      "Epoch [2/10] Batch [10300/20655] Loss_D_A: 0.9912376403808594 Loss_D_B: 0.9490716457366943 Loss_G: 0.7280327677726746\n",
      "Epoch [2/10] Batch [10400/20655] Loss_D_A: 0.668548583984375 Loss_D_B: 0.6092196702957153 Loss_G: 0.45921260118484497\n",
      "Epoch [2/10] Batch [10500/20655] Loss_D_A: 0.21746766567230225 Loss_D_B: 0.8009238243103027 Loss_G: 0.0828714370727539\n",
      "Epoch [2/10] Batch [10600/20655] Loss_D_A: 0.9785517454147339 Loss_D_B: 0.3138308525085449 Loss_G: 0.49396878480911255\n",
      "Epoch [2/10] Batch [10700/20655] Loss_D_A: 0.5975019931793213 Loss_D_B: 0.017645835876464844 Loss_G: 0.8128952980041504\n",
      "Epoch [2/10] Batch [10800/20655] Loss_D_A: 0.03543573617935181 Loss_D_B: 0.24867230653762817 Loss_G: 0.8266803622245789\n",
      "Epoch [2/10] Batch [10900/20655] Loss_D_A: 0.18521076440811157 Loss_D_B: 0.23992198705673218 Loss_G: 0.3813270330429077\n",
      "Epoch [2/10] Batch [11000/20655] Loss_D_A: 0.7819604873657227 Loss_D_B: 0.69515460729599 Loss_G: 0.653393030166626\n",
      "Epoch [2/10] Batch [11100/20655] Loss_D_A: 0.688718318939209 Loss_D_B: 0.49190467596054077 Loss_G: 0.4183397889137268\n",
      "Epoch [2/10] Batch [11200/20655] Loss_D_A: 0.550285816192627 Loss_D_B: 0.2805715799331665 Loss_G: 0.6957457661628723\n",
      "Epoch [2/10] Batch [11300/20655] Loss_D_A: 0.21369636058807373 Loss_D_B: 0.05082130432128906 Loss_G: 0.06389331817626953\n",
      "Epoch [2/10] Batch [11400/20655] Loss_D_A: 0.5896249413490295 Loss_D_B: 0.29466497898101807 Loss_G: 0.13944542407989502\n",
      "Epoch [2/10] Batch [11500/20655] Loss_D_A: 0.4150674343109131 Loss_D_B: 0.6534708738327026 Loss_G: 0.6561299562454224\n",
      "Epoch [2/10] Batch [11600/20655] Loss_D_A: 0.5543290376663208 Loss_D_B: 0.7222959995269775 Loss_G: 0.9212403893470764\n",
      "Epoch [2/10] Batch [11700/20655] Loss_D_A: 0.4550134539604187 Loss_D_B: 0.5558021664619446 Loss_G: 0.9505967497825623\n",
      "Epoch [2/10] Batch [11800/20655] Loss_D_A: 0.8873345851898193 Loss_D_B: 0.5737181305885315 Loss_G: 0.9868700504302979\n",
      "Epoch [2/10] Batch [11900/20655] Loss_D_A: 0.8025650978088379 Loss_D_B: 0.36338740587234497 Loss_G: 0.8418945074081421\n",
      "Epoch [2/10] Batch [12000/20655] Loss_D_A: 0.9646039009094238 Loss_D_B: 0.34068775177001953 Loss_G: 0.890462338924408\n",
      "Epoch [2/10] Batch [12100/20655] Loss_D_A: 0.48768454790115356 Loss_D_B: 0.73383629322052 Loss_G: 0.8532896041870117\n",
      "Epoch [2/10] Batch [12200/20655] Loss_D_A: 0.6601771712303162 Loss_D_B: 0.09293431043624878 Loss_G: 0.7159152626991272\n",
      "Epoch [2/10] Batch [12300/20655] Loss_D_A: 0.5708668828010559 Loss_D_B: 0.30332863330841064 Loss_G: 0.146775484085083\n",
      "Epoch [2/10] Batch [12400/20655] Loss_D_A: 0.16358345746994019 Loss_D_B: 0.5543052554130554 Loss_G: 0.8838863372802734\n",
      "Epoch [2/10] Batch [12500/20655] Loss_D_A: 0.08592355251312256 Loss_D_B: 0.7759065628051758 Loss_G: 0.7083126902580261\n",
      "Epoch [2/10] Batch [12600/20655] Loss_D_A: 0.6678874492645264 Loss_D_B: 0.09138154983520508 Loss_G: 0.8917746543884277\n",
      "Epoch [2/10] Batch [12700/20655] Loss_D_A: 0.9359009265899658 Loss_D_B: 0.6773377060890198 Loss_G: 0.6348371505737305\n",
      "Epoch [2/10] Batch [12800/20655] Loss_D_A: 0.5744068622589111 Loss_D_B: 0.9541693925857544 Loss_G: 0.8376616835594177\n",
      "Epoch [2/10] Batch [12900/20655] Loss_D_A: 0.3108198642730713 Loss_D_B: 0.8239808678627014 Loss_G: 0.32923394441604614\n",
      "Epoch [2/10] Batch [13000/20655] Loss_D_A: 0.8605515360832214 Loss_D_B: 0.7135795950889587 Loss_G: 0.6067991256713867\n",
      "Epoch [2/10] Batch [13100/20655] Loss_D_A: 0.45118778944015503 Loss_D_B: 0.7125029563903809 Loss_G: 0.5456094741821289\n",
      "Epoch [2/10] Batch [13200/20655] Loss_D_A: 0.03431439399719238 Loss_D_B: 0.44657230377197266 Loss_G: 0.5981271266937256\n",
      "Epoch [2/10] Batch [13300/20655] Loss_D_A: 0.9795416593551636 Loss_D_B: 0.6198017001152039 Loss_G: 0.30510175228118896\n",
      "Epoch [2/10] Batch [13400/20655] Loss_D_A: 0.08421117067337036 Loss_D_B: 0.2653590440750122 Loss_G: 0.07410621643066406\n",
      "Epoch [2/10] Batch [13500/20655] Loss_D_A: 0.2348821759223938 Loss_D_B: 0.6655278205871582 Loss_G: 0.9265468716621399\n",
      "Epoch [2/10] Batch [13600/20655] Loss_D_A: 0.8250410556793213 Loss_D_B: 0.788306474685669 Loss_G: 0.6249683499336243\n",
      "Epoch [2/10] Batch [13700/20655] Loss_D_A: 0.9966965913772583 Loss_D_B: 0.44885653257369995 Loss_G: 0.7177338600158691\n",
      "Epoch [2/10] Batch [13800/20655] Loss_D_A: 0.957323431968689 Loss_D_B: 0.7434685826301575 Loss_G: 0.7526511549949646\n",
      "Epoch [2/10] Batch [13900/20655] Loss_D_A: 0.6297218799591064 Loss_D_B: 0.5481374263763428 Loss_G: 0.9202126860618591\n",
      "Epoch [2/10] Batch [14000/20655] Loss_D_A: 0.9930901527404785 Loss_D_B: 0.35045260190963745 Loss_G: 0.28827232122421265\n",
      "Epoch [2/10] Batch [14100/20655] Loss_D_A: 0.3646509647369385 Loss_D_B: 0.9884834885597229 Loss_G: 0.7800061702728271\n",
      "Epoch [2/10] Batch [14200/20655] Loss_D_A: 0.5451503992080688 Loss_D_B: 0.34969794750213623 Loss_G: 0.39786458015441895\n",
      "Epoch [2/10] Batch [14300/20655] Loss_D_A: 0.7557267546653748 Loss_D_B: 0.7754920721054077 Loss_G: 0.5194011926651001\n",
      "Epoch [2/10] Batch [14400/20655] Loss_D_A: 0.979560136795044 Loss_D_B: 0.8446066379547119 Loss_G: 0.006263315677642822\n",
      "Epoch [2/10] Batch [14500/20655] Loss_D_A: 0.8039286136627197 Loss_D_B: 0.897107720375061 Loss_G: 0.6405924558639526\n",
      "Epoch [2/10] Batch [14600/20655] Loss_D_A: 0.8001195788383484 Loss_D_B: 0.3239181637763977 Loss_G: 0.5714224576950073\n",
      "Epoch [2/10] Batch [14700/20655] Loss_D_A: 0.5109656453132629 Loss_D_B: 0.7868915796279907 Loss_G: 0.3930724263191223\n",
      "Epoch [2/10] Batch [14800/20655] Loss_D_A: 0.004982888698577881 Loss_D_B: 0.2723467946052551 Loss_G: 0.1394944190979004\n",
      "Epoch [2/10] Batch [14900/20655] Loss_D_A: 0.8760177493095398 Loss_D_B: 0.053234755992889404 Loss_G: 0.5747925043106079\n",
      "Epoch [2/10] Batch [15000/20655] Loss_D_A: 0.9814034700393677 Loss_D_B: 0.8200410604476929 Loss_G: 0.026601016521453857\n",
      "Epoch [2/10] Batch [15100/20655] Loss_D_A: 0.1476854681968689 Loss_D_B: 0.9923288226127625 Loss_G: 0.7073015570640564\n",
      "Epoch [2/10] Batch [15200/20655] Loss_D_A: 0.05832207202911377 Loss_D_B: 0.4503788352012634 Loss_G: 0.2147923707962036\n",
      "Epoch [2/10] Batch [15300/20655] Loss_D_A: 0.6041328310966492 Loss_D_B: 0.6384096741676331 Loss_G: 0.8229985237121582\n",
      "Epoch [2/10] Batch [15400/20655] Loss_D_A: 0.7362752556800842 Loss_D_B: 0.8652611970901489 Loss_G: 0.9203786849975586\n",
      "Epoch [2/10] Batch [15500/20655] Loss_D_A: 0.3260536789894104 Loss_D_B: 0.7898556590080261 Loss_G: 0.002268970012664795\n",
      "Epoch [2/10] Batch [15600/20655] Loss_D_A: 0.8879975080490112 Loss_D_B: 0.17163598537445068 Loss_G: 0.6145638227462769\n",
      "Epoch [2/10] Batch [15700/20655] Loss_D_A: 0.07977843284606934 Loss_D_B: 0.20282965898513794 Loss_G: 0.2056795358657837\n",
      "Epoch [2/10] Batch [15800/20655] Loss_D_A: 0.2672664523124695 Loss_D_B: 0.7826970815658569 Loss_G: 0.5713065266609192\n",
      "Epoch [2/10] Batch [15900/20655] Loss_D_A: 0.5625942945480347 Loss_D_B: 0.6063162684440613 Loss_G: 0.49891197681427\n",
      "Epoch [2/10] Batch [16000/20655] Loss_D_A: 0.9805948734283447 Loss_D_B: 0.73337721824646 Loss_G: 0.23181378841400146\n",
      "Epoch [2/10] Batch [16100/20655] Loss_D_A: 0.8876813650131226 Loss_D_B: 0.8621514439582825 Loss_G: 0.9289793968200684\n",
      "Epoch [2/10] Batch [16200/20655] Loss_D_A: 0.15099143981933594 Loss_D_B: 0.8851680755615234 Loss_G: 0.9603981971740723\n",
      "Epoch [2/10] Batch [16300/20655] Loss_D_A: 0.7209442853927612 Loss_D_B: 0.9346157908439636 Loss_G: 0.6403429508209229\n",
      "Epoch [2/10] Batch [16400/20655] Loss_D_A: 0.15276110172271729 Loss_D_B: 0.2255192995071411 Loss_G: 0.025188803672790527\n",
      "Epoch [2/10] Batch [16500/20655] Loss_D_A: 0.8747491240501404 Loss_D_B: 0.14521610736846924 Loss_G: 0.27880996465682983\n",
      "Epoch [2/10] Batch [16600/20655] Loss_D_A: 0.07010853290557861 Loss_D_B: 0.1276450753211975 Loss_G: 0.9804282188415527\n",
      "Epoch [2/10] Batch [16700/20655] Loss_D_A: 0.5052129626274109 Loss_D_B: 0.8941119313240051 Loss_G: 0.8890979290008545\n",
      "Epoch [2/10] Batch [16800/20655] Loss_D_A: 0.9484012126922607 Loss_D_B: 0.9549014568328857 Loss_G: 0.44235384464263916\n",
      "Epoch [2/10] Batch [16900/20655] Loss_D_A: 0.10730278491973877 Loss_D_B: 0.18305712938308716 Loss_G: 0.825669527053833\n",
      "Epoch [2/10] Batch [17000/20655] Loss_D_A: 0.5989431738853455 Loss_D_B: 0.9297218322753906 Loss_G: 0.5973167419433594\n",
      "Epoch [2/10] Batch [17100/20655] Loss_D_A: 0.6117850542068481 Loss_D_B: 0.23227179050445557 Loss_G: 0.21244299411773682\n",
      "Epoch [2/10] Batch [17200/20655] Loss_D_A: 0.3658254146575928 Loss_D_B: 0.9799185395240784 Loss_G: 0.27273160219192505\n",
      "Epoch [2/10] Batch [17300/20655] Loss_D_A: 0.48031967878341675 Loss_D_B: 0.6354437470436096 Loss_G: 0.7595002055168152\n",
      "Epoch [2/10] Batch [17400/20655] Loss_D_A: 0.9116165041923523 Loss_D_B: 0.8405988216400146 Loss_G: 0.4074053168296814\n",
      "Epoch [2/10] Batch [17500/20655] Loss_D_A: 0.1800229549407959 Loss_D_B: 0.7997214198112488 Loss_G: 0.571855366230011\n",
      "Epoch [2/10] Batch [17600/20655] Loss_D_A: 0.4092543125152588 Loss_D_B: 0.677635669708252 Loss_G: 0.056467413902282715\n",
      "Epoch [2/10] Batch [17700/20655] Loss_D_A: 0.5747666358947754 Loss_D_B: 0.9785167574882507 Loss_G: 0.8037108182907104\n",
      "Epoch [2/10] Batch [17800/20655] Loss_D_A: 0.6138717532157898 Loss_D_B: 0.6295905709266663 Loss_G: 0.01951056718826294\n",
      "Epoch [2/10] Batch [17900/20655] Loss_D_A: 0.4712035059928894 Loss_D_B: 0.7124362587928772 Loss_G: 0.8082141280174255\n",
      "Epoch [2/10] Batch [18000/20655] Loss_D_A: 0.9147565960884094 Loss_D_B: 0.12429803609848022 Loss_G: 0.36994045972824097\n",
      "Epoch [2/10] Batch [18100/20655] Loss_D_A: 0.8970602750778198 Loss_D_B: 0.05311161279678345 Loss_G: 0.8773424625396729\n",
      "Epoch [2/10] Batch [18200/20655] Loss_D_A: 0.5129860639572144 Loss_D_B: 0.3084688186645508 Loss_G: 0.5018258690834045\n",
      "Epoch [2/10] Batch [18300/20655] Loss_D_A: 0.39930564165115356 Loss_D_B: 0.1451740860939026 Loss_G: 0.7976133823394775\n",
      "Epoch [2/10] Batch [18400/20655] Loss_D_A: 0.18275117874145508 Loss_D_B: 0.5591335892677307 Loss_G: 0.8354593515396118\n",
      "Epoch [2/10] Batch [18500/20655] Loss_D_A: 0.2926216721534729 Loss_D_B: 0.6522557735443115 Loss_G: 0.7102924585342407\n",
      "Epoch [2/10] Batch [18600/20655] Loss_D_A: 0.8457865715026855 Loss_D_B: 0.22269922494888306 Loss_G: 0.559691309928894\n",
      "Epoch [2/10] Batch [18700/20655] Loss_D_A: 0.8161323070526123 Loss_D_B: 0.8275678157806396 Loss_G: 0.007140755653381348\n",
      "Epoch [2/10] Batch [18800/20655] Loss_D_A: 0.3916333317756653 Loss_D_B: 0.9140618443489075 Loss_G: 0.9384208917617798\n",
      "Epoch [2/10] Batch [18900/20655] Loss_D_A: 0.003628969192504883 Loss_D_B: 0.3695061206817627 Loss_G: 0.970754861831665\n",
      "Epoch [2/10] Batch [19000/20655] Loss_D_A: 0.17658603191375732 Loss_D_B: 0.4902997612953186 Loss_G: 0.7608669996261597\n",
      "Epoch [2/10] Batch [19100/20655] Loss_D_A: 0.07376831769943237 Loss_D_B: 0.27990221977233887 Loss_G: 0.50223708152771\n",
      "Epoch [2/10] Batch [19200/20655] Loss_D_A: 0.5435195565223694 Loss_D_B: 0.9329962730407715 Loss_G: 0.06045675277709961\n",
      "Epoch [2/10] Batch [19300/20655] Loss_D_A: 0.10428673028945923 Loss_D_B: 0.47163915634155273 Loss_G: 0.6690843105316162\n",
      "Epoch [2/10] Batch [19400/20655] Loss_D_A: 0.0870363712310791 Loss_D_B: 0.6103712916374207 Loss_G: 0.46845030784606934\n",
      "Epoch [2/10] Batch [19500/20655] Loss_D_A: 0.2965371012687683 Loss_D_B: 0.21980124711990356 Loss_G: 0.9896826148033142\n",
      "Epoch [2/10] Batch [19600/20655] Loss_D_A: 0.6570804715156555 Loss_D_B: 0.8265746235847473 Loss_G: 0.41849958896636963\n",
      "Epoch [2/10] Batch [19700/20655] Loss_D_A: 0.052162885665893555 Loss_D_B: 0.644166886806488 Loss_G: 0.7277194857597351\n",
      "Epoch [2/10] Batch [19800/20655] Loss_D_A: 0.05047738552093506 Loss_D_B: 0.02274078130722046 Loss_G: 0.764367401599884\n",
      "Epoch [2/10] Batch [19900/20655] Loss_D_A: 0.8023392558097839 Loss_D_B: 0.1353563666343689 Loss_G: 0.5770720839500427\n",
      "Epoch [2/10] Batch [20000/20655] Loss_D_A: 0.7182803750038147 Loss_D_B: 0.47830355167388916 Loss_G: 0.4182792901992798\n",
      "Epoch [2/10] Batch [20100/20655] Loss_D_A: 0.3919216990470886 Loss_D_B: 0.4168117046356201 Loss_G: 0.752848744392395\n",
      "Epoch [2/10] Batch [20200/20655] Loss_D_A: 0.3842182159423828 Loss_D_B: 0.8063083291053772 Loss_G: 0.5035591125488281\n",
      "Epoch [2/10] Batch [20300/20655] Loss_D_A: 0.6400156617164612 Loss_D_B: 0.5542289018630981 Loss_G: 0.4415667653083801\n",
      "Epoch [2/10] Batch [20400/20655] Loss_D_A: 0.9072223901748657 Loss_D_B: 0.9273411631584167 Loss_G: 0.5211394429206848\n",
      "Epoch [2/10] Batch [20500/20655] Loss_D_A: 0.8819430470466614 Loss_D_B: 0.6068019270896912 Loss_G: 0.507337749004364\n",
      "Epoch [2/10] Batch [20600/20655] Loss_D_A: 0.6502988338470459 Loss_D_B: 0.6852486729621887 Loss_G: 0.8309945464134216\n",
      "Epoch [3/10] Batch [0/20655] Loss_D_A: 0.4786849617958069 Loss_D_B: 0.38527894020080566 Loss_G: 0.1476733684539795\n",
      "Epoch [3/10] Batch [0/20655] Loss_D_A: 0.6684178113937378 Loss_D_B: 0.5650300979614258 Loss_G: 0.45536452531814575\n",
      "Epoch [3/10] Batch [100/20655] Loss_D_A: 0.706542432308197 Loss_D_B: 0.07956749200820923 Loss_G: 0.5478875041007996\n",
      "Epoch [3/10] Batch [200/20655] Loss_D_A: 0.0006375908851623535 Loss_D_B: 0.3649064898490906 Loss_G: 0.012314140796661377\n",
      "Epoch [3/10] Batch [300/20655] Loss_D_A: 0.667737603187561 Loss_D_B: 0.3926522135734558 Loss_G: 0.7007004022598267\n",
      "Epoch [3/10] Batch [400/20655] Loss_D_A: 0.9982161521911621 Loss_D_B: 0.07800799608230591 Loss_G: 0.9249525666236877\n",
      "Epoch [3/10] Batch [500/20655] Loss_D_A: 0.7400302886962891 Loss_D_B: 0.2918834090232849 Loss_G: 0.20700055360794067\n",
      "Epoch [3/10] Batch [600/20655] Loss_D_A: 0.856392502784729 Loss_D_B: 0.2627304196357727 Loss_G: 0.9673241972923279\n",
      "Epoch [3/10] Batch [700/20655] Loss_D_A: 0.7879533171653748 Loss_D_B: 0.8329364657402039 Loss_G: 0.6016625165939331\n",
      "Epoch [3/10] Batch [800/20655] Loss_D_A: 0.5151711106300354 Loss_D_B: 0.5179855823516846 Loss_G: 0.11569446325302124\n",
      "Epoch [3/10] Batch [900/20655] Loss_D_A: 0.9615921378135681 Loss_D_B: 0.8060293793678284 Loss_G: 0.4798813462257385\n",
      "Epoch [3/10] Batch [1000/20655] Loss_D_A: 0.040910959243774414 Loss_D_B: 0.7350507378578186 Loss_G: 0.6706379055976868\n",
      "Epoch [3/10] Batch [1100/20655] Loss_D_A: 0.7079675197601318 Loss_D_B: 0.9575260281562805 Loss_G: 0.38324111700057983\n",
      "Epoch [3/10] Batch [1200/20655] Loss_D_A: 0.887790858745575 Loss_D_B: 0.6385529041290283 Loss_G: 0.0334622859954834\n",
      "Epoch [3/10] Batch [1300/20655] Loss_D_A: 0.1632477045059204 Loss_D_B: 0.36021435260772705 Loss_G: 0.6852384209632874\n",
      "Epoch [3/10] Batch [1400/20655] Loss_D_A: 0.4004027843475342 Loss_D_B: 0.821113109588623 Loss_G: 0.02451246976852417\n",
      "Epoch [3/10] Batch [1500/20655] Loss_D_A: 0.330805242061615 Loss_D_B: 0.20221519470214844 Loss_G: 0.1324271559715271\n",
      "Epoch [3/10] Batch [1600/20655] Loss_D_A: 0.3499460220336914 Loss_D_B: 0.422967791557312 Loss_G: 0.3110808730125427\n",
      "Epoch [3/10] Batch [1700/20655] Loss_D_A: 0.12118339538574219 Loss_D_B: 0.3788195252418518 Loss_G: 0.5120205879211426\n",
      "Epoch [3/10] Batch [1800/20655] Loss_D_A: 0.17535769939422607 Loss_D_B: 0.5056334733963013 Loss_G: 0.22095483541488647\n",
      "Epoch [3/10] Batch [1900/20655] Loss_D_A: 0.7614491581916809 Loss_D_B: 0.4006270170211792 Loss_G: 0.5445399284362793\n",
      "Epoch [3/10] Batch [2000/20655] Loss_D_A: 0.2536889910697937 Loss_D_B: 0.754713773727417 Loss_G: 0.7589771151542664\n",
      "Epoch [3/10] Batch [2100/20655] Loss_D_A: 0.22161656618118286 Loss_D_B: 0.45915329456329346 Loss_G: 0.626435399055481\n",
      "Epoch [3/10] Batch [2200/20655] Loss_D_A: 0.28046154975891113 Loss_D_B: 0.5127541422843933 Loss_G: 0.6626957058906555\n",
      "Epoch [3/10] Batch [2300/20655] Loss_D_A: 0.5670920610427856 Loss_D_B: 0.9849050641059875 Loss_G: 0.7700129747390747\n",
      "Epoch [3/10] Batch [2400/20655] Loss_D_A: 0.2884494662284851 Loss_D_B: 0.2746947407722473 Loss_G: 0.06366527080535889\n",
      "Epoch [3/10] Batch [2500/20655] Loss_D_A: 0.559994101524353 Loss_D_B: 0.18508970737457275 Loss_G: 0.5587236285209656\n",
      "Epoch [3/10] Batch [2600/20655] Loss_D_A: 0.09000980854034424 Loss_D_B: 0.48049622774124146 Loss_G: 0.18175047636032104\n",
      "Epoch [3/10] Batch [2700/20655] Loss_D_A: 0.47528934478759766 Loss_D_B: 0.18783122301101685 Loss_G: 0.5365433692932129\n",
      "Epoch [3/10] Batch [2800/20655] Loss_D_A: 0.972233235836029 Loss_D_B: 0.8058432340621948 Loss_G: 0.8442159295082092\n",
      "Epoch [3/10] Batch [2900/20655] Loss_D_A: 0.4970129728317261 Loss_D_B: 0.21285569667816162 Loss_G: 0.2568574547767639\n",
      "Epoch [3/10] Batch [3000/20655] Loss_D_A: 0.20107656717300415 Loss_D_B: 0.5186145901679993 Loss_G: 0.7493124604225159\n",
      "Epoch [3/10] Batch [3100/20655] Loss_D_A: 0.7408700585365295 Loss_D_B: 0.6091442108154297 Loss_G: 0.4169906973838806\n",
      "Epoch [3/10] Batch [3200/20655] Loss_D_A: 0.40975844860076904 Loss_D_B: 0.8575526475906372 Loss_G: 0.5504319667816162\n",
      "Epoch [3/10] Batch [3300/20655] Loss_D_A: 0.27035558223724365 Loss_D_B: 0.20501357316970825 Loss_G: 0.18286126852035522\n",
      "Epoch [3/10] Batch [3400/20655] Loss_D_A: 0.5978090763092041 Loss_D_B: 0.7366914749145508 Loss_G: 0.963446319103241\n",
      "Epoch [3/10] Batch [3500/20655] Loss_D_A: 0.6926285028457642 Loss_D_B: 0.7493579983711243 Loss_G: 0.0013312101364135742\n",
      "Epoch [3/10] Batch [3600/20655] Loss_D_A: 0.8475562930107117 Loss_D_B: 0.07435822486877441 Loss_G: 0.4147796034812927\n",
      "Epoch [3/10] Batch [3700/20655] Loss_D_A: 0.0641602873802185 Loss_D_B: 0.17802494764328003 Loss_G: 0.45875197649002075\n",
      "Epoch [3/10] Batch [3800/20655] Loss_D_A: 0.4218866229057312 Loss_D_B: 0.29929131269454956 Loss_G: 0.16502028703689575\n",
      "Epoch [3/10] Batch [3900/20655] Loss_D_A: 0.7485198378562927 Loss_D_B: 0.5707905888557434 Loss_G: 0.6618512868881226\n",
      "Epoch [3/10] Batch [4000/20655] Loss_D_A: 0.7555838227272034 Loss_D_B: 0.4091634154319763 Loss_G: 0.16388750076293945\n",
      "Epoch [3/10] Batch [4100/20655] Loss_D_A: 0.7418325543403625 Loss_D_B: 0.22498339414596558 Loss_G: 0.2832111120223999\n",
      "Epoch [3/10] Batch [4200/20655] Loss_D_A: 0.33927446603775024 Loss_D_B: 0.7631993293762207 Loss_G: 0.26632076501846313\n",
      "Epoch [3/10] Batch [4300/20655] Loss_D_A: 0.9031949639320374 Loss_D_B: 0.9608855247497559 Loss_G: 0.7775113582611084\n",
      "Epoch [3/10] Batch [4400/20655] Loss_D_A: 0.6494506597518921 Loss_D_B: 0.9038603901863098 Loss_G: 0.38426148891448975\n",
      "Epoch [3/10] Batch [4500/20655] Loss_D_A: 0.020754456520080566 Loss_D_B: 0.9261493682861328 Loss_G: 0.8038371205329895\n",
      "Epoch [3/10] Batch [4600/20655] Loss_D_A: 0.3488194942474365 Loss_D_B: 0.4860103130340576 Loss_G: 0.4635680913925171\n",
      "Epoch [3/10] Batch [4700/20655] Loss_D_A: 0.05051243305206299 Loss_D_B: 0.7684528231620789 Loss_G: 0.9527717232704163\n",
      "Epoch [3/10] Batch [4800/20655] Loss_D_A: 0.21149247884750366 Loss_D_B: 0.5379077792167664 Loss_G: 0.352267324924469\n",
      "Epoch [3/10] Batch [4900/20655] Loss_D_A: 0.8898524045944214 Loss_D_B: 0.9349336624145508 Loss_G: 0.8246707320213318\n",
      "Epoch [3/10] Batch [5000/20655] Loss_D_A: 0.7892211079597473 Loss_D_B: 0.7351825833320618 Loss_G: 0.5553559064865112\n",
      "Epoch [3/10] Batch [5100/20655] Loss_D_A: 0.239111065864563 Loss_D_B: 0.5213905572891235 Loss_G: 0.5401889681816101\n",
      "Epoch [3/10] Batch [5200/20655] Loss_D_A: 0.6026743054389954 Loss_D_B: 0.8946364521980286 Loss_G: 0.8362765312194824\n",
      "Epoch [3/10] Batch [5300/20655] Loss_D_A: 0.6358289122581482 Loss_D_B: 0.44316333532333374 Loss_G: 0.6163645386695862\n",
      "Epoch [3/10] Batch [5400/20655] Loss_D_A: 0.9087356328964233 Loss_D_B: 0.834475040435791 Loss_G: 0.1709999442100525\n",
      "Epoch [3/10] Batch [5500/20655] Loss_D_A: 0.18904566764831543 Loss_D_B: 0.5494726896286011 Loss_G: 0.5672337412834167\n",
      "Epoch [3/10] Batch [5600/20655] Loss_D_A: 0.8418329358100891 Loss_D_B: 0.08163303136825562 Loss_G: 0.7542058825492859\n",
      "Epoch [3/10] Batch [5700/20655] Loss_D_A: 0.7172260880470276 Loss_D_B: 0.23674649000167847 Loss_G: 0.9267139434814453\n",
      "Epoch [3/10] Batch [5800/20655] Loss_D_A: 0.9536855816841125 Loss_D_B: 0.8080724477767944 Loss_G: 0.2783162593841553\n",
      "Epoch [3/10] Batch [5900/20655] Loss_D_A: 0.5411341786384583 Loss_D_B: 0.4675101637840271 Loss_G: 0.03457033634185791\n",
      "Epoch [3/10] Batch [6000/20655] Loss_D_A: 0.5260536670684814 Loss_D_B: 0.8661239147186279 Loss_G: 0.45435458421707153\n",
      "Epoch [3/10] Batch [6100/20655] Loss_D_A: 0.32485663890838623 Loss_D_B: 0.5236601829528809 Loss_G: 0.4786330461502075\n",
      "Epoch [3/10] Batch [6200/20655] Loss_D_A: 0.8566667437553406 Loss_D_B: 0.9285097122192383 Loss_G: 0.8516771197319031\n",
      "Epoch [3/10] Batch [6300/20655] Loss_D_A: 0.22945433855056763 Loss_D_B: 0.3077118396759033 Loss_G: 0.6182387471199036\n",
      "Epoch [3/10] Batch [6400/20655] Loss_D_A: 0.39131349325180054 Loss_D_B: 0.7389870882034302 Loss_G: 0.556007444858551\n",
      "Epoch [3/10] Batch [6500/20655] Loss_D_A: 0.8665238618850708 Loss_D_B: 0.10582822561264038 Loss_G: 0.5215173363685608\n",
      "Epoch [3/10] Batch [6600/20655] Loss_D_A: 0.00888282060623169 Loss_D_B: 0.5278878211975098 Loss_G: 0.44775503873825073\n",
      "Epoch [3/10] Batch [6700/20655] Loss_D_A: 0.906123161315918 Loss_D_B: 0.9877405166625977 Loss_G: 0.4075416326522827\n",
      "Epoch [3/10] Batch [6800/20655] Loss_D_A: 0.9225991368293762 Loss_D_B: 0.5206970572471619 Loss_G: 0.18365269899368286\n",
      "Epoch [3/10] Batch [6900/20655] Loss_D_A: 0.1233055591583252 Loss_D_B: 0.9859618544578552 Loss_G: 0.7526807188987732\n",
      "Epoch [3/10] Batch [7000/20655] Loss_D_A: 0.8511654138565063 Loss_D_B: 0.017841577529907227 Loss_G: 0.08767038583755493\n",
      "Epoch [3/10] Batch [7100/20655] Loss_D_A: 0.8391834497451782 Loss_D_B: 0.050397276878356934 Loss_G: 0.6513317227363586\n",
      "Epoch [3/10] Batch [7200/20655] Loss_D_A: 0.11633789539337158 Loss_D_B: 0.40043026208877563 Loss_G: 0.015596508979797363\n",
      "Epoch [3/10] Batch [7300/20655] Loss_D_A: 0.2888781428337097 Loss_D_B: 0.8100761771202087 Loss_G: 0.05269986391067505\n",
      "Epoch [3/10] Batch [7400/20655] Loss_D_A: 0.26434725522994995 Loss_D_B: 0.44108957052230835 Loss_G: 0.03795403242111206\n",
      "Epoch [3/10] Batch [7500/20655] Loss_D_A: 0.007301986217498779 Loss_D_B: 0.780182421207428 Loss_G: 0.9934223294258118\n",
      "Epoch [3/10] Batch [7600/20655] Loss_D_A: 0.9062122702598572 Loss_D_B: 0.6214838624000549 Loss_G: 0.7104557752609253\n",
      "Epoch [3/10] Batch [7700/20655] Loss_D_A: 0.8251702189445496 Loss_D_B: 0.9867967963218689 Loss_G: 0.09210729598999023\n",
      "Epoch [3/10] Batch [7800/20655] Loss_D_A: 0.3914983868598938 Loss_D_B: 0.7344932556152344 Loss_G: 0.26649540662765503\n",
      "Epoch [3/10] Batch [7900/20655] Loss_D_A: 0.5571971535682678 Loss_D_B: 0.7672218680381775 Loss_G: 0.03867340087890625\n",
      "Epoch [3/10] Batch [8000/20655] Loss_D_A: 0.1154211163520813 Loss_D_B: 0.5451864004135132 Loss_G: 0.8585699200630188\n",
      "Epoch [3/10] Batch [8100/20655] Loss_D_A: 0.926389753818512 Loss_D_B: 0.8401988744735718 Loss_G: 0.9263110160827637\n",
      "Epoch [3/10] Batch [8200/20655] Loss_D_A: 0.8425532579421997 Loss_D_B: 0.10273563861846924 Loss_G: 0.5330031514167786\n",
      "Epoch [3/10] Batch [8300/20655] Loss_D_A: 0.6997575163841248 Loss_D_B: 0.3702406883239746 Loss_G: 0.8917922377586365\n",
      "Epoch [3/10] Batch [8400/20655] Loss_D_A: 0.8777628540992737 Loss_D_B: 0.4128039479255676 Loss_G: 0.6420674324035645\n",
      "Epoch [3/10] Batch [8500/20655] Loss_D_A: 0.4109925627708435 Loss_D_B: 0.3663157820701599 Loss_G: 0.8527700901031494\n",
      "Epoch [3/10] Batch [8600/20655] Loss_D_A: 0.5248976349830627 Loss_D_B: 0.7353991866111755 Loss_G: 0.3305521011352539\n",
      "Epoch [3/10] Batch [8700/20655] Loss_D_A: 0.6775858998298645 Loss_D_B: 0.7045485973358154 Loss_G: 0.9190382957458496\n",
      "Epoch [3/10] Batch [8800/20655] Loss_D_A: 0.39772677421569824 Loss_D_B: 0.40060293674468994 Loss_G: 0.6819062829017639\n",
      "Epoch [3/10] Batch [8900/20655] Loss_D_A: 0.05333369970321655 Loss_D_B: 0.3650602698326111 Loss_G: 0.6972772479057312\n",
      "Epoch [3/10] Batch [9000/20655] Loss_D_A: 0.6915894150733948 Loss_D_B: 0.17661088705062866 Loss_G: 0.6474692225456238\n",
      "Epoch [3/10] Batch [9100/20655] Loss_D_A: 0.3666338324546814 Loss_D_B: 0.8985226154327393 Loss_G: 0.06767767667770386\n",
      "Epoch [3/10] Batch [9200/20655] Loss_D_A: 0.823373019695282 Loss_D_B: 0.9147257208824158 Loss_G: 0.09226810932159424\n",
      "Epoch [3/10] Batch [9300/20655] Loss_D_A: 0.02066481113433838 Loss_D_B: 0.8915554881095886 Loss_G: 0.7200446724891663\n",
      "Epoch [3/10] Batch [9400/20655] Loss_D_A: 0.49514931440353394 Loss_D_B: 0.6255780458450317 Loss_G: 0.6418060660362244\n",
      "Epoch [3/10] Batch [9500/20655] Loss_D_A: 0.15856397151947021 Loss_D_B: 0.8354001045227051 Loss_G: 0.20772939920425415\n",
      "Epoch [3/10] Batch [9600/20655] Loss_D_A: 0.16991084814071655 Loss_D_B: 0.9296197295188904 Loss_G: 0.009512841701507568\n",
      "Epoch [3/10] Batch [9700/20655] Loss_D_A: 0.6696135997772217 Loss_D_B: 0.07343780994415283 Loss_G: 0.45586520433425903\n",
      "Epoch [3/10] Batch [9800/20655] Loss_D_A: 0.6802694201469421 Loss_D_B: 0.930448055267334 Loss_G: 0.5282585024833679\n",
      "Epoch [3/10] Batch [9900/20655] Loss_D_A: 0.6437282562255859 Loss_D_B: 0.16625940799713135 Loss_G: 0.24805378913879395\n",
      "Epoch [3/10] Batch [10000/20655] Loss_D_A: 0.01860898733139038 Loss_D_B: 0.40632593631744385 Loss_G: 0.7080397009849548\n",
      "Epoch [3/10] Batch [10100/20655] Loss_D_A: 0.5606809854507446 Loss_D_B: 0.7039324045181274 Loss_G: 0.7928028106689453\n",
      "Epoch [3/10] Batch [10200/20655] Loss_D_A: 0.13497275114059448 Loss_D_B: 0.764428973197937 Loss_G: 0.48680585622787476\n",
      "Epoch [3/10] Batch [10300/20655] Loss_D_A: 0.939104437828064 Loss_D_B: 0.4204556941986084 Loss_G: 0.5973321795463562\n",
      "Epoch [3/10] Batch [10400/20655] Loss_D_A: 0.11676418781280518 Loss_D_B: 0.5214430093765259 Loss_G: 0.05497241020202637\n",
      "Epoch [3/10] Batch [10500/20655] Loss_D_A: 0.3653908967971802 Loss_D_B: 0.6042929887771606 Loss_G: 0.25646907091140747\n",
      "Epoch [3/10] Batch [10600/20655] Loss_D_A: 0.8058680891990662 Loss_D_B: 0.34397822618484497 Loss_G: 0.24626177549362183\n",
      "Epoch [3/10] Batch [10700/20655] Loss_D_A: 0.4827660918235779 Loss_D_B: 0.7661635875701904 Loss_G: 0.7404594421386719\n",
      "Epoch [3/10] Batch [10800/20655] Loss_D_A: 0.9932795166969299 Loss_D_B: 0.18610501289367676 Loss_G: 0.45587438344955444\n",
      "Epoch [3/10] Batch [10900/20655] Loss_D_A: 0.37014055252075195 Loss_D_B: 0.1408584713935852 Loss_G: 0.1563408374786377\n",
      "Epoch [3/10] Batch [11000/20655] Loss_D_A: 0.35792768001556396 Loss_D_B: 0.7135338187217712 Loss_G: 0.8432092666625977\n",
      "Epoch [3/10] Batch [11100/20655] Loss_D_A: 0.26961594820022583 Loss_D_B: 0.6342890858650208 Loss_G: 0.8666809797286987\n",
      "Epoch [3/10] Batch [11200/20655] Loss_D_A: 0.7465539574623108 Loss_D_B: 0.8815253973007202 Loss_G: 0.08354240655899048\n",
      "Epoch [3/10] Batch [11300/20655] Loss_D_A: 0.2340502142906189 Loss_D_B: 0.18461602926254272 Loss_G: 0.43990665674209595\n",
      "Epoch [3/10] Batch [11400/20655] Loss_D_A: 0.7508828043937683 Loss_D_B: 0.6886367201805115 Loss_G: 0.9996663331985474\n",
      "Epoch [3/10] Batch [11500/20655] Loss_D_A: 0.2729336619377136 Loss_D_B: 0.4376012086868286 Loss_G: 0.08884716033935547\n",
      "Epoch [3/10] Batch [11600/20655] Loss_D_A: 0.10076701641082764 Loss_D_B: 0.46031421422958374 Loss_G: 0.03539341688156128\n",
      "Epoch [3/10] Batch [11700/20655] Loss_D_A: 0.8359061479568481 Loss_D_B: 0.24915021657943726 Loss_G: 0.9497917294502258\n",
      "Epoch [3/10] Batch [11800/20655] Loss_D_A: 0.836776077747345 Loss_D_B: 0.7902460694313049 Loss_G: 0.018726587295532227\n",
      "Epoch [3/10] Batch [11900/20655] Loss_D_A: 0.4761766791343689 Loss_D_B: 0.497164249420166 Loss_G: 0.81937575340271\n",
      "Epoch [3/10] Batch [12000/20655] Loss_D_A: 0.4242662787437439 Loss_D_B: 0.6230338215827942 Loss_G: 0.23298245668411255\n",
      "Epoch [3/10] Batch [12100/20655] Loss_D_A: 0.8144927024841309 Loss_D_B: 0.9447068572044373 Loss_G: 0.31976401805877686\n",
      "Epoch [3/10] Batch [12200/20655] Loss_D_A: 0.41641390323638916 Loss_D_B: 0.7995790243148804 Loss_G: 0.475746750831604\n",
      "Epoch [3/10] Batch [12300/20655] Loss_D_A: 0.029109477996826172 Loss_D_B: 0.8053724765777588 Loss_G: 0.5927132368087769\n",
      "Epoch [3/10] Batch [12400/20655] Loss_D_A: 0.5454334616661072 Loss_D_B: 0.06178539991378784 Loss_G: 0.11656594276428223\n",
      "Epoch [3/10] Batch [12500/20655] Loss_D_A: 0.4079650044441223 Loss_D_B: 0.12309026718139648 Loss_G: 0.326757550239563\n",
      "Epoch [3/10] Batch [12600/20655] Loss_D_A: 0.2813720703125 Loss_D_B: 0.14921361207962036 Loss_G: 0.12669587135314941\n",
      "Epoch [3/10] Batch [12700/20655] Loss_D_A: 0.15817564725875854 Loss_D_B: 0.9869208335876465 Loss_G: 0.12096112966537476\n",
      "Epoch [3/10] Batch [12800/20655] Loss_D_A: 0.909970760345459 Loss_D_B: 0.7985886931419373 Loss_G: 0.03475981950759888\n",
      "Epoch [3/10] Batch [12900/20655] Loss_D_A: 0.06531727313995361 Loss_D_B: 0.12523126602172852 Loss_G: 0.9778395295143127\n",
      "Epoch [3/10] Batch [13000/20655] Loss_D_A: 0.30297422409057617 Loss_D_B: 0.05742126703262329 Loss_G: 0.2791362404823303\n",
      "Epoch [3/10] Batch [13100/20655] Loss_D_A: 0.5043159127235413 Loss_D_B: 0.34481149911880493 Loss_G: 0.9911945462226868\n",
      "Epoch [3/10] Batch [13200/20655] Loss_D_A: 0.9748834371566772 Loss_D_B: 0.4793452024459839 Loss_G: 0.9246010184288025\n",
      "Epoch [3/10] Batch [13300/20655] Loss_D_A: 0.5375000834465027 Loss_D_B: 0.26584553718566895 Loss_G: 0.3726988434791565\n",
      "Epoch [3/10] Batch [13400/20655] Loss_D_A: 0.9444599747657776 Loss_D_B: 0.25853967666625977 Loss_G: 0.05397707223892212\n",
      "Epoch [3/10] Batch [13500/20655] Loss_D_A: 0.15922415256500244 Loss_D_B: 0.602100133895874 Loss_G: 0.8432560563087463\n",
      "Epoch [3/10] Batch [13600/20655] Loss_D_A: 0.8666677474975586 Loss_D_B: 0.7369705438613892 Loss_G: 0.3213098645210266\n",
      "Epoch [3/10] Batch [13700/20655] Loss_D_A: 0.9562111496925354 Loss_D_B: 0.20556998252868652 Loss_G: 0.17132222652435303\n",
      "Epoch [3/10] Batch [13800/20655] Loss_D_A: 0.5736401081085205 Loss_D_B: 0.26319438219070435 Loss_G: 0.21122419834136963\n",
      "Epoch [3/10] Batch [13900/20655] Loss_D_A: 0.70074862241745 Loss_D_B: 0.0007349848747253418 Loss_G: 0.7808048725128174\n",
      "Epoch [3/10] Batch [14000/20655] Loss_D_A: 0.653501033782959 Loss_D_B: 0.07355856895446777 Loss_G: 0.7355693578720093\n",
      "Epoch [3/10] Batch [14100/20655] Loss_D_A: 0.13137418031692505 Loss_D_B: 0.43178772926330566 Loss_G: 0.3248014450073242\n",
      "Epoch [3/10] Batch [14200/20655] Loss_D_A: 0.9803135395050049 Loss_D_B: 0.9493037462234497 Loss_G: 0.2925989031791687\n",
      "Epoch [3/10] Batch [14300/20655] Loss_D_A: 0.49244576692581177 Loss_D_B: 0.7228524088859558 Loss_G: 0.3468606472015381\n",
      "Epoch [3/10] Batch [14400/20655] Loss_D_A: 0.17201173305511475 Loss_D_B: 0.6467150449752808 Loss_G: 0.21806693077087402\n",
      "Epoch [3/10] Batch [14500/20655] Loss_D_A: 0.2271333932876587 Loss_D_B: 0.16424721479415894 Loss_G: 0.7540909051895142\n",
      "Epoch [3/10] Batch [14600/20655] Loss_D_A: 0.19425225257873535 Loss_D_B: 0.43856143951416016 Loss_G: 0.027971863746643066\n",
      "Epoch [3/10] Batch [14700/20655] Loss_D_A: 0.6027387976646423 Loss_D_B: 0.8892533779144287 Loss_G: 0.6298272609710693\n",
      "Epoch [3/10] Batch [14800/20655] Loss_D_A: 0.2997325658798218 Loss_D_B: 0.34753549098968506 Loss_G: 0.28434449434280396\n",
      "Epoch [3/10] Batch [14900/20655] Loss_D_A: 0.072093665599823 Loss_D_B: 0.7838164567947388 Loss_G: 0.8219940066337585\n",
      "Epoch [3/10] Batch [15000/20655] Loss_D_A: 0.883475124835968 Loss_D_B: 0.008435070514678955 Loss_G: 0.973870038986206\n",
      "Epoch [3/10] Batch [15100/20655] Loss_D_A: 0.8579068183898926 Loss_D_B: 0.23448926210403442 Loss_G: 0.8726484775543213\n",
      "Epoch [3/10] Batch [15200/20655] Loss_D_A: 0.48698586225509644 Loss_D_B: 0.8137756586074829 Loss_G: 0.06966906785964966\n",
      "Epoch [3/10] Batch [15300/20655] Loss_D_A: 0.0592195987701416 Loss_D_B: 0.31683844327926636 Loss_G: 0.9634798169136047\n",
      "Epoch [3/10] Batch [15400/20655] Loss_D_A: 0.27825450897216797 Loss_D_B: 0.12234342098236084 Loss_G: 0.23541879653930664\n",
      "Epoch [3/10] Batch [15500/20655] Loss_D_A: 0.8836866021156311 Loss_D_B: 0.13611197471618652 Loss_G: 0.11680275201797485\n",
      "Epoch [3/10] Batch [15600/20655] Loss_D_A: 0.6407189965248108 Loss_D_B: 0.08079493045806885 Loss_G: 0.4561999440193176\n",
      "Epoch [3/10] Batch [15700/20655] Loss_D_A: 0.6375000476837158 Loss_D_B: 0.7133539915084839 Loss_G: 0.9405985474586487\n",
      "Epoch [3/10] Batch [15800/20655] Loss_D_A: 0.024754464626312256 Loss_D_B: 0.948482871055603 Loss_G: 0.360140860080719\n",
      "Epoch [3/10] Batch [15900/20655] Loss_D_A: 0.9638543725013733 Loss_D_B: 0.17069047689437866 Loss_G: 0.9563453197479248\n",
      "Epoch [3/10] Batch [16000/20655] Loss_D_A: 0.41184884309768677 Loss_D_B: 0.740220844745636 Loss_G: 0.05612534284591675\n",
      "Epoch [3/10] Batch [16100/20655] Loss_D_A: 0.5284289717674255 Loss_D_B: 0.9114536643028259 Loss_G: 0.316639244556427\n",
      "Epoch [3/10] Batch [16200/20655] Loss_D_A: 0.8173681497573853 Loss_D_B: 0.7433098554611206 Loss_G: 0.05192762613296509\n",
      "Epoch [3/10] Batch [16300/20655] Loss_D_A: 0.3130476474761963 Loss_D_B: 0.7783499956130981 Loss_G: 0.45314258337020874\n",
      "Epoch [3/10] Batch [16400/20655] Loss_D_A: 0.457760751247406 Loss_D_B: 0.2521117329597473 Loss_G: 0.37931549549102783\n",
      "Epoch [3/10] Batch [16500/20655] Loss_D_A: 0.8614937663078308 Loss_D_B: 0.6575002670288086 Loss_G: 0.173597514629364\n",
      "Epoch [3/10] Batch [16600/20655] Loss_D_A: 0.15701574087142944 Loss_D_B: 0.6718518137931824 Loss_G: 0.958503246307373\n",
      "Epoch [3/10] Batch [16700/20655] Loss_D_A: 0.5819066762924194 Loss_D_B: 0.7891470193862915 Loss_G: 0.6800987720489502\n",
      "Epoch [3/10] Batch [16800/20655] Loss_D_A: 0.8755974769592285 Loss_D_B: 0.6059885621070862 Loss_G: 0.2825503349304199\n",
      "Epoch [3/10] Batch [16900/20655] Loss_D_A: 0.667523205280304 Loss_D_B: 0.526360034942627 Loss_G: 0.37709444761276245\n",
      "Epoch [3/10] Batch [17000/20655] Loss_D_A: 0.13606232404708862 Loss_D_B: 0.6296384334564209 Loss_G: 0.05673450231552124\n",
      "Epoch [3/10] Batch [17100/20655] Loss_D_A: 0.5852344632148743 Loss_D_B: 0.8985074162483215 Loss_G: 0.21614158153533936\n",
      "Epoch [3/10] Batch [17200/20655] Loss_D_A: 0.38286399841308594 Loss_D_B: 0.1986609697341919 Loss_G: 0.7542939782142639\n",
      "Epoch [3/10] Batch [17300/20655] Loss_D_A: 0.13386082649230957 Loss_D_B: 0.5062324404716492 Loss_G: 0.040289878845214844\n",
      "Epoch [3/10] Batch [17400/20655] Loss_D_A: 0.666149914264679 Loss_D_B: 0.5454327464103699 Loss_G: 0.977200448513031\n",
      "Epoch [3/10] Batch [17500/20655] Loss_D_A: 0.5690092444419861 Loss_D_B: 0.2113126516342163 Loss_G: 0.02420032024383545\n",
      "Epoch [3/10] Batch [17600/20655] Loss_D_A: 0.22864192724227905 Loss_D_B: 0.5891868472099304 Loss_G: 0.932449996471405\n",
      "Epoch [3/10] Batch [17700/20655] Loss_D_A: 0.46729129552841187 Loss_D_B: 0.9134867191314697 Loss_G: 0.4278019666671753\n",
      "Epoch [3/10] Batch [17800/20655] Loss_D_A: 0.4346961975097656 Loss_D_B: 0.38032883405685425 Loss_G: 0.5786178112030029\n",
      "Epoch [3/10] Batch [17900/20655] Loss_D_A: 0.14653992652893066 Loss_D_B: 0.24929827451705933 Loss_G: 0.4837799072265625\n",
      "Epoch [3/10] Batch [18000/20655] Loss_D_A: 0.3815975785255432 Loss_D_B: 0.4522743225097656 Loss_G: 0.5718662142753601\n",
      "Epoch [3/10] Batch [18100/20655] Loss_D_A: 0.03992372751235962 Loss_D_B: 0.228349506855011 Loss_G: 0.7329647541046143\n",
      "Epoch [3/10] Batch [18200/20655] Loss_D_A: 0.34987127780914307 Loss_D_B: 0.9490044713020325 Loss_G: 0.39459240436553955\n",
      "Epoch [3/10] Batch [18300/20655] Loss_D_A: 0.9625856876373291 Loss_D_B: 0.8258780837059021 Loss_G: 0.7609599232673645\n",
      "Epoch [3/10] Batch [18400/20655] Loss_D_A: 0.12418866157531738 Loss_D_B: 0.5507351160049438 Loss_G: 0.08406972885131836\n",
      "Epoch [3/10] Batch [18500/20655] Loss_D_A: 0.6913632154464722 Loss_D_B: 0.1907188892364502 Loss_G: 0.31701505184173584\n",
      "Epoch [3/10] Batch [18600/20655] Loss_D_A: 0.6174010038375854 Loss_D_B: 0.6155417561531067 Loss_G: 0.3814714550971985\n",
      "Epoch [3/10] Batch [18700/20655] Loss_D_A: 0.20880097150802612 Loss_D_B: 0.2492240071296692 Loss_G: 0.3309236168861389\n",
      "Epoch [3/10] Batch [18800/20655] Loss_D_A: 0.6799138784408569 Loss_D_B: 0.4069545865058899 Loss_G: 0.27127331495285034\n",
      "Epoch [3/10] Batch [18900/20655] Loss_D_A: 0.5192049145698547 Loss_D_B: 0.9693641662597656 Loss_G: 0.031949758529663086\n",
      "Epoch [3/10] Batch [19000/20655] Loss_D_A: 0.23195844888687134 Loss_D_B: 0.4098150134086609 Loss_G: 0.8600966930389404\n",
      "Epoch [3/10] Batch [19100/20655] Loss_D_A: 0.9025004506111145 Loss_D_B: 0.6623397469520569 Loss_G: 0.21873712539672852\n",
      "Epoch [3/10] Batch [19200/20655] Loss_D_A: 0.6414198875427246 Loss_D_B: 0.057316601276397705 Loss_G: 0.790895938873291\n",
      "Epoch [3/10] Batch [19300/20655] Loss_D_A: 0.2728598713874817 Loss_D_B: 0.8168182373046875 Loss_G: 0.6416487097740173\n",
      "Epoch [3/10] Batch [19400/20655] Loss_D_A: 0.5848331451416016 Loss_D_B: 0.03925752639770508 Loss_G: 0.8649008274078369\n",
      "Epoch [3/10] Batch [19500/20655] Loss_D_A: 0.06902587413787842 Loss_D_B: 0.416512131690979 Loss_G: 0.7895248532295227\n",
      "Epoch [3/10] Batch [19600/20655] Loss_D_A: 0.2396562695503235 Loss_D_B: 0.8773125410079956 Loss_G: 0.7283354997634888\n",
      "Epoch [3/10] Batch [19700/20655] Loss_D_A: 0.6795929074287415 Loss_D_B: 0.16343998908996582 Loss_G: 0.15654218196868896\n",
      "Epoch [3/10] Batch [19800/20655] Loss_D_A: 0.3919216990470886 Loss_D_B: 0.2122238278388977 Loss_G: 0.45249640941619873\n",
      "Epoch [3/10] Batch [19900/20655] Loss_D_A: 0.21402454376220703 Loss_D_B: 0.7818759679794312 Loss_G: 0.33863115310668945\n",
      "Epoch [3/10] Batch [20000/20655] Loss_D_A: 0.9631212949752808 Loss_D_B: 0.3803643584251404 Loss_G: 0.6658092141151428\n",
      "Epoch [3/10] Batch [20100/20655] Loss_D_A: 0.8332599997520447 Loss_D_B: 0.6362674236297607 Loss_G: 0.7955976128578186\n",
      "Epoch [3/10] Batch [20200/20655] Loss_D_A: 0.6698337197303772 Loss_D_B: 0.7980642318725586 Loss_G: 0.24051862955093384\n",
      "Epoch [3/10] Batch [20300/20655] Loss_D_A: 0.45776963233947754 Loss_D_B: 0.7304280400276184 Loss_G: 0.6532419323921204\n",
      "Epoch [3/10] Batch [20400/20655] Loss_D_A: 0.4424111247062683 Loss_D_B: 0.5032086968421936 Loss_G: 0.4723842740058899\n",
      "Epoch [3/10] Batch [20500/20655] Loss_D_A: 0.23754745721817017 Loss_D_B: 0.1332252025604248 Loss_G: 0.6134474873542786\n",
      "Epoch [3/10] Batch [20600/20655] Loss_D_A: 0.5145915746688843 Loss_D_B: 0.23915624618530273 Loss_G: 0.47842174768447876\n",
      "Epoch [4/10] Batch [0/20655] Loss_D_A: 0.6684178113937378 Loss_D_B: 0.5650300979614258 Loss_G: 0.45536452531814575\n",
      "Epoch [4/10] Batch [0/20655] Loss_D_A: 0.8354266881942749 Loss_D_B: 0.6545500159263611 Loss_G: 0.8789441585540771\n",
      "Epoch [4/10] Batch [100/20655] Loss_D_A: 0.8826759457588196 Loss_D_B: 0.6688177585601807 Loss_G: 0.9659186601638794\n",
      "Epoch [4/10] Batch [200/20655] Loss_D_A: 0.8915432691574097 Loss_D_B: 0.9398826360702515 Loss_G: 0.7016318440437317\n",
      "Epoch [4/10] Batch [300/20655] Loss_D_A: 0.714006245136261 Loss_D_B: 0.44739866256713867 Loss_G: 0.5133911371231079\n",
      "Epoch [4/10] Batch [400/20655] Loss_D_A: 0.7568256855010986 Loss_D_B: 0.1491626501083374 Loss_G: 0.042322754859924316\n",
      "Epoch [4/10] Batch [500/20655] Loss_D_A: 0.32742100954055786 Loss_D_B: 0.05536222457885742 Loss_G: 0.2489781379699707\n",
      "Epoch [4/10] Batch [600/20655] Loss_D_A: 0.08182156085968018 Loss_D_B: 0.2129509449005127 Loss_G: 0.32591259479522705\n",
      "Epoch [4/10] Batch [700/20655] Loss_D_A: 0.8264943957328796 Loss_D_B: 0.6673728227615356 Loss_G: 0.7299326062202454\n",
      "Epoch [4/10] Batch [800/20655] Loss_D_A: 0.9480674266815186 Loss_D_B: 0.34819847345352173 Loss_G: 0.5618151426315308\n",
      "Epoch [4/10] Batch [900/20655] Loss_D_A: 0.19631904363632202 Loss_D_B: 0.2191755175590515 Loss_G: 0.3703533411026001\n",
      "Epoch [4/10] Batch [1000/20655] Loss_D_A: 0.7786699533462524 Loss_D_B: 0.08257222175598145 Loss_G: 0.864092230796814\n",
      "Epoch [4/10] Batch [1100/20655] Loss_D_A: 0.6524500250816345 Loss_D_B: 0.2926923632621765 Loss_G: 0.1555265188217163\n",
      "Epoch [4/10] Batch [1200/20655] Loss_D_A: 0.21501535177230835 Loss_D_B: 0.2781651020050049 Loss_G: 0.5540946125984192\n",
      "Epoch [4/10] Batch [1300/20655] Loss_D_A: 0.47054243087768555 Loss_D_B: 0.29288309812545776 Loss_G: 0.6233595013618469\n",
      "Epoch [4/10] Batch [1400/20655] Loss_D_A: 0.44461798667907715 Loss_D_B: 0.7335597276687622 Loss_G: 0.9996731877326965\n",
      "Epoch [4/10] Batch [1500/20655] Loss_D_A: 0.6910175085067749 Loss_D_B: 0.18745625019073486 Loss_G: 0.23353064060211182\n",
      "Epoch [4/10] Batch [1600/20655] Loss_D_A: 0.31281721591949463 Loss_D_B: 0.9926193952560425 Loss_G: 0.5251739025115967\n",
      "Epoch [4/10] Batch [1700/20655] Loss_D_A: 0.24825704097747803 Loss_D_B: 0.8894460201263428 Loss_G: 0.07047849893569946\n",
      "Epoch [4/10] Batch [1800/20655] Loss_D_A: 0.19284981489181519 Loss_D_B: 0.34689968824386597 Loss_G: 0.7660298943519592\n",
      "Epoch [4/10] Batch [1900/20655] Loss_D_A: 0.11648166179656982 Loss_D_B: 0.8448714017868042 Loss_G: 0.4963992238044739\n",
      "Epoch [4/10] Batch [2000/20655] Loss_D_A: 0.10919356346130371 Loss_D_B: 0.8316798806190491 Loss_G: 0.6974968314170837\n",
      "Epoch [4/10] Batch [2100/20655] Loss_D_A: 0.916140079498291 Loss_D_B: 0.3636924624443054 Loss_G: 0.5829247832298279\n",
      "Epoch [4/10] Batch [2200/20655] Loss_D_A: 0.08472555875778198 Loss_D_B: 0.41198813915252686 Loss_G: 0.8681565523147583\n",
      "Epoch [4/10] Batch [2300/20655] Loss_D_A: 0.11891460418701172 Loss_D_B: 0.3319016098976135 Loss_G: 0.13706660270690918\n",
      "Epoch [4/10] Batch [2400/20655] Loss_D_A: 0.15815138816833496 Loss_D_B: 0.5111640095710754 Loss_G: 0.8733271360397339\n",
      "Epoch [4/10] Batch [2500/20655] Loss_D_A: 0.3352498412132263 Loss_D_B: 0.9101433157920837 Loss_G: 0.41596323251724243\n",
      "Epoch [4/10] Batch [2600/20655] Loss_D_A: 0.02169203758239746 Loss_D_B: 0.9740279912948608 Loss_G: 0.6711093783378601\n",
      "Epoch [4/10] Batch [2700/20655] Loss_D_A: 0.49145323038101196 Loss_D_B: 0.19833046197891235 Loss_G: 0.36478710174560547\n",
      "Epoch [4/10] Batch [2800/20655] Loss_D_A: 0.46720659732818604 Loss_D_B: 0.0744931697845459 Loss_G: 0.6029698848724365\n",
      "Epoch [4/10] Batch [2900/20655] Loss_D_A: 0.2913075089454651 Loss_D_B: 0.3831152319908142 Loss_G: 0.6281785368919373\n",
      "Epoch [4/10] Batch [3000/20655] Loss_D_A: 0.42086678743362427 Loss_D_B: 0.7797268033027649 Loss_G: 0.8618646264076233\n",
      "Epoch [4/10] Batch [3100/20655] Loss_D_A: 0.12435585260391235 Loss_D_B: 0.36821186542510986 Loss_G: 0.5510730743408203\n",
      "Epoch [4/10] Batch [3200/20655] Loss_D_A: 0.956911563873291 Loss_D_B: 0.9849221706390381 Loss_G: 0.8015727400779724\n",
      "Epoch [4/10] Batch [3300/20655] Loss_D_A: 0.3225530982017517 Loss_D_B: 0.8695485591888428 Loss_G: 0.11001592874526978\n",
      "Epoch [4/10] Batch [3400/20655] Loss_D_A: 0.5572877526283264 Loss_D_B: 0.03285038471221924 Loss_G: 0.7416023015975952\n",
      "Epoch [4/10] Batch [3500/20655] Loss_D_A: 0.00575411319732666 Loss_D_B: 0.17083632946014404 Loss_G: 0.9417895078659058\n",
      "Epoch [4/10] Batch [3600/20655] Loss_D_A: 0.26060664653778076 Loss_D_B: 0.18880879878997803 Loss_G: 0.7916968464851379\n",
      "Epoch [4/10] Batch [3700/20655] Loss_D_A: 0.7291762828826904 Loss_D_B: 0.9767706990242004 Loss_G: 0.33691608905792236\n",
      "Epoch [4/10] Batch [3800/20655] Loss_D_A: 0.3109326958656311 Loss_D_B: 0.9912810325622559 Loss_G: 0.08539217710494995\n",
      "Epoch [4/10] Batch [3900/20655] Loss_D_A: 0.36575067043304443 Loss_D_B: 0.27677369117736816 Loss_G: 0.48266923427581787\n",
      "Epoch [4/10] Batch [4000/20655] Loss_D_A: 0.13116806745529175 Loss_D_B: 0.49670088291168213 Loss_G: 0.9026761054992676\n",
      "Epoch [4/10] Batch [4100/20655] Loss_D_A: 0.9744840860366821 Loss_D_B: 0.999249279499054 Loss_G: 0.1149834394454956\n",
      "Epoch [4/10] Batch [4200/20655] Loss_D_A: 0.49956071376800537 Loss_D_B: 0.12628740072250366 Loss_G: 0.04697638750076294\n",
      "Epoch [4/10] Batch [4300/20655] Loss_D_A: 0.3375323414802551 Loss_D_B: 0.16663479804992676 Loss_G: 0.18663471937179565\n",
      "Epoch [4/10] Batch [4400/20655] Loss_D_A: 0.27509593963623047 Loss_D_B: 0.11659824848175049 Loss_G: 0.8548837900161743\n",
      "Epoch [4/10] Batch [4500/20655] Loss_D_A: 0.8011912107467651 Loss_D_B: 0.4929984211921692 Loss_G: 0.43313664197921753\n",
      "Epoch [4/10] Batch [4600/20655] Loss_D_A: 0.8524776101112366 Loss_D_B: 0.30313611030578613 Loss_G: 0.20098745822906494\n",
      "Epoch [4/10] Batch [4700/20655] Loss_D_A: 0.8879470825195312 Loss_D_B: 0.9166740775108337 Loss_G: 0.6938579082489014\n",
      "Epoch [4/10] Batch [4800/20655] Loss_D_A: 0.747582733631134 Loss_D_B: 0.30482327938079834 Loss_G: 0.5866831541061401\n",
      "Epoch [4/10] Batch [4900/20655] Loss_D_A: 0.5762102007865906 Loss_D_B: 0.08859425783157349 Loss_G: 0.18984627723693848\n",
      "Epoch [4/10] Batch [5000/20655] Loss_D_A: 0.9661537408828735 Loss_D_B: 0.4576799273490906 Loss_G: 0.19489789009094238\n",
      "Epoch [4/10] Batch [5100/20655] Loss_D_A: 0.8752105236053467 Loss_D_B: 0.07421320676803589 Loss_G: 0.5115456581115723\n",
      "Epoch [4/10] Batch [5200/20655] Loss_D_A: 0.1969209909439087 Loss_D_B: 0.7434486150741577 Loss_G: 0.7487431764602661\n",
      "Epoch [4/10] Batch [5300/20655] Loss_D_A: 0.4409588575363159 Loss_D_B: 0.8194289207458496 Loss_G: 0.7533291578292847\n",
      "Epoch [4/10] Batch [5400/20655] Loss_D_A: 0.5050932765007019 Loss_D_B: 0.39294761419296265 Loss_G: 0.162372887134552\n",
      "Epoch [4/10] Batch [5500/20655] Loss_D_A: 0.8283339142799377 Loss_D_B: 0.5802928805351257 Loss_G: 0.9224684834480286\n",
      "Epoch [4/10] Batch [5600/20655] Loss_D_A: 0.7212063670158386 Loss_D_B: 0.8992336392402649 Loss_G: 0.007168948650360107\n",
      "Epoch [4/10] Batch [5700/20655] Loss_D_A: 0.27443236112594604 Loss_D_B: 0.645060122013092 Loss_G: 0.8253347277641296\n",
      "Epoch [4/10] Batch [5800/20655] Loss_D_A: 0.4846665859222412 Loss_D_B: 0.9368354082107544 Loss_G: 0.5422110557556152\n",
      "Epoch [4/10] Batch [5900/20655] Loss_D_A: 0.3863993287086487 Loss_D_B: 0.6802835464477539 Loss_G: 0.09934490919113159\n",
      "Epoch [4/10] Batch [6000/20655] Loss_D_A: 0.3028358817100525 Loss_D_B: 0.21708965301513672 Loss_G: 0.43216413259506226\n",
      "Epoch [4/10] Batch [6100/20655] Loss_D_A: 0.7806360721588135 Loss_D_B: 0.9210614562034607 Loss_G: 0.6628274321556091\n",
      "Epoch [4/10] Batch [6200/20655] Loss_D_A: 0.3418516516685486 Loss_D_B: 0.7356157302856445 Loss_G: 0.8371812701225281\n",
      "Epoch [4/10] Batch [6300/20655] Loss_D_A: 0.5102734565734863 Loss_D_B: 0.6815901398658752 Loss_G: 0.6388471126556396\n",
      "Epoch [4/10] Batch [6400/20655] Loss_D_A: 0.041693806648254395 Loss_D_B: 0.7535350322723389 Loss_G: 0.208446204662323\n",
      "Epoch [4/10] Batch [6500/20655] Loss_D_A: 0.15852856636047363 Loss_D_B: 0.5849955081939697 Loss_G: 0.4735944867134094\n",
      "Epoch [4/10] Batch [6600/20655] Loss_D_A: 0.7241483330726624 Loss_D_B: 0.7751293778419495 Loss_G: 0.24879252910614014\n",
      "Epoch [4/10] Batch [6700/20655] Loss_D_A: 0.26446157693862915 Loss_D_B: 0.1412590742111206 Loss_G: 0.4423433542251587\n",
      "Epoch [4/10] Batch [6800/20655] Loss_D_A: 0.38669437170028687 Loss_D_B: 0.23423504829406738 Loss_G: 0.03799569606781006\n",
      "Epoch [4/10] Batch [6900/20655] Loss_D_A: 0.9517700672149658 Loss_D_B: 0.19782626628875732 Loss_G: 0.9638012051582336\n",
      "Epoch [4/10] Batch [7000/20655] Loss_D_A: 0.09836935997009277 Loss_D_B: 0.6266653537750244 Loss_G: 0.5856128931045532\n",
      "Epoch [4/10] Batch [7100/20655] Loss_D_A: 0.47771716117858887 Loss_D_B: 0.11195904016494751 Loss_G: 0.06898325681686401\n",
      "Epoch [4/10] Batch [7200/20655] Loss_D_A: 0.36597758531570435 Loss_D_B: 0.7068023681640625 Loss_G: 0.14992016553878784\n",
      "Epoch [4/10] Batch [7300/20655] Loss_D_A: 0.5638827085494995 Loss_D_B: 0.9777448177337646 Loss_G: 0.9352645874023438\n",
      "Epoch [4/10] Batch [7400/20655] Loss_D_A: 0.5005512237548828 Loss_D_B: 0.08340197801589966 Loss_G: 0.10985058546066284\n",
      "Epoch [4/10] Batch [7500/20655] Loss_D_A: 0.7230356335639954 Loss_D_B: 0.6665785312652588 Loss_G: 0.1404232382774353\n",
      "Epoch [4/10] Batch [7600/20655] Loss_D_A: 0.4997881054878235 Loss_D_B: 0.029047608375549316 Loss_G: 0.9563181400299072\n",
      "Epoch [4/10] Batch [7700/20655] Loss_D_A: 0.8596499562263489 Loss_D_B: 0.9660244584083557 Loss_G: 0.0669592022895813\n",
      "Epoch [4/10] Batch [7800/20655] Loss_D_A: 0.4540151357650757 Loss_D_B: 0.6711978912353516 Loss_G: 0.36834561824798584\n",
      "Epoch [4/10] Batch [7900/20655] Loss_D_A: 0.3880978226661682 Loss_D_B: 0.3808588981628418 Loss_G: 0.39820897579193115\n",
      "Epoch [4/10] Batch [8000/20655] Loss_D_A: 0.2754700183868408 Loss_D_B: 0.1731022596359253 Loss_G: 0.3055298328399658\n",
      "Epoch [4/10] Batch [8100/20655] Loss_D_A: 0.39616066217422485 Loss_D_B: 0.3998808264732361 Loss_G: 0.10883641242980957\n",
      "Epoch [4/10] Batch [8200/20655] Loss_D_A: 0.21122854948043823 Loss_D_B: 0.7511823177337646 Loss_G: 0.3819171190261841\n",
      "Epoch [4/10] Batch [8300/20655] Loss_D_A: 0.16314977407455444 Loss_D_B: 0.6093569397926331 Loss_G: 0.08331632614135742\n",
      "Epoch [4/10] Batch [8400/20655] Loss_D_A: 0.5788912773132324 Loss_D_B: 0.5361920595169067 Loss_G: 0.6932457685470581\n",
      "Epoch [4/10] Batch [8500/20655] Loss_D_A: 0.6331981420516968 Loss_D_B: 0.09837710857391357 Loss_G: 0.7203346490859985\n",
      "Epoch [4/10] Batch [8600/20655] Loss_D_A: 0.47429174184799194 Loss_D_B: 0.6544980406761169 Loss_G: 0.31429797410964966\n",
      "Epoch [4/10] Batch [8700/20655] Loss_D_A: 0.47975093126296997 Loss_D_B: 0.1730474829673767 Loss_G: 0.7055172324180603\n",
      "Epoch [4/10] Batch [8800/20655] Loss_D_A: 0.1055305004119873 Loss_D_B: 0.7985192537307739 Loss_G: 0.06332755088806152\n",
      "Epoch [4/10] Batch [8900/20655] Loss_D_A: 0.0847901701927185 Loss_D_B: 0.6157225966453552 Loss_G: 0.29174089431762695\n",
      "Epoch [4/10] Batch [9000/20655] Loss_D_A: 0.525111198425293 Loss_D_B: 0.554781973361969 Loss_G: 0.799302339553833\n",
      "Epoch [4/10] Batch [9100/20655] Loss_D_A: 0.0802735686302185 Loss_D_B: 0.19650393724441528 Loss_G: 0.5726599097251892\n",
      "Epoch [4/10] Batch [9200/20655] Loss_D_A: 0.004213511943817139 Loss_D_B: 0.9110502004623413 Loss_G: 0.0966750979423523\n",
      "Epoch [4/10] Batch [9300/20655] Loss_D_A: 0.14933562278747559 Loss_D_B: 0.8101126551628113 Loss_G: 0.7029605507850647\n",
      "Epoch [4/10] Batch [9400/20655] Loss_D_A: 0.08195137977600098 Loss_D_B: 0.5312625169754028 Loss_G: 0.46759867668151855\n",
      "Epoch [4/10] Batch [9500/20655] Loss_D_A: 0.35885870456695557 Loss_D_B: 0.16980469226837158 Loss_G: 0.9169748425483704\n",
      "Epoch [4/10] Batch [9600/20655] Loss_D_A: 0.8680161833763123 Loss_D_B: 0.07942676544189453 Loss_G: 0.2851545810699463\n",
      "Epoch [4/10] Batch [9700/20655] Loss_D_A: 0.21329790353775024 Loss_D_B: 0.7683083415031433 Loss_G: 0.42229920625686646\n",
      "Epoch [4/10] Batch [9800/20655] Loss_D_A: 0.6989836692810059 Loss_D_B: 0.8921098709106445 Loss_G: 0.9370704293251038\n",
      "Epoch [4/10] Batch [9900/20655] Loss_D_A: 0.9691498279571533 Loss_D_B: 0.3474830389022827 Loss_G: 0.7738955616950989\n",
      "Epoch [4/10] Batch [10000/20655] Loss_D_A: 0.4189135432243347 Loss_D_B: 0.3228979706764221 Loss_G: 0.30222153663635254\n",
      "Epoch [4/10] Batch [10100/20655] Loss_D_A: 0.4656485319137573 Loss_D_B: 0.7854881882667542 Loss_G: 0.24317806959152222\n",
      "Epoch [4/10] Batch [10200/20655] Loss_D_A: 0.6560085415840149 Loss_D_B: 0.21883535385131836 Loss_G: 0.7382948398590088\n",
      "Epoch [4/10] Batch [10300/20655] Loss_D_A: 0.9419679641723633 Loss_D_B: 0.912979006767273 Loss_G: 0.8064135909080505\n",
      "Epoch [4/10] Batch [10400/20655] Loss_D_A: 0.2188502550125122 Loss_D_B: 0.2940819263458252 Loss_G: 0.0060190558433532715\n",
      "Epoch [4/10] Batch [10500/20655] Loss_D_A: 0.07401496171951294 Loss_D_B: 0.6563594341278076 Loss_G: 0.2605610489845276\n",
      "Epoch [4/10] Batch [10600/20655] Loss_D_A: 0.5811701416969299 Loss_D_B: 0.43977487087249756 Loss_G: 0.6882652044296265\n",
      "Epoch [4/10] Batch [10700/20655] Loss_D_A: 0.6127414703369141 Loss_D_B: 0.8384485840797424 Loss_G: 0.704997181892395\n",
      "Epoch [4/10] Batch [10800/20655] Loss_D_A: 0.4614088535308838 Loss_D_B: 0.7109534740447998 Loss_G: 0.8057934641838074\n",
      "Epoch [4/10] Batch [10900/20655] Loss_D_A: 0.8218012452125549 Loss_D_B: 0.8077759146690369 Loss_G: 0.6163077354431152\n",
      "Epoch [4/10] Batch [11000/20655] Loss_D_A: 0.4597957134246826 Loss_D_B: 0.08348393440246582 Loss_G: 0.522682785987854\n",
      "Epoch [4/10] Batch [11100/20655] Loss_D_A: 0.8021801710128784 Loss_D_B: 0.8037267327308655 Loss_G: 0.6691766381263733\n",
      "Epoch [4/10] Batch [11200/20655] Loss_D_A: 0.8665671944618225 Loss_D_B: 0.952136754989624 Loss_G: 0.5276366472244263\n",
      "Epoch [4/10] Batch [11300/20655] Loss_D_A: 0.7705642580986023 Loss_D_B: 0.22357630729675293 Loss_G: 0.5103858709335327\n",
      "Epoch [4/10] Batch [11400/20655] Loss_D_A: 0.8874157071113586 Loss_D_B: 0.32617032527923584 Loss_G: 0.0009993314743041992\n",
      "Epoch [4/10] Batch [11500/20655] Loss_D_A: 0.8570908904075623 Loss_D_B: 0.8332428932189941 Loss_G: 0.7595877051353455\n",
      "Epoch [4/10] Batch [11600/20655] Loss_D_A: 0.2576900124549866 Loss_D_B: 0.6079380512237549 Loss_G: 0.3327179551124573\n",
      "Epoch [4/10] Batch [11700/20655] Loss_D_A: 0.2189660668373108 Loss_D_B: 0.6631909012794495 Loss_G: 0.6491105556488037\n",
      "Epoch [4/10] Batch [11800/20655] Loss_D_A: 0.8813308477401733 Loss_D_B: 0.3633555769920349 Loss_G: 0.7763319611549377\n",
      "Epoch [4/10] Batch [11900/20655] Loss_D_A: 0.6662924289703369 Loss_D_B: 0.7498281598091125 Loss_G: 0.7766338586807251\n",
      "Epoch [4/10] Batch [12000/20655] Loss_D_A: 0.5174057483673096 Loss_D_B: 0.26801198720932007 Loss_G: 0.7293318510055542\n",
      "Epoch [4/10] Batch [12100/20655] Loss_D_A: 0.5314533114433289 Loss_D_B: 0.17702597379684448 Loss_G: 0.9442514777183533\n",
      "Epoch [4/10] Batch [12200/20655] Loss_D_A: 0.39030778408050537 Loss_D_B: 0.03709644079208374 Loss_G: 0.4314367175102234\n",
      "Epoch [4/10] Batch [12300/20655] Loss_D_A: 0.47647160291671753 Loss_D_B: 0.7859656810760498 Loss_G: 0.7090813517570496\n",
      "Epoch [4/10] Batch [12400/20655] Loss_D_A: 0.026483893394470215 Loss_D_B: 0.013458073139190674 Loss_G: 0.9031551480293274\n",
      "Epoch [4/10] Batch [12500/20655] Loss_D_A: 0.5932596921920776 Loss_D_B: 0.19720125198364258 Loss_G: 0.8394454717636108\n",
      "Epoch [4/10] Batch [12600/20655] Loss_D_A: 0.44801634550094604 Loss_D_B: 0.8240905404090881 Loss_G: 0.26930487155914307\n",
      "Epoch [4/10] Batch [12700/20655] Loss_D_A: 0.13589125871658325 Loss_D_B: 0.08856523036956787 Loss_G: 0.12426960468292236\n",
      "Epoch [4/10] Batch [12800/20655] Loss_D_A: 0.5592211484909058 Loss_D_B: 0.01302027702331543 Loss_G: 0.38492655754089355\n",
      "Epoch [4/10] Batch [12900/20655] Loss_D_A: 0.6320726871490479 Loss_D_B: 0.14491409063339233 Loss_G: 0.04801821708679199\n",
      "Epoch [4/10] Batch [13000/20655] Loss_D_A: 0.3868666887283325 Loss_D_B: 0.7648451328277588 Loss_G: 0.7875314950942993\n",
      "Epoch [4/10] Batch [13100/20655] Loss_D_A: 0.25973236560821533 Loss_D_B: 0.17010337114334106 Loss_G: 0.05661308765411377\n",
      "Epoch [4/10] Batch [13200/20655] Loss_D_A: 0.48671722412109375 Loss_D_B: 0.49280351400375366 Loss_G: 0.3227742910385132\n",
      "Epoch [4/10] Batch [13300/20655] Loss_D_A: 0.013158023357391357 Loss_D_B: 0.2299799919128418 Loss_G: 0.3349030613899231\n",
      "Epoch [4/10] Batch [13400/20655] Loss_D_A: 0.247441828250885 Loss_D_B: 0.9031460881233215 Loss_G: 0.5415408611297607\n",
      "Epoch [4/10] Batch [13500/20655] Loss_D_A: 0.6054762601852417 Loss_D_B: 0.5800399780273438 Loss_G: 0.12474048137664795\n",
      "Epoch [4/10] Batch [13600/20655] Loss_D_A: 0.7816923260688782 Loss_D_B: 0.07187443971633911 Loss_G: 0.09510445594787598\n",
      "Epoch [4/10] Batch [13700/20655] Loss_D_A: 0.5788671970367432 Loss_D_B: 0.8966036438941956 Loss_G: 0.7696477174758911\n",
      "Epoch [4/10] Batch [13800/20655] Loss_D_A: 0.9433419108390808 Loss_D_B: 0.9926441311836243 Loss_G: 0.2007225751876831\n",
      "Epoch [4/10] Batch [13900/20655] Loss_D_A: 0.8349738717079163 Loss_D_B: 0.5515468120574951 Loss_G: 0.05772566795349121\n",
      "Epoch [4/10] Batch [14000/20655] Loss_D_A: 0.05627995729446411 Loss_D_B: 0.3018573522567749 Loss_G: 0.9049839377403259\n",
      "Epoch [4/10] Batch [14100/20655] Loss_D_A: 0.059116244316101074 Loss_D_B: 0.9014215469360352 Loss_G: 0.40693730115890503\n",
      "Epoch [4/10] Batch [14200/20655] Loss_D_A: 0.1668052077293396 Loss_D_B: 0.0060127973556518555 Loss_G: 0.4204070568084717\n",
      "Epoch [4/10] Batch [14300/20655] Loss_D_A: 0.39832448959350586 Loss_D_B: 0.4658016562461853 Loss_G: 0.592627763748169\n",
      "Epoch [4/10] Batch [14400/20655] Loss_D_A: 0.8754284977912903 Loss_D_B: 0.9907338619232178 Loss_G: 0.3062257170677185\n",
      "Epoch [4/10] Batch [14500/20655] Loss_D_A: 0.6510568857192993 Loss_D_B: 0.3526490330696106 Loss_G: 0.9527086019515991\n",
      "Epoch [4/10] Batch [14600/20655] Loss_D_A: 0.36796504259109497 Loss_D_B: 0.7185612320899963 Loss_G: 0.8236916065216064\n",
      "Epoch [4/10] Batch [14700/20655] Loss_D_A: 0.3358628749847412 Loss_D_B: 0.967586874961853 Loss_G: 0.4801795482635498\n",
      "Epoch [4/10] Batch [14800/20655] Loss_D_A: 0.4735879898071289 Loss_D_B: 0.008418917655944824 Loss_G: 0.40212011337280273\n",
      "Epoch [4/10] Batch [14900/20655] Loss_D_A: 0.7746086716651917 Loss_D_B: 0.7539774179458618 Loss_G: 0.49356716871261597\n",
      "Epoch [4/10] Batch [15000/20655] Loss_D_A: 0.04162585735321045 Loss_D_B: 0.3576032519340515 Loss_G: 0.6790639758110046\n",
      "Epoch [4/10] Batch [15100/20655] Loss_D_A: 0.23812294006347656 Loss_D_B: 0.7041594982147217 Loss_G: 0.18462461233139038\n",
      "Epoch [4/10] Batch [15200/20655] Loss_D_A: 0.5265067219734192 Loss_D_B: 0.5108612775802612 Loss_G: 0.735884964466095\n",
      "Epoch [4/10] Batch [15300/20655] Loss_D_A: 0.8517802357673645 Loss_D_B: 0.041667640209198 Loss_G: 0.7539301514625549\n",
      "Epoch [4/10] Batch [15400/20655] Loss_D_A: 0.6082318425178528 Loss_D_B: 0.2596502900123596 Loss_G: 0.24696969985961914\n",
      "Epoch [4/10] Batch [15500/20655] Loss_D_A: 0.962428629398346 Loss_D_B: 0.5472344160079956 Loss_G: 0.8647742867469788\n",
      "Epoch [4/10] Batch [15600/20655] Loss_D_A: 0.3049996495246887 Loss_D_B: 0.730023980140686 Loss_G: 0.9283769726753235\n",
      "Epoch [4/10] Batch [15700/20655] Loss_D_A: 0.5198595523834229 Loss_D_B: 0.5063014626502991 Loss_G: 0.9562188386917114\n",
      "Epoch [4/10] Batch [15800/20655] Loss_D_A: 0.08677256107330322 Loss_D_B: 0.927504301071167 Loss_G: 0.9723264575004578\n",
      "Epoch [4/10] Batch [15900/20655] Loss_D_A: 0.44334298372268677 Loss_D_B: 0.7657042145729065 Loss_G: 0.17089897394180298\n",
      "Epoch [4/10] Batch [16000/20655] Loss_D_A: 0.018353700637817383 Loss_D_B: 0.5496055483818054 Loss_G: 0.025184690952301025\n",
      "Epoch [4/10] Batch [16100/20655] Loss_D_A: 0.766146183013916 Loss_D_B: 0.5335054397583008 Loss_G: 0.5341691970825195\n",
      "Epoch [4/10] Batch [16200/20655] Loss_D_A: 0.3916863799095154 Loss_D_B: 0.1621764898300171 Loss_G: 0.48253899812698364\n",
      "Epoch [4/10] Batch [16300/20655] Loss_D_A: 0.8557422161102295 Loss_D_B: 0.8093364834785461 Loss_G: 0.37942832708358765\n",
      "Epoch [4/10] Batch [16400/20655] Loss_D_A: 0.5746429562568665 Loss_D_B: 0.17712819576263428 Loss_G: 0.345994234085083\n",
      "Epoch [4/10] Batch [16500/20655] Loss_D_A: 0.25514835119247437 Loss_D_B: 0.5831397771835327 Loss_G: 0.3973659873008728\n",
      "Epoch [4/10] Batch [16600/20655] Loss_D_A: 0.7694782614707947 Loss_D_B: 0.3398101329803467 Loss_G: 0.13935405015945435\n",
      "Epoch [4/10] Batch [16700/20655] Loss_D_A: 0.9084209203720093 Loss_D_B: 0.27469104528427124 Loss_G: 0.8190411925315857\n",
      "Epoch [4/10] Batch [16800/20655] Loss_D_A: 0.732306718826294 Loss_D_B: 0.29243701696395874 Loss_G: 0.25090521574020386\n",
      "Epoch [4/10] Batch [16900/20655] Loss_D_A: 0.4521445035934448 Loss_D_B: 0.6823710203170776 Loss_G: 0.8730669021606445\n",
      "Epoch [4/10] Batch [17000/20655] Loss_D_A: 0.7167913913726807 Loss_D_B: 0.29091089963912964 Loss_G: 0.35986262559890747\n",
      "Epoch [4/10] Batch [17100/20655] Loss_D_A: 0.07489567995071411 Loss_D_B: 0.2697474956512451 Loss_G: 0.8037070035934448\n",
      "Epoch [4/10] Batch [17200/20655] Loss_D_A: 0.33556997776031494 Loss_D_B: 0.19445836544036865 Loss_G: 0.8923588395118713\n",
      "Epoch [4/10] Batch [17300/20655] Loss_D_A: 0.3571767210960388 Loss_D_B: 0.687191367149353 Loss_G: 0.2460005283355713\n",
      "Epoch [4/10] Batch [17400/20655] Loss_D_A: 0.06023120880126953 Loss_D_B: 0.1346440315246582 Loss_G: 0.11540424823760986\n",
      "Epoch [4/10] Batch [17500/20655] Loss_D_A: 0.29730838537216187 Loss_D_B: 0.763114333152771 Loss_G: 0.9931357502937317\n",
      "Epoch [4/10] Batch [17600/20655] Loss_D_A: 0.224759042263031 Loss_D_B: 0.06031644344329834 Loss_G: 0.7694808840751648\n",
      "Epoch [4/10] Batch [17700/20655] Loss_D_A: 0.6139885187149048 Loss_D_B: 0.1235242486000061 Loss_G: 0.25731730461120605\n",
      "Epoch [4/10] Batch [17800/20655] Loss_D_A: 0.19469070434570312 Loss_D_B: 0.9539492726325989 Loss_G: 0.12592840194702148\n",
      "Epoch [4/10] Batch [17900/20655] Loss_D_A: 0.9653753042221069 Loss_D_B: 0.3017575740814209 Loss_G: 0.5564476847648621\n",
      "Epoch [4/10] Batch [18000/20655] Loss_D_A: 0.7481762766838074 Loss_D_B: 0.2888430953025818 Loss_G: 0.4501758813858032\n",
      "Epoch [4/10] Batch [18100/20655] Loss_D_A: 0.3235675096511841 Loss_D_B: 0.40274500846862793 Loss_G: 0.20019453763961792\n",
      "Epoch [4/10] Batch [18200/20655] Loss_D_A: 0.525812029838562 Loss_D_B: 0.6845400929450989 Loss_G: 0.036155760288238525\n",
      "Epoch [4/10] Batch [18300/20655] Loss_D_A: 0.6781880259513855 Loss_D_B: 0.3005182147026062 Loss_G: 0.759159505367279\n",
      "Epoch [4/10] Batch [18400/20655] Loss_D_A: 0.6797415614128113 Loss_D_B: 0.4803977608680725 Loss_G: 0.9823958277702332\n",
      "Epoch [4/10] Batch [18500/20655] Loss_D_A: 0.02317798137664795 Loss_D_B: 0.41216790676116943 Loss_G: 0.21333074569702148\n",
      "Epoch [4/10] Batch [18600/20655] Loss_D_A: 0.34221726655960083 Loss_D_B: 0.05727428197860718 Loss_G: 0.08885341882705688\n",
      "Epoch [4/10] Batch [18700/20655] Loss_D_A: 0.9360768795013428 Loss_D_B: 0.7141175270080566 Loss_G: 0.1771097183227539\n",
      "Epoch [4/10] Batch [18800/20655] Loss_D_A: 0.7212401628494263 Loss_D_B: 0.6832585334777832 Loss_G: 0.8398059010505676\n",
      "Epoch [4/10] Batch [18900/20655] Loss_D_A: 0.021149277687072754 Loss_D_B: 0.038314998149871826 Loss_G: 0.9500288963317871\n",
      "Epoch [4/10] Batch [19000/20655] Loss_D_A: 0.9106882214546204 Loss_D_B: 0.7355166673660278 Loss_G: 0.7913590669631958\n",
      "Epoch [4/10] Batch [19100/20655] Loss_D_A: 0.3657231330871582 Loss_D_B: 0.3110920190811157 Loss_G: 0.6210507154464722\n",
      "Epoch [4/10] Batch [19200/20655] Loss_D_A: 0.06979787349700928 Loss_D_B: 0.4477139711380005 Loss_G: 0.49395203590393066\n",
      "Epoch [4/10] Batch [19300/20655] Loss_D_A: 0.44153374433517456 Loss_D_B: 0.9483458995819092 Loss_G: 0.2762012481689453\n",
      "Epoch [4/10] Batch [19400/20655] Loss_D_A: 0.7472733855247498 Loss_D_B: 0.8205167651176453 Loss_G: 0.794859766960144\n",
      "Epoch [4/10] Batch [19500/20655] Loss_D_A: 0.1911987066268921 Loss_D_B: 0.08356708288192749 Loss_G: 0.9894018173217773\n",
      "Epoch [4/10] Batch [19600/20655] Loss_D_A: 0.878474771976471 Loss_D_B: 0.3076934218406677 Loss_G: 0.15389728546142578\n",
      "Epoch [4/10] Batch [19700/20655] Loss_D_A: 0.1544395089149475 Loss_D_B: 0.13814115524291992 Loss_G: 0.029205799102783203\n",
      "Epoch [4/10] Batch [19800/20655] Loss_D_A: 0.20942801237106323 Loss_D_B: 0.9449514150619507 Loss_G: 0.7074723839759827\n",
      "Epoch [4/10] Batch [19900/20655] Loss_D_A: 0.7924976348876953 Loss_D_B: 0.27368396520614624 Loss_G: 0.35536426305770874\n",
      "Epoch [4/10] Batch [20000/20655] Loss_D_A: 0.44236135482788086 Loss_D_B: 0.21940988302230835 Loss_G: 0.41662079095840454\n",
      "Epoch [4/10] Batch [20100/20655] Loss_D_A: 0.7459911108016968 Loss_D_B: 0.9541159272193909 Loss_G: 0.3154832124710083\n",
      "Epoch [4/10] Batch [20200/20655] Loss_D_A: 0.8758453130722046 Loss_D_B: 0.7592093348503113 Loss_G: 0.8791362643241882\n",
      "Epoch [4/10] Batch [20300/20655] Loss_D_A: 0.3545267581939697 Loss_D_B: 0.8364896178245544 Loss_G: 0.29491162300109863\n",
      "Epoch [4/10] Batch [20400/20655] Loss_D_A: 0.8656874895095825 Loss_D_B: 0.9605007767677307 Loss_G: 0.8831127882003784\n",
      "Epoch [4/10] Batch [20500/20655] Loss_D_A: 0.7590485215187073 Loss_D_B: 0.1870405673980713 Loss_G: 0.9890062212944031\n",
      "Epoch [4/10] Batch [20600/20655] Loss_D_A: 0.25953978300094604 Loss_D_B: 0.08598566055297852 Loss_G: 0.3124043941497803\n",
      "Epoch [5/10] Batch [0/20655] Loss_D_A: 0.8354266881942749 Loss_D_B: 0.6545500159263611 Loss_G: 0.8789441585540771\n",
      "Epoch [5/10] Batch [0/20655] Loss_D_A: 0.37108564376831055 Loss_D_B: 0.6348156929016113 Loss_G: 0.704127311706543\n",
      "Epoch [5/10] Batch [100/20655] Loss_D_A: 0.9168147444725037 Loss_D_B: 0.9033909440040588 Loss_G: 0.3731672763824463\n",
      "Epoch [5/10] Batch [200/20655] Loss_D_A: 0.9915050268173218 Loss_D_B: 0.29642564058303833 Loss_G: 0.31004518270492554\n",
      "Epoch [5/10] Batch [300/20655] Loss_D_A: 0.07054203748703003 Loss_D_B: 0.34780776500701904 Loss_G: 0.27792441844940186\n",
      "Epoch [5/10] Batch [400/20655] Loss_D_A: 0.3126102089881897 Loss_D_B: 0.2379651665687561 Loss_G: 0.5703511834144592\n",
      "Epoch [5/10] Batch [500/20655] Loss_D_A: 0.8276571035385132 Loss_D_B: 0.8627355694770813 Loss_G: 0.5828394293785095\n",
      "Epoch [5/10] Batch [600/20655] Loss_D_A: 0.15073823928833008 Loss_D_B: 0.45143550634384155 Loss_G: 0.5592610239982605\n",
      "Epoch [5/10] Batch [700/20655] Loss_D_A: 0.7797805070877075 Loss_D_B: 0.01617521047592163 Loss_G: 0.7609976530075073\n",
      "Epoch [5/10] Batch [800/20655] Loss_D_A: 0.9701802134513855 Loss_D_B: 0.20758044719696045 Loss_G: 0.0346948504447937\n",
      "Epoch [5/10] Batch [900/20655] Loss_D_A: 0.6959535479545593 Loss_D_B: 0.41042566299438477 Loss_G: 0.8014883399009705\n",
      "Epoch [5/10] Batch [1000/20655] Loss_D_A: 0.510195791721344 Loss_D_B: 0.687757134437561 Loss_G: 0.2750098705291748\n",
      "Epoch [5/10] Batch [1100/20655] Loss_D_A: 0.035946786403656006 Loss_D_B: 0.3591240644454956 Loss_G: 0.7238132953643799\n",
      "Epoch [5/10] Batch [1200/20655] Loss_D_A: 0.2418537735939026 Loss_D_B: 0.7887206673622131 Loss_G: 0.4014362096786499\n",
      "Epoch [5/10] Batch [1300/20655] Loss_D_A: 0.4412071108818054 Loss_D_B: 0.9374035000801086 Loss_G: 0.19596487283706665\n",
      "Epoch [5/10] Batch [1400/20655] Loss_D_A: 0.510230302810669 Loss_D_B: 0.6907967329025269 Loss_G: 0.09128540754318237\n",
      "Epoch [5/10] Batch [1500/20655] Loss_D_A: 0.1500651240348816 Loss_D_B: 0.09753590822219849 Loss_G: 0.8034124970436096\n",
      "Epoch [5/10] Batch [1600/20655] Loss_D_A: 0.15824657678604126 Loss_D_B: 0.9027585983276367 Loss_G: 0.8087462782859802\n",
      "Epoch [5/10] Batch [1700/20655] Loss_D_A: 0.19645214080810547 Loss_D_B: 0.44888877868652344 Loss_G: 0.4749566912651062\n",
      "Epoch [5/10] Batch [1800/20655] Loss_D_A: 0.7293041944503784 Loss_D_B: 0.615328311920166 Loss_G: 0.6916963458061218\n",
      "Epoch [5/10] Batch [1900/20655] Loss_D_A: 0.13051366806030273 Loss_D_B: 0.7042104601860046 Loss_G: 0.80436110496521\n",
      "Epoch [5/10] Batch [2000/20655] Loss_D_A: 0.37571024894714355 Loss_D_B: 0.22873568534851074 Loss_G: 0.7700753808021545\n",
      "Epoch [5/10] Batch [2100/20655] Loss_D_A: 0.5682370066642761 Loss_D_B: 0.05130046606063843 Loss_G: 0.006692051887512207\n",
      "Epoch [5/10] Batch [2200/20655] Loss_D_A: 0.29973769187927246 Loss_D_B: 0.9713915586471558 Loss_G: 0.5222383141517639\n",
      "Epoch [5/10] Batch [2300/20655] Loss_D_A: 0.5194885730743408 Loss_D_B: 0.025214195251464844 Loss_G: 0.06986266374588013\n",
      "Epoch [5/10] Batch [2400/20655] Loss_D_A: 0.4507078528404236 Loss_D_B: 0.15584170818328857 Loss_G: 0.8730117678642273\n",
      "Epoch [5/10] Batch [2500/20655] Loss_D_A: 0.9124638438224792 Loss_D_B: 0.38506758213043213 Loss_G: 0.45415180921554565\n",
      "Epoch [5/10] Batch [2600/20655] Loss_D_A: 0.7384955286979675 Loss_D_B: 0.15451043844223022 Loss_G: 0.618377149105072\n",
      "Epoch [5/10] Batch [2700/20655] Loss_D_A: 0.6692602634429932 Loss_D_B: 0.7360630035400391 Loss_G: 0.8764747977256775\n",
      "Epoch [5/10] Batch [2800/20655] Loss_D_A: 0.03213256597518921 Loss_D_B: 0.09135836362838745 Loss_G: 0.937759280204773\n",
      "Epoch [5/10] Batch [2900/20655] Loss_D_A: 0.6214175224304199 Loss_D_B: 0.2843610644340515 Loss_G: 0.14015543460845947\n",
      "Epoch [5/10] Batch [3000/20655] Loss_D_A: 0.02183401584625244 Loss_D_B: 0.589594841003418 Loss_G: 0.5823990106582642\n",
      "Epoch [5/10] Batch [3100/20655] Loss_D_A: 0.8126376867294312 Loss_D_B: 0.3904215693473816 Loss_G: 0.8111444115638733\n",
      "Epoch [5/10] Batch [3200/20655] Loss_D_A: 0.45272910594940186 Loss_D_B: 0.6596875190734863 Loss_G: 0.2182694673538208\n",
      "Epoch [5/10] Batch [3300/20655] Loss_D_A: 0.464232861995697 Loss_D_B: 0.3726773262023926 Loss_G: 0.34890216588974\n",
      "Epoch [5/10] Batch [3400/20655] Loss_D_A: 0.9999585747718811 Loss_D_B: 0.4545498490333557 Loss_G: 0.006774187088012695\n",
      "Epoch [5/10] Batch [3500/20655] Loss_D_A: 0.587879478931427 Loss_D_B: 0.4759424924850464 Loss_G: 0.3559027314186096\n",
      "Epoch [5/10] Batch [3600/20655] Loss_D_A: 0.5273211002349854 Loss_D_B: 0.857725203037262 Loss_G: 0.6177354454994202\n",
      "Epoch [5/10] Batch [3700/20655] Loss_D_A: 0.19675534963607788 Loss_D_B: 0.18093925714492798 Loss_G: 0.6938797235488892\n",
      "Epoch [5/10] Batch [3800/20655] Loss_D_A: 0.6311078071594238 Loss_D_B: 0.07581979036331177 Loss_G: 0.8843517899513245\n",
      "Epoch [5/10] Batch [3900/20655] Loss_D_A: 0.9958985447883606 Loss_D_B: 0.281141459941864 Loss_G: 0.3944290280342102\n",
      "Epoch [5/10] Batch [4000/20655] Loss_D_A: 0.7307604551315308 Loss_D_B: 0.8395589590072632 Loss_G: 0.9109360575675964\n",
      "Epoch [5/10] Batch [4100/20655] Loss_D_A: 0.913692831993103 Loss_D_B: 0.5966914892196655 Loss_G: 0.2444014549255371\n",
      "Epoch [5/10] Batch [4200/20655] Loss_D_A: 0.9345988035202026 Loss_D_B: 0.35473936796188354 Loss_G: 0.9032895565032959\n",
      "Epoch [5/10] Batch [4300/20655] Loss_D_A: 0.17608880996704102 Loss_D_B: 0.8851509690284729 Loss_G: 0.94297856092453\n",
      "Epoch [5/10] Batch [4400/20655] Loss_D_A: 0.0389399528503418 Loss_D_B: 0.5256678462028503 Loss_G: 0.32533329725265503\n",
      "Epoch [5/10] Batch [4500/20655] Loss_D_A: 0.9821987748146057 Loss_D_B: 0.729941725730896 Loss_G: 0.6273664236068726\n",
      "Epoch [5/10] Batch [4600/20655] Loss_D_A: 0.8330395817756653 Loss_D_B: 0.6678235530853271 Loss_G: 0.10547596216201782\n",
      "Epoch [5/10] Batch [4700/20655] Loss_D_A: 0.3461817502975464 Loss_D_B: 0.21077537536621094 Loss_G: 0.6331007480621338\n",
      "Epoch [5/10] Batch [4800/20655] Loss_D_A: 0.3292784094810486 Loss_D_B: 0.7239995002746582 Loss_G: 0.6840454339981079\n",
      "Epoch [5/10] Batch [4900/20655] Loss_D_A: 0.6207786798477173 Loss_D_B: 0.8797019720077515 Loss_G: 0.7047255635261536\n",
      "Epoch [5/10] Batch [5000/20655] Loss_D_A: 0.7879216074943542 Loss_D_B: 0.46328502893447876 Loss_G: 0.3226788640022278\n",
      "Epoch [5/10] Batch [5100/20655] Loss_D_A: 0.07519727945327759 Loss_D_B: 0.987363338470459 Loss_G: 0.7614982724189758\n",
      "Epoch [5/10] Batch [5200/20655] Loss_D_A: 0.3285287022590637 Loss_D_B: 0.887588381767273 Loss_G: 0.4775489568710327\n",
      "Epoch [5/10] Batch [5300/20655] Loss_D_A: 0.605873703956604 Loss_D_B: 0.10552752017974854 Loss_G: 0.2194003462791443\n",
      "Epoch [5/10] Batch [5400/20655] Loss_D_A: 0.01122593879699707 Loss_D_B: 0.28543686866760254 Loss_G: 0.4056399464607239\n",
      "Epoch [5/10] Batch [5500/20655] Loss_D_A: 0.35111695528030396 Loss_D_B: 0.5122467875480652 Loss_G: 0.061340391635894775\n",
      "Epoch [5/10] Batch [5600/20655] Loss_D_A: 0.9771735072135925 Loss_D_B: 0.7791361212730408 Loss_G: 0.47722989320755005\n",
      "Epoch [5/10] Batch [5700/20655] Loss_D_A: 0.28964537382125854 Loss_D_B: 0.8976520299911499 Loss_G: 0.019142210483551025\n",
      "Epoch [5/10] Batch [5800/20655] Loss_D_A: 0.5498706102371216 Loss_D_B: 0.3729683756828308 Loss_G: 0.013994455337524414\n",
      "Epoch [5/10] Batch [5900/20655] Loss_D_A: 0.5324162244796753 Loss_D_B: 0.8555147647857666 Loss_G: 0.4803127646446228\n",
      "Epoch [5/10] Batch [6000/20655] Loss_D_A: 0.08089381456375122 Loss_D_B: 0.7869278788566589 Loss_G: 0.2584342956542969\n",
      "Epoch [5/10] Batch [6100/20655] Loss_D_A: 0.84443199634552 Loss_D_B: 0.9398035407066345 Loss_G: 0.84568852186203\n",
      "Epoch [5/10] Batch [6200/20655] Loss_D_A: 0.31740349531173706 Loss_D_B: 0.444261372089386 Loss_G: 0.7173686623573303\n",
      "Epoch [5/10] Batch [6300/20655] Loss_D_A: 0.2444518804550171 Loss_D_B: 0.581683337688446 Loss_G: 0.0243300199508667\n",
      "Epoch [5/10] Batch [6400/20655] Loss_D_A: 0.34959644079208374 Loss_D_B: 0.8006911873817444 Loss_G: 0.16001993417739868\n",
      "Epoch [5/10] Batch [6500/20655] Loss_D_A: 0.5205866694450378 Loss_D_B: 0.23884928226470947 Loss_G: 0.2613346576690674\n",
      "Epoch [5/10] Batch [6600/20655] Loss_D_A: 0.41428494453430176 Loss_D_B: 0.9689515233039856 Loss_G: 0.37277907133102417\n",
      "Epoch [5/10] Batch [6700/20655] Loss_D_A: 0.8950742483139038 Loss_D_B: 0.3226824998855591 Loss_G: 0.9247743487358093\n",
      "Epoch [5/10] Batch [6800/20655] Loss_D_A: 0.6225340962409973 Loss_D_B: 0.8229880928993225 Loss_G: 0.7389800548553467\n",
      "Epoch [5/10] Batch [6900/20655] Loss_D_A: 0.9969899654388428 Loss_D_B: 0.04313582181930542 Loss_G: 0.5893874764442444\n",
      "Epoch [5/10] Batch [7000/20655] Loss_D_A: 0.48772484064102173 Loss_D_B: 0.9819015264511108 Loss_G: 0.6112422943115234\n",
      "Epoch [5/10] Batch [7100/20655] Loss_D_A: 0.19522321224212646 Loss_D_B: 0.7411604523658752 Loss_G: 0.34824174642562866\n",
      "Epoch [5/10] Batch [7200/20655] Loss_D_A: 0.060044944286346436 Loss_D_B: 0.27815765142440796 Loss_G: 0.5965405702590942\n",
      "Epoch [5/10] Batch [7300/20655] Loss_D_A: 0.7676222324371338 Loss_D_B: 0.8292659521102905 Loss_G: 0.3888053894042969\n",
      "Epoch [5/10] Batch [7400/20655] Loss_D_A: 0.30978846549987793 Loss_D_B: 0.9257615804672241 Loss_G: 0.3623116612434387\n",
      "Epoch [5/10] Batch [7500/20655] Loss_D_A: 0.9726898074150085 Loss_D_B: 0.9446495771408081 Loss_G: 0.5844587087631226\n",
      "Epoch [5/10] Batch [7600/20655] Loss_D_A: 0.033413589000701904 Loss_D_B: 0.11869663000106812 Loss_G: 0.34154975414276123\n",
      "Epoch [5/10] Batch [7700/20655] Loss_D_A: 0.8012581467628479 Loss_D_B: 0.28841203451156616 Loss_G: 0.48809462785720825\n",
      "Epoch [5/10] Batch [7800/20655] Loss_D_A: 0.8006263375282288 Loss_D_B: 0.6096689701080322 Loss_G: 0.6111080646514893\n",
      "Epoch [5/10] Batch [7900/20655] Loss_D_A: 0.8469728827476501 Loss_D_B: 0.5404587984085083 Loss_G: 0.125011146068573\n",
      "Epoch [5/10] Batch [8000/20655] Loss_D_A: 0.4484674334526062 Loss_D_B: 0.32625263929367065 Loss_G: 0.1368657946586609\n",
      "Epoch [5/10] Batch [8100/20655] Loss_D_A: 0.7693894505500793 Loss_D_B: 0.6211718320846558 Loss_G: 0.29737281799316406\n",
      "Epoch [5/10] Batch [8200/20655] Loss_D_A: 0.08831673860549927 Loss_D_B: 0.687605619430542 Loss_G: 0.06673604249954224\n",
      "Epoch [5/10] Batch [8300/20655] Loss_D_A: 0.3633994460105896 Loss_D_B: 0.9303181767463684 Loss_G: 0.33952343463897705\n",
      "Epoch [5/10] Batch [8400/20655] Loss_D_A: 0.33722054958343506 Loss_D_B: 0.20732426643371582 Loss_G: 0.996468722820282\n",
      "Epoch [5/10] Batch [8500/20655] Loss_D_A: 0.6074433326721191 Loss_D_B: 0.403891384601593 Loss_G: 0.46652674674987793\n",
      "Epoch [5/10] Batch [8600/20655] Loss_D_A: 0.6005955338478088 Loss_D_B: 0.1465820074081421 Loss_G: 0.7050168514251709\n",
      "Epoch [5/10] Batch [8700/20655] Loss_D_A: 0.15670514106750488 Loss_D_B: 0.16862452030181885 Loss_G: 0.8057700395584106\n",
      "Epoch [5/10] Batch [8800/20655] Loss_D_A: 0.5453187227249146 Loss_D_B: 0.6473474502563477 Loss_G: 0.5972529649734497\n",
      "Epoch [5/10] Batch [8900/20655] Loss_D_A: 0.07774335145950317 Loss_D_B: 0.2913845181465149 Loss_G: 0.6833650469779968\n",
      "Epoch [5/10] Batch [9000/20655] Loss_D_A: 0.9099379777908325 Loss_D_B: 0.10132706165313721 Loss_G: 0.39562857151031494\n",
      "Epoch [5/10] Batch [9100/20655] Loss_D_A: 0.25516897439956665 Loss_D_B: 0.3224800229072571 Loss_G: 0.5414541363716125\n",
      "Epoch [5/10] Batch [9200/20655] Loss_D_A: 0.8466898202896118 Loss_D_B: 0.4936631917953491 Loss_G: 0.8812524080276489\n",
      "Epoch [5/10] Batch [9300/20655] Loss_D_A: 0.9665626883506775 Loss_D_B: 0.6160416603088379 Loss_G: 0.7611461281776428\n",
      "Epoch [5/10] Batch [9400/20655] Loss_D_A: 0.8343443274497986 Loss_D_B: 0.2723095417022705 Loss_G: 0.09740781784057617\n",
      "Epoch [5/10] Batch [9500/20655] Loss_D_A: 0.8205246925354004 Loss_D_B: 0.9423526525497437 Loss_G: 0.25630563497543335\n",
      "Epoch [5/10] Batch [9600/20655] Loss_D_A: 0.466194748878479 Loss_D_B: 0.4429885745048523 Loss_G: 0.5119071006774902\n",
      "Epoch [5/10] Batch [9700/20655] Loss_D_A: 0.6221919059753418 Loss_D_B: 0.18317937850952148 Loss_G: 0.670287549495697\n",
      "Epoch [5/10] Batch [9800/20655] Loss_D_A: 0.0005773305892944336 Loss_D_B: 0.5601800680160522 Loss_G: 0.9994314908981323\n",
      "Epoch [5/10] Batch [9900/20655] Loss_D_A: 0.27102363109588623 Loss_D_B: 0.1380872130393982 Loss_G: 0.7413803935050964\n",
      "Epoch [5/10] Batch [10000/20655] Loss_D_A: 0.6836057901382446 Loss_D_B: 0.4357369542121887 Loss_G: 0.7679581642150879\n",
      "Epoch [5/10] Batch [10100/20655] Loss_D_A: 0.21926146745681763 Loss_D_B: 0.5094839930534363 Loss_G: 0.07969361543655396\n",
      "Epoch [5/10] Batch [10200/20655] Loss_D_A: 0.24940377473831177 Loss_D_B: 0.3692159056663513 Loss_G: 0.027008652687072754\n",
      "Epoch [5/10] Batch [10300/20655] Loss_D_A: 0.4700705409049988 Loss_D_B: 0.40572094917297363 Loss_G: 0.20360445976257324\n",
      "Epoch [5/10] Batch [10400/20655] Loss_D_A: 0.5662589073181152 Loss_D_B: 0.5003501176834106 Loss_G: 0.7277676463127136\n",
      "Epoch [5/10] Batch [10500/20655] Loss_D_A: 0.9241628050804138 Loss_D_B: 0.38086026906967163 Loss_G: 0.631020724773407\n",
      "Epoch [5/10] Batch [10600/20655] Loss_D_A: 0.4720633029937744 Loss_D_B: 0.04960447549819946 Loss_G: 0.47599542140960693\n",
      "Epoch [5/10] Batch [10700/20655] Loss_D_A: 0.4242696166038513 Loss_D_B: 0.3459097743034363 Loss_G: 0.08071601390838623\n",
      "Epoch [5/10] Batch [10800/20655] Loss_D_A: 0.9764435887336731 Loss_D_B: 0.2939455509185791 Loss_G: 0.5466380715370178\n",
      "Epoch [5/10] Batch [10900/20655] Loss_D_A: 0.8138114809989929 Loss_D_B: 0.5757885575294495 Loss_G: 0.8600200414657593\n",
      "Epoch [5/10] Batch [11000/20655] Loss_D_A: 0.44054609537124634 Loss_D_B: 0.39661043882369995 Loss_G: 0.21439802646636963\n",
      "Epoch [5/10] Batch [11100/20655] Loss_D_A: 0.3167272210121155 Loss_D_B: 0.3937820792198181 Loss_G: 0.5671699643135071\n",
      "Epoch [5/10] Batch [11200/20655] Loss_D_A: 0.31223785877227783 Loss_D_B: 0.4077284336090088 Loss_G: 0.36283379793167114\n",
      "Epoch [5/10] Batch [11300/20655] Loss_D_A: 0.021176517009735107 Loss_D_B: 0.4273243546485901 Loss_G: 0.4422929883003235\n",
      "Epoch [5/10] Batch [11400/20655] Loss_D_A: 0.04705679416656494 Loss_D_B: 0.540535032749176 Loss_G: 0.4716007709503174\n",
      "Epoch [5/10] Batch [11500/20655] Loss_D_A: 0.7947443723678589 Loss_D_B: 0.28732067346572876 Loss_G: 0.30419260263442993\n",
      "Epoch [5/10] Batch [11600/20655] Loss_D_A: 0.7632620930671692 Loss_D_B: 0.6036891341209412 Loss_G: 0.3428410291671753\n",
      "Epoch [5/10] Batch [11700/20655] Loss_D_A: 0.015733301639556885 Loss_D_B: 0.14404058456420898 Loss_G: 0.2628667950630188\n",
      "Epoch [5/10] Batch [11800/20655] Loss_D_A: 0.555381178855896 Loss_D_B: 0.586401641368866 Loss_G: 0.2735525369644165\n",
      "Epoch [5/10] Batch [11900/20655] Loss_D_A: 0.13407349586486816 Loss_D_B: 0.3451094627380371 Loss_G: 0.6408853530883789\n",
      "Epoch [5/10] Batch [12000/20655] Loss_D_A: 0.9670094847679138 Loss_D_B: 0.6436118483543396 Loss_G: 0.5109007954597473\n",
      "Epoch [5/10] Batch [12100/20655] Loss_D_A: 0.5389026999473572 Loss_D_B: 0.7938820719718933 Loss_G: 0.7127822041511536\n",
      "Epoch [5/10] Batch [12200/20655] Loss_D_A: 0.12171316146850586 Loss_D_B: 0.5859878659248352 Loss_G: 0.3734416365623474\n",
      "Epoch [5/10] Batch [12300/20655] Loss_D_A: 0.8750038146972656 Loss_D_B: 0.6247730255126953 Loss_G: 0.644432544708252\n",
      "Epoch [5/10] Batch [12400/20655] Loss_D_A: 0.9943996071815491 Loss_D_B: 0.4391357898712158 Loss_G: 0.029313087463378906\n",
      "Epoch [5/10] Batch [12500/20655] Loss_D_A: 0.35667508840560913 Loss_D_B: 0.6662001013755798 Loss_G: 0.2523300051689148\n",
      "Epoch [5/10] Batch [12600/20655] Loss_D_A: 0.25797802209854126 Loss_D_B: 0.03928905725479126 Loss_G: 0.018236517906188965\n",
      "Epoch [5/10] Batch [12700/20655] Loss_D_A: 0.6176525354385376 Loss_D_B: 0.7417184710502625 Loss_G: 0.8104293942451477\n",
      "Epoch [5/10] Batch [12800/20655] Loss_D_A: 0.34877198934555054 Loss_D_B: 0.46280133724212646 Loss_G: 0.850328266620636\n",
      "Epoch [5/10] Batch [12900/20655] Loss_D_A: 0.4880412817001343 Loss_D_B: 0.7983148097991943 Loss_G: 0.15488839149475098\n",
      "Epoch [5/10] Batch [13000/20655] Loss_D_A: 0.42599910497665405 Loss_D_B: 0.7960748672485352 Loss_G: 0.3637425899505615\n",
      "Epoch [5/10] Batch [13100/20655] Loss_D_A: 0.08882206678390503 Loss_D_B: 0.6507134437561035 Loss_G: 0.897055983543396\n",
      "Epoch [5/10] Batch [13200/20655] Loss_D_A: 0.43575191497802734 Loss_D_B: 0.7331579923629761 Loss_G: 0.8995521664619446\n",
      "Epoch [5/10] Batch [13300/20655] Loss_D_A: 0.03974485397338867 Loss_D_B: 0.9910327196121216 Loss_G: 0.25316107273101807\n",
      "Epoch [5/10] Batch [13400/20655] Loss_D_A: 0.8822154402732849 Loss_D_B: 0.12768417596817017 Loss_G: 0.45619940757751465\n",
      "Epoch [5/10] Batch [13500/20655] Loss_D_A: 0.7685285210609436 Loss_D_B: 0.1933385133743286 Loss_G: 0.39523839950561523\n",
      "Epoch [5/10] Batch [13600/20655] Loss_D_A: 0.0440521240234375 Loss_D_B: 0.49845778942108154 Loss_G: 0.003201127052307129\n",
      "Epoch [5/10] Batch [13700/20655] Loss_D_A: 0.0004642605781555176 Loss_D_B: 0.1112738847732544 Loss_G: 0.045033276081085205\n",
      "Epoch [5/10] Batch [13800/20655] Loss_D_A: 0.27976465225219727 Loss_D_B: 0.23415791988372803 Loss_G: 0.3525511622428894\n",
      "Epoch [5/10] Batch [13900/20655] Loss_D_A: 0.9195260405540466 Loss_D_B: 0.6746395230293274 Loss_G: 0.4050433039665222\n",
      "Epoch [5/10] Batch [14000/20655] Loss_D_A: 0.3691455125808716 Loss_D_B: 0.28762805461883545 Loss_G: 0.009490966796875\n",
      "Epoch [5/10] Batch [14100/20655] Loss_D_A: 0.8525309562683105 Loss_D_B: 0.8693535923957825 Loss_G: 0.5193198919296265\n",
      "Epoch [5/10] Batch [14200/20655] Loss_D_A: 0.23400157690048218 Loss_D_B: 0.0052397847175598145 Loss_G: 0.37060028314590454\n",
      "Epoch [5/10] Batch [14300/20655] Loss_D_A: 0.13296514749526978 Loss_D_B: 0.4234917163848877 Loss_G: 0.9241610765457153\n",
      "Epoch [5/10] Batch [14400/20655] Loss_D_A: 0.03907322883605957 Loss_D_B: 0.5205856561660767 Loss_G: 0.039017558097839355\n",
      "Epoch [5/10] Batch [14500/20655] Loss_D_A: 0.5106198191642761 Loss_D_B: 0.13983237743377686 Loss_G: 0.4309896230697632\n",
      "Epoch [5/10] Batch [14600/20655] Loss_D_A: 0.013731062412261963 Loss_D_B: 0.07691687345504761 Loss_G: 0.10466808080673218\n",
      "Epoch [5/10] Batch [14700/20655] Loss_D_A: 0.09078502655029297 Loss_D_B: 0.8519216775894165 Loss_G: 0.40391772985458374\n",
      "Epoch [5/10] Batch [14800/20655] Loss_D_A: 0.35024434328079224 Loss_D_B: 0.14107710123062134 Loss_G: 0.058587491512298584\n",
      "Epoch [5/10] Batch [14900/20655] Loss_D_A: 0.5818009376525879 Loss_D_B: 0.5593193173408508 Loss_G: 0.2963361144065857\n",
      "Epoch [5/10] Batch [15000/20655] Loss_D_A: 0.44055402278900146 Loss_D_B: 0.7529118061065674 Loss_G: 0.8528668284416199\n",
      "Epoch [5/10] Batch [15100/20655] Loss_D_A: 0.513108491897583 Loss_D_B: 0.04433941841125488 Loss_G: 0.046472132205963135\n",
      "Epoch [5/10] Batch [15200/20655] Loss_D_A: 0.691575288772583 Loss_D_B: 0.9390785098075867 Loss_G: 0.47309762239456177\n",
      "Epoch [5/10] Batch [15300/20655] Loss_D_A: 0.5997998714447021 Loss_D_B: 0.6813915371894836 Loss_G: 0.13032889366149902\n",
      "Epoch [5/10] Batch [15400/20655] Loss_D_A: 0.8577451109886169 Loss_D_B: 0.6986078023910522 Loss_G: 0.4961364269256592\n",
      "Epoch [5/10] Batch [15500/20655] Loss_D_A: 0.20672214031219482 Loss_D_B: 0.7043381929397583 Loss_G: 0.3697589635848999\n",
      "Epoch [5/10] Batch [15600/20655] Loss_D_A: 0.5739157199859619 Loss_D_B: 0.9459355473518372 Loss_G: 0.9626902937889099\n",
      "Epoch [5/10] Batch [15700/20655] Loss_D_A: 0.3863152861595154 Loss_D_B: 0.9152271151542664 Loss_G: 0.8632903695106506\n",
      "Epoch [5/10] Batch [15800/20655] Loss_D_A: 0.027796030044555664 Loss_D_B: 0.013744056224822998 Loss_G: 0.9641704559326172\n",
      "Epoch [5/10] Batch [15900/20655] Loss_D_A: 0.6468971371650696 Loss_D_B: 0.9106465578079224 Loss_G: 0.9315212368965149\n",
      "Epoch [5/10] Batch [16000/20655] Loss_D_A: 0.23108923435211182 Loss_D_B: 0.5842180848121643 Loss_G: 0.9356046319007874\n",
      "Epoch [5/10] Batch [16100/20655] Loss_D_A: 0.6276282072067261 Loss_D_B: 0.8807745575904846 Loss_G: 0.08401215076446533\n",
      "Epoch [5/10] Batch [16200/20655] Loss_D_A: 0.6655561923980713 Loss_D_B: 0.8385369181632996 Loss_G: 0.08841979503631592\n",
      "Epoch [5/10] Batch [16300/20655] Loss_D_A: 0.3608326315879822 Loss_D_B: 0.9654350876808167 Loss_G: 0.5070810317993164\n",
      "Epoch [5/10] Batch [16400/20655] Loss_D_A: 0.014114558696746826 Loss_D_B: 0.5539200305938721 Loss_G: 0.5434368848800659\n",
      "Epoch [5/10] Batch [16500/20655] Loss_D_A: 0.09246689081192017 Loss_D_B: 0.5578649640083313 Loss_G: 0.06020230054855347\n",
      "Epoch [5/10] Batch [16600/20655] Loss_D_A: 0.4136914610862732 Loss_D_B: 0.4793875813484192 Loss_G: 0.6685271263122559\n",
      "Epoch [5/10] Batch [16700/20655] Loss_D_A: 0.8389798402786255 Loss_D_B: 0.6914734244346619 Loss_G: 0.6128434538841248\n",
      "Epoch [5/10] Batch [16800/20655] Loss_D_A: 0.9371789693832397 Loss_D_B: 0.6095166802406311 Loss_G: 0.49213308095932007\n",
      "Epoch [5/10] Batch [16900/20655] Loss_D_A: 0.21638625860214233 Loss_D_B: 0.7297804355621338 Loss_G: 0.871913492679596\n",
      "Epoch [5/10] Batch [17000/20655] Loss_D_A: 0.19317913055419922 Loss_D_B: 0.6844694018363953 Loss_G: 0.8149133324623108\n",
      "Epoch [5/10] Batch [17100/20655] Loss_D_A: 0.5073845982551575 Loss_D_B: 0.6501888036727905 Loss_G: 0.7235651612281799\n",
      "Epoch [5/10] Batch [17200/20655] Loss_D_A: 0.06498414278030396 Loss_D_B: 0.4281727075576782 Loss_G: 0.531995415687561\n",
      "Epoch [5/10] Batch [17300/20655] Loss_D_A: 0.6620749235153198 Loss_D_B: 0.31375253200531006 Loss_G: 0.9087256193161011\n",
      "Epoch [5/10] Batch [17400/20655] Loss_D_A: 0.0529555082321167 Loss_D_B: 0.30759328603744507 Loss_G: 0.1313578486442566\n",
      "Epoch [5/10] Batch [17500/20655] Loss_D_A: 0.31124401092529297 Loss_D_B: 0.8105417490005493 Loss_G: 0.813951313495636\n",
      "Epoch [5/10] Batch [17600/20655] Loss_D_A: 0.9374144673347473 Loss_D_B: 0.959438681602478 Loss_G: 0.622463583946228\n",
      "Epoch [5/10] Batch [17700/20655] Loss_D_A: 0.6104919910430908 Loss_D_B: 0.059737324714660645 Loss_G: 0.9975382089614868\n",
      "Epoch [5/10] Batch [17800/20655] Loss_D_A: 0.08932483196258545 Loss_D_B: 0.3305243253707886 Loss_G: 0.27656805515289307\n",
      "Epoch [5/10] Batch [17900/20655] Loss_D_A: 0.768403947353363 Loss_D_B: 0.5322462916374207 Loss_G: 0.3594934344291687\n",
      "Epoch [5/10] Batch [18000/20655] Loss_D_A: 0.06758582592010498 Loss_D_B: 0.7742356657981873 Loss_G: 0.5880175828933716\n",
      "Epoch [5/10] Batch [18100/20655] Loss_D_A: 0.3846665024757385 Loss_D_B: 0.37022697925567627 Loss_G: 0.542510986328125\n",
      "Epoch [5/10] Batch [18200/20655] Loss_D_A: 0.9084798097610474 Loss_D_B: 0.7961075305938721 Loss_G: 0.889015793800354\n",
      "Epoch [5/10] Batch [18300/20655] Loss_D_A: 0.3386572003364563 Loss_D_B: 0.2783733010292053 Loss_G: 0.23120176792144775\n",
      "Epoch [5/10] Batch [18400/20655] Loss_D_A: 0.9013299345970154 Loss_D_B: 0.5781442523002625 Loss_G: 0.9362484216690063\n",
      "Epoch [5/10] Batch [18500/20655] Loss_D_A: 0.6029801368713379 Loss_D_B: 0.30358266830444336 Loss_G: 0.5660285353660583\n",
      "Epoch [5/10] Batch [18600/20655] Loss_D_A: 0.3349304795265198 Loss_D_B: 0.6150121092796326 Loss_G: 0.46225494146347046\n",
      "Epoch [5/10] Batch [18700/20655] Loss_D_A: 0.35422641038894653 Loss_D_B: 0.2762730121612549 Loss_G: 0.8230118155479431\n",
      "Epoch [5/10] Batch [18800/20655] Loss_D_A: 0.6868167519569397 Loss_D_B: 0.8062533140182495 Loss_G: 0.3045029640197754\n",
      "Epoch [5/10] Batch [18900/20655] Loss_D_A: 0.053619444370269775 Loss_D_B: 0.18240314722061157 Loss_G: 0.5675967335700989\n",
      "Epoch [5/10] Batch [19000/20655] Loss_D_A: 0.7396049499511719 Loss_D_B: 0.8158774971961975 Loss_G: 0.7480189204216003\n",
      "Epoch [5/10] Batch [19100/20655] Loss_D_A: 0.9207327961921692 Loss_D_B: 0.5060853362083435 Loss_G: 0.869765043258667\n",
      "Epoch [5/10] Batch [19200/20655] Loss_D_A: 0.40778350830078125 Loss_D_B: 0.6295490264892578 Loss_G: 0.683990478515625\n",
      "Epoch [5/10] Batch [19300/20655] Loss_D_A: 0.6781020164489746 Loss_D_B: 0.5416972041130066 Loss_G: 0.7009441256523132\n",
      "Epoch [5/10] Batch [19400/20655] Loss_D_A: 0.9304967522621155 Loss_D_B: 0.030325889587402344 Loss_G: 0.23712706565856934\n",
      "Epoch [5/10] Batch [19500/20655] Loss_D_A: 0.05278867483139038 Loss_D_B: 0.7927409410476685 Loss_G: 0.8249248266220093\n",
      "Epoch [5/10] Batch [19600/20655] Loss_D_A: 0.1364649534225464 Loss_D_B: 0.21798789501190186 Loss_G: 0.6267460584640503\n",
      "Epoch [5/10] Batch [19700/20655] Loss_D_A: 0.8910185694694519 Loss_D_B: 0.9012861251831055 Loss_G: 0.904392659664154\n",
      "Epoch [5/10] Batch [19800/20655] Loss_D_A: 0.6602745056152344 Loss_D_B: 0.8792557716369629 Loss_G: 0.627163827419281\n",
      "Epoch [5/10] Batch [19900/20655] Loss_D_A: 0.5654408931732178 Loss_D_B: 0.9747241735458374 Loss_G: 0.37143707275390625\n",
      "Epoch [5/10] Batch [20000/20655] Loss_D_A: 0.7182095646858215 Loss_D_B: 0.2875763177871704 Loss_G: 0.8937093615531921\n",
      "Epoch [5/10] Batch [20100/20655] Loss_D_A: 0.45912909507751465 Loss_D_B: 0.6123729348182678 Loss_G: 0.13573408126831055\n",
      "Epoch [5/10] Batch [20200/20655] Loss_D_A: 0.9292313456535339 Loss_D_B: 0.6785324215888977 Loss_G: 0.4793984889984131\n",
      "Epoch [5/10] Batch [20300/20655] Loss_D_A: 0.6166378855705261 Loss_D_B: 0.5989694595336914 Loss_G: 0.16903769969940186\n",
      "Epoch [5/10] Batch [20400/20655] Loss_D_A: 0.02857184410095215 Loss_D_B: 0.2698831558227539 Loss_G: 0.5251554250717163\n",
      "Epoch [5/10] Batch [20500/20655] Loss_D_A: 0.12440037727355957 Loss_D_B: 0.3735724091529846 Loss_G: 0.39219093322753906\n",
      "Epoch [5/10] Batch [20600/20655] Loss_D_A: 0.08010411262512207 Loss_D_B: 0.032145559787750244 Loss_G: 0.05724269151687622\n",
      "Epoch [6/10] Batch [0/20655] Loss_D_A: 0.37108564376831055 Loss_D_B: 0.6348156929016113 Loss_G: 0.704127311706543\n",
      "Epoch [6/10] Batch [0/20655] Loss_D_A: 0.15960389375686646 Loss_D_B: 0.5763391852378845 Loss_G: 0.7755647897720337\n",
      "Epoch [6/10] Batch [100/20655] Loss_D_A: 0.37130069732666016 Loss_D_B: 0.2832004427909851 Loss_G: 0.4502745270729065\n",
      "Epoch [6/10] Batch [200/20655] Loss_D_A: 0.8307144045829773 Loss_D_B: 0.24173295497894287 Loss_G: 0.30172520875930786\n",
      "Epoch [6/10] Batch [300/20655] Loss_D_A: 0.24731135368347168 Loss_D_B: 0.15451079607009888 Loss_G: 0.9423477053642273\n",
      "Epoch [6/10] Batch [400/20655] Loss_D_A: 0.7219208478927612 Loss_D_B: 0.28562623262405396 Loss_G: 0.8628023862838745\n",
      "Epoch [6/10] Batch [500/20655] Loss_D_A: 0.9435036778450012 Loss_D_B: 0.9703015089035034 Loss_G: 0.8036905527114868\n",
      "Epoch [6/10] Batch [600/20655] Loss_D_A: 0.6960431337356567 Loss_D_B: 0.4328886866569519 Loss_G: 0.545915424823761\n",
      "Epoch [6/10] Batch [700/20655] Loss_D_A: 0.15760314464569092 Loss_D_B: 0.584815263748169 Loss_G: 0.6596867442131042\n",
      "Epoch [6/10] Batch [800/20655] Loss_D_A: 0.953834056854248 Loss_D_B: 0.30264461040496826 Loss_G: 0.9596887826919556\n",
      "Epoch [6/10] Batch [900/20655] Loss_D_A: 0.8230957388877869 Loss_D_B: 0.17681866884231567 Loss_G: 0.21515625715255737\n",
      "Epoch [6/10] Batch [1000/20655] Loss_D_A: 0.2649391293525696 Loss_D_B: 0.2783735990524292 Loss_G: 0.4849686026573181\n",
      "Epoch [6/10] Batch [1100/20655] Loss_D_A: 0.31068503856658936 Loss_D_B: 0.8219861388206482 Loss_G: 0.8283987641334534\n",
      "Epoch [6/10] Batch [1200/20655] Loss_D_A: 0.9321309328079224 Loss_D_B: 0.5489827990531921 Loss_G: 0.9993536472320557\n",
      "Epoch [6/10] Batch [1300/20655] Loss_D_A: 0.5663945078849792 Loss_D_B: 0.2074519395828247 Loss_G: 0.09185522794723511\n",
      "Epoch [6/10] Batch [1400/20655] Loss_D_A: 0.09735912084579468 Loss_D_B: 0.5813063383102417 Loss_G: 0.42685604095458984\n",
      "Epoch [6/10] Batch [1500/20655] Loss_D_A: 0.9929541945457458 Loss_D_B: 0.94126296043396 Loss_G: 0.14774441719055176\n",
      "Epoch [6/10] Batch [1600/20655] Loss_D_A: 0.7175817489624023 Loss_D_B: 0.7243616580963135 Loss_G: 0.46183842420578003\n",
      "Epoch [6/10] Batch [1700/20655] Loss_D_A: 0.10405981540679932 Loss_D_B: 0.7241106629371643 Loss_G: 0.30295056104660034\n",
      "Epoch [6/10] Batch [1800/20655] Loss_D_A: 0.8897664546966553 Loss_D_B: 0.5914427042007446 Loss_G: 0.48186999559402466\n",
      "Epoch [6/10] Batch [1900/20655] Loss_D_A: 0.09759706258773804 Loss_D_B: 0.4210855960845947 Loss_G: 0.21301978826522827\n",
      "Epoch [6/10] Batch [2000/20655] Loss_D_A: 0.8293247222900391 Loss_D_B: 0.23947811126708984 Loss_G: 0.6654314994812012\n",
      "Epoch [6/10] Batch [2100/20655] Loss_D_A: 0.6182612776756287 Loss_D_B: 0.2090601921081543 Loss_G: 0.2566367983818054\n",
      "Epoch [6/10] Batch [2200/20655] Loss_D_A: 0.07213562726974487 Loss_D_B: 0.22594279050827026 Loss_G: 0.5468899607658386\n",
      "Epoch [6/10] Batch [2300/20655] Loss_D_A: 0.43524444103240967 Loss_D_B: 0.5195596218109131 Loss_G: 0.14115947484970093\n",
      "Epoch [6/10] Batch [2400/20655] Loss_D_A: 0.5953735709190369 Loss_D_B: 0.8737680315971375 Loss_G: 0.4145367741584778\n",
      "Epoch [6/10] Batch [2500/20655] Loss_D_A: 0.14742940664291382 Loss_D_B: 0.7696225047111511 Loss_G: 0.1003878116607666\n",
      "Epoch [6/10] Batch [2600/20655] Loss_D_A: 0.5643762946128845 Loss_D_B: 0.5768064260482788 Loss_G: 0.291354238986969\n",
      "Epoch [6/10] Batch [2700/20655] Loss_D_A: 0.567855715751648 Loss_D_B: 0.7445752024650574 Loss_G: 0.2766895890235901\n",
      "Epoch [6/10] Batch [2800/20655] Loss_D_A: 0.06070965528488159 Loss_D_B: 0.35434746742248535 Loss_G: 0.40157026052474976\n",
      "Epoch [6/10] Batch [2900/20655] Loss_D_A: 0.4248298406600952 Loss_D_B: 0.3102778196334839 Loss_G: 0.5212159156799316\n",
      "Epoch [6/10] Batch [3000/20655] Loss_D_A: 0.16089951992034912 Loss_D_B: 0.12738019227981567 Loss_G: 0.32158076763153076\n",
      "Epoch [6/10] Batch [3100/20655] Loss_D_A: 0.8949148058891296 Loss_D_B: 0.9371066689491272 Loss_G: 0.22979086637496948\n",
      "Epoch [6/10] Batch [3200/20655] Loss_D_A: 0.16171127557754517 Loss_D_B: 0.35262054204940796 Loss_G: 0.991012454032898\n",
      "Epoch [6/10] Batch [3300/20655] Loss_D_A: 0.03735083341598511 Loss_D_B: 0.04525643587112427 Loss_G: 0.9179915189743042\n",
      "Epoch [6/10] Batch [3400/20655] Loss_D_A: 0.42864787578582764 Loss_D_B: 0.9031707644462585 Loss_G: 0.9846851825714111\n",
      "Epoch [6/10] Batch [3500/20655] Loss_D_A: 0.8022809028625488 Loss_D_B: 0.9772340059280396 Loss_G: 0.7373366355895996\n",
      "Epoch [6/10] Batch [3600/20655] Loss_D_A: 0.6645420789718628 Loss_D_B: 0.2440718412399292 Loss_G: 0.921011745929718\n",
      "Epoch [6/10] Batch [3700/20655] Loss_D_A: 0.6343041658401489 Loss_D_B: 0.6743898391723633 Loss_G: 0.7629432082176208\n",
      "Epoch [6/10] Batch [3800/20655] Loss_D_A: 0.6092115044593811 Loss_D_B: 0.4210484027862549 Loss_G: 0.6348528265953064\n",
      "Epoch [6/10] Batch [3900/20655] Loss_D_A: 0.7908969521522522 Loss_D_B: 0.3278263211250305 Loss_G: 0.5672308802604675\n",
      "Epoch [6/10] Batch [4000/20655] Loss_D_A: 0.7239752411842346 Loss_D_B: 0.4291219115257263 Loss_G: 0.2523123025894165\n",
      "Epoch [6/10] Batch [4100/20655] Loss_D_A: 0.4236077666282654 Loss_D_B: 0.3192629814147949 Loss_G: 0.512546956539154\n",
      "Epoch [6/10] Batch [4200/20655] Loss_D_A: 0.7849888801574707 Loss_D_B: 0.39181679487228394 Loss_G: 0.47623682022094727\n",
      "Epoch [6/10] Batch [4300/20655] Loss_D_A: 0.6800771951675415 Loss_D_B: 0.9288840889930725 Loss_G: 0.026065409183502197\n",
      "Epoch [6/10] Batch [4400/20655] Loss_D_A: 0.6972125172615051 Loss_D_B: 0.010270893573760986 Loss_G: 0.7664905786514282\n",
      "Epoch [6/10] Batch [4500/20655] Loss_D_A: 0.464594304561615 Loss_D_B: 0.9459071159362793 Loss_G: 0.34400999546051025\n",
      "Epoch [6/10] Batch [4600/20655] Loss_D_A: 0.66375732421875 Loss_D_B: 0.6126643419265747 Loss_G: 0.4345328211784363\n",
      "Epoch [6/10] Batch [4700/20655] Loss_D_A: 0.5049563050270081 Loss_D_B: 0.45577722787857056 Loss_G: 0.5518267154693604\n",
      "Epoch [6/10] Batch [4800/20655] Loss_D_A: 0.38767582178115845 Loss_D_B: 0.3578680753707886 Loss_G: 0.7888898849487305\n",
      "Epoch [6/10] Batch [4900/20655] Loss_D_A: 0.4483017325401306 Loss_D_B: 0.6333798170089722 Loss_G: 0.522169828414917\n",
      "Epoch [6/10] Batch [5000/20655] Loss_D_A: 0.2797849178314209 Loss_D_B: 0.7546145915985107 Loss_G: 0.27649354934692383\n",
      "Epoch [6/10] Batch [5100/20655] Loss_D_A: 0.9176913499832153 Loss_D_B: 0.743036150932312 Loss_G: 0.9948690533638\n",
      "Epoch [6/10] Batch [5200/20655] Loss_D_A: 0.4205421805381775 Loss_D_B: 0.8136000633239746 Loss_G: 0.5080162286758423\n",
      "Epoch [6/10] Batch [5300/20655] Loss_D_A: 0.7760340571403503 Loss_D_B: 0.10458767414093018 Loss_G: 0.05220514535903931\n",
      "Epoch [6/10] Batch [5400/20655] Loss_D_A: 0.90190589427948 Loss_D_B: 0.22791504859924316 Loss_G: 0.9813185930252075\n",
      "Epoch [6/10] Batch [5500/20655] Loss_D_A: 0.2546766996383667 Loss_D_B: 0.7102423906326294 Loss_G: 0.8816502690315247\n",
      "Epoch [6/10] Batch [5600/20655] Loss_D_A: 0.1531551480293274 Loss_D_B: 0.0449674129486084 Loss_G: 0.32523638010025024\n",
      "Epoch [6/10] Batch [5700/20655] Loss_D_A: 0.685750424861908 Loss_D_B: 0.8944289088249207 Loss_G: 0.5319031476974487\n",
      "Epoch [6/10] Batch [5800/20655] Loss_D_A: 0.9363541007041931 Loss_D_B: 0.8712582588195801 Loss_G: 0.8313266634941101\n",
      "Epoch [6/10] Batch [5900/20655] Loss_D_A: 0.9327390789985657 Loss_D_B: 0.8512229919433594 Loss_G: 0.1804993748664856\n",
      "Epoch [6/10] Batch [6000/20655] Loss_D_A: 0.9735379815101624 Loss_D_B: 0.2795436382293701 Loss_G: 0.23184281587600708\n",
      "Epoch [6/10] Batch [6100/20655] Loss_D_A: 0.2244817614555359 Loss_D_B: 0.07406985759735107 Loss_G: 0.43630462884902954\n",
      "Epoch [6/10] Batch [6200/20655] Loss_D_A: 0.5755408406257629 Loss_D_B: 0.41816991567611694 Loss_G: 0.040294766426086426\n",
      "Epoch [6/10] Batch [6300/20655] Loss_D_A: 0.17630410194396973 Loss_D_B: 0.30581313371658325 Loss_G: 0.43966203927993774\n",
      "Epoch [6/10] Batch [6400/20655] Loss_D_A: 0.7871453166007996 Loss_D_B: 0.27478331327438354 Loss_G: 0.729185938835144\n",
      "Epoch [6/10] Batch [6500/20655] Loss_D_A: 0.2618069648742676 Loss_D_B: 0.8892340660095215 Loss_G: 0.13193517923355103\n",
      "Epoch [6/10] Batch [6600/20655] Loss_D_A: 0.13328063488006592 Loss_D_B: 0.31498217582702637 Loss_G: 0.8640539646148682\n",
      "Epoch [6/10] Batch [6700/20655] Loss_D_A: 0.4049137830734253 Loss_D_B: 0.01720607280731201 Loss_G: 0.24821585416793823\n",
      "Epoch [6/10] Batch [6800/20655] Loss_D_A: 0.16384780406951904 Loss_D_B: 0.371843159198761 Loss_G: 0.013427436351776123\n",
      "Epoch [6/10] Batch [6900/20655] Loss_D_A: 0.33184146881103516 Loss_D_B: 0.3804531693458557 Loss_G: 0.009978294372558594\n",
      "Epoch [6/10] Batch [7000/20655] Loss_D_A: 0.20422828197479248 Loss_D_B: 0.5192813873291016 Loss_G: 0.07617658376693726\n",
      "Epoch [6/10] Batch [7100/20655] Loss_D_A: 0.6790962815284729 Loss_D_B: 0.141431987285614 Loss_G: 0.4135515093803406\n",
      "Epoch [6/10] Batch [7200/20655] Loss_D_A: 0.46863293647766113 Loss_D_B: 0.06378453969955444 Loss_G: 0.04152482748031616\n",
      "Epoch [6/10] Batch [7300/20655] Loss_D_A: 0.20164382457733154 Loss_D_B: 0.9138492345809937 Loss_G: 0.017305493354797363\n",
      "Epoch [6/10] Batch [7400/20655] Loss_D_A: 0.8184571266174316 Loss_D_B: 0.7709639072418213 Loss_G: 0.6718649864196777\n",
      "Epoch [6/10] Batch [7500/20655] Loss_D_A: 0.3708822727203369 Loss_D_B: 0.951850175857544 Loss_G: 0.9196721911430359\n",
      "Epoch [6/10] Batch [7600/20655] Loss_D_A: 0.7357743382453918 Loss_D_B: 0.11414313316345215 Loss_G: 0.5880607962608337\n",
      "Epoch [6/10] Batch [7700/20655] Loss_D_A: 0.17085540294647217 Loss_D_B: 0.24130195379257202 Loss_G: 0.2826639413833618\n",
      "Epoch [6/10] Batch [7800/20655] Loss_D_A: 0.31267356872558594 Loss_D_B: 0.24622321128845215 Loss_G: 0.5967922210693359\n",
      "Epoch [6/10] Batch [7900/20655] Loss_D_A: 0.6706881523132324 Loss_D_B: 0.7771972417831421 Loss_G: 0.316392183303833\n",
      "Epoch [6/10] Batch [8000/20655] Loss_D_A: 0.8764001727104187 Loss_D_B: 0.2214057445526123 Loss_G: 0.4518824815750122\n",
      "Epoch [6/10] Batch [8100/20655] Loss_D_A: 0.9172930121421814 Loss_D_B: 0.7590338587760925 Loss_G: 0.8601863384246826\n",
      "Epoch [6/10] Batch [8200/20655] Loss_D_A: 0.024078667163848877 Loss_D_B: 0.0372161865234375 Loss_G: 0.9933624863624573\n",
      "Epoch [6/10] Batch [8300/20655] Loss_D_A: 0.06587457656860352 Loss_D_B: 0.4002203345298767 Loss_G: 0.3374364376068115\n",
      "Epoch [6/10] Batch [8400/20655] Loss_D_A: 0.17413842678070068 Loss_D_B: 0.3166170120239258 Loss_G: 0.40007203817367554\n",
      "Epoch [6/10] Batch [8500/20655] Loss_D_A: 0.5046452879905701 Loss_D_B: 0.8675549030303955 Loss_G: 0.7110365033149719\n",
      "Epoch [6/10] Batch [8600/20655] Loss_D_A: 0.4358779191970825 Loss_D_B: 0.9423518776893616 Loss_G: 0.4865384101867676\n",
      "Epoch [6/10] Batch [8700/20655] Loss_D_A: 0.5170040726661682 Loss_D_B: 0.31596267223358154 Loss_G: 0.16271454095840454\n",
      "Epoch [6/10] Batch [8800/20655] Loss_D_A: 0.36747050285339355 Loss_D_B: 0.9512162208557129 Loss_G: 0.11057090759277344\n",
      "Epoch [6/10] Batch [8900/20655] Loss_D_A: 0.3125125765800476 Loss_D_B: 0.46771132946014404 Loss_G: 0.8577269315719604\n",
      "Epoch [6/10] Batch [9000/20655] Loss_D_A: 0.25389283895492554 Loss_D_B: 0.21214842796325684 Loss_G: 0.4255101680755615\n",
      "Epoch [6/10] Batch [9100/20655] Loss_D_A: 0.30392205715179443 Loss_D_B: 0.6995349526405334 Loss_G: 0.6892400979995728\n",
      "Epoch [6/10] Batch [9200/20655] Loss_D_A: 0.17389237880706787 Loss_D_B: 0.0024167895317077637 Loss_G: 0.23896276950836182\n",
      "Epoch [6/10] Batch [9300/20655] Loss_D_A: 0.8950100541114807 Loss_D_B: 0.41826188564300537 Loss_G: 0.735596239566803\n",
      "Epoch [6/10] Batch [9400/20655] Loss_D_A: 0.22691106796264648 Loss_D_B: 0.6475473642349243 Loss_G: 0.40503764152526855\n",
      "Epoch [6/10] Batch [9500/20655] Loss_D_A: 0.6970508098602295 Loss_D_B: 0.581134021282196 Loss_G: 0.277685284614563\n",
      "Epoch [6/10] Batch [9600/20655] Loss_D_A: 0.6411476135253906 Loss_D_B: 0.7458179593086243 Loss_G: 0.845211386680603\n",
      "Epoch [6/10] Batch [9700/20655] Loss_D_A: 0.15094465017318726 Loss_D_B: 0.5834051370620728 Loss_G: 0.08534175157546997\n",
      "Epoch [6/10] Batch [9800/20655] Loss_D_A: 0.3402175307273865 Loss_D_B: 0.7324292063713074 Loss_G: 0.9163742661476135\n",
      "Epoch [6/10] Batch [9900/20655] Loss_D_A: 0.8157472014427185 Loss_D_B: 0.21547698974609375 Loss_G: 0.013417601585388184\n",
      "Epoch [6/10] Batch [10000/20655] Loss_D_A: 0.4388467073440552 Loss_D_B: 0.19574785232543945 Loss_G: 0.4521108865737915\n",
      "Epoch [6/10] Batch [10100/20655] Loss_D_A: 0.5842106342315674 Loss_D_B: 0.8302286267280579 Loss_G: 0.4632629156112671\n",
      "Epoch [6/10] Batch [10200/20655] Loss_D_A: 0.7182935476303101 Loss_D_B: 0.7869882583618164 Loss_G: 0.5023844242095947\n",
      "Epoch [6/10] Batch [10300/20655] Loss_D_A: 0.1732286810874939 Loss_D_B: 0.09429800510406494 Loss_G: 0.8181565999984741\n",
      "Epoch [6/10] Batch [10400/20655] Loss_D_A: 0.2791815400123596 Loss_D_B: 0.4101946949958801 Loss_G: 0.47308140993118286\n",
      "Epoch [6/10] Batch [10500/20655] Loss_D_A: 0.1088220477104187 Loss_D_B: 0.7895495295524597 Loss_G: 0.255679190158844\n",
      "Epoch [6/10] Batch [10600/20655] Loss_D_A: 0.9332451820373535 Loss_D_B: 0.715836763381958 Loss_G: 0.9626725912094116\n",
      "Epoch [6/10] Batch [10700/20655] Loss_D_A: 0.740044116973877 Loss_D_B: 0.8676775693893433 Loss_G: 0.9029557108879089\n",
      "Epoch [6/10] Batch [10800/20655] Loss_D_A: 0.03329825401306152 Loss_D_B: 0.9261770844459534 Loss_G: 0.8112412691116333\n",
      "Epoch [6/10] Batch [10900/20655] Loss_D_A: 0.3454741835594177 Loss_D_B: 0.7078767418861389 Loss_G: 0.29723137617111206\n",
      "Epoch [6/10] Batch [11000/20655] Loss_D_A: 0.7172765731811523 Loss_D_B: 0.4956135153770447 Loss_G: 0.8882994651794434\n",
      "Epoch [6/10] Batch [11100/20655] Loss_D_A: 0.42677217721939087 Loss_D_B: 0.9624719023704529 Loss_G: 0.4719770550727844\n",
      "Epoch [6/10] Batch [11200/20655] Loss_D_A: 0.2169848084449768 Loss_D_B: 0.34004151821136475 Loss_G: 0.3614940643310547\n",
      "Epoch [6/10] Batch [11300/20655] Loss_D_A: 0.9500385522842407 Loss_D_B: 0.923163890838623 Loss_G: 0.19360798597335815\n",
      "Epoch [6/10] Batch [11400/20655] Loss_D_A: 0.04524916410446167 Loss_D_B: 0.3157649636268616 Loss_G: 0.7353841066360474\n",
      "Epoch [6/10] Batch [11500/20655] Loss_D_A: 0.06356489658355713 Loss_D_B: 0.8288805484771729 Loss_G: 0.9303911328315735\n",
      "Epoch [6/10] Batch [11600/20655] Loss_D_A: 0.5192821621894836 Loss_D_B: 0.9094746708869934 Loss_G: 0.3566780090332031\n",
      "Epoch [6/10] Batch [11700/20655] Loss_D_A: 0.23007267713546753 Loss_D_B: 0.8758929967880249 Loss_G: 0.4835965037345886\n",
      "Epoch [6/10] Batch [11800/20655] Loss_D_A: 0.36793816089630127 Loss_D_B: 0.24354994297027588 Loss_G: 0.8181741833686829\n",
      "Epoch [6/10] Batch [11900/20655] Loss_D_A: 0.6437984704971313 Loss_D_B: 0.00993955135345459 Loss_G: 0.303226113319397\n",
      "Epoch [6/10] Batch [12000/20655] Loss_D_A: 0.9628157019615173 Loss_D_B: 0.3779890537261963 Loss_G: 0.3153298497200012\n",
      "Epoch [6/10] Batch [12100/20655] Loss_D_A: 0.2015247344970703 Loss_D_B: 0.20205157995224 Loss_G: 0.878659725189209\n",
      "Epoch [6/10] Batch [12200/20655] Loss_D_A: 0.4429450035095215 Loss_D_B: 0.20721209049224854 Loss_G: 0.5300463438034058\n",
      "Epoch [6/10] Batch [12300/20655] Loss_D_A: 0.7151636481285095 Loss_D_B: 0.5479254722595215 Loss_G: 0.27770769596099854\n",
      "Epoch [6/10] Batch [12400/20655] Loss_D_A: 0.9313864707946777 Loss_D_B: 0.07286077737808228 Loss_G: 0.15872985124588013\n",
      "Epoch [6/10] Batch [12500/20655] Loss_D_A: 0.4291132688522339 Loss_D_B: 0.1314084529876709 Loss_G: 0.7925885915756226\n",
      "Epoch [6/10] Batch [12600/20655] Loss_D_A: 0.06245088577270508 Loss_D_B: 0.719059944152832 Loss_G: 0.9079299569129944\n",
      "Epoch [6/10] Batch [12700/20655] Loss_D_A: 0.7449092268943787 Loss_D_B: 0.543730616569519 Loss_G: 0.5285808444023132\n",
      "Epoch [6/10] Batch [12800/20655] Loss_D_A: 0.984170138835907 Loss_D_B: 0.6023280024528503 Loss_G: 0.14404714107513428\n",
      "Epoch [6/10] Batch [12900/20655] Loss_D_A: 0.5122936964035034 Loss_D_B: 0.48526763916015625 Loss_G: 0.7240869402885437\n",
      "Epoch [6/10] Batch [13000/20655] Loss_D_A: 0.314473032951355 Loss_D_B: 0.9712881445884705 Loss_G: 0.7165217399597168\n",
      "Epoch [6/10] Batch [13100/20655] Loss_D_A: 0.9524571895599365 Loss_D_B: 0.5707225203514099 Loss_G: 0.2728939652442932\n",
      "Epoch [6/10] Batch [13200/20655] Loss_D_A: 0.11576849222183228 Loss_D_B: 0.045415520668029785 Loss_G: 0.7908704876899719\n",
      "Epoch [6/10] Batch [13300/20655] Loss_D_A: 0.8064150214195251 Loss_D_B: 0.05192387104034424 Loss_G: 0.8153821229934692\n",
      "Epoch [6/10] Batch [13400/20655] Loss_D_A: 0.25095129013061523 Loss_D_B: 0.1353069543838501 Loss_G: 0.576915979385376\n",
      "Epoch [6/10] Batch [13500/20655] Loss_D_A: 0.4293869733810425 Loss_D_B: 0.6426060199737549 Loss_G: 0.28050363063812256\n",
      "Epoch [6/10] Batch [13600/20655] Loss_D_A: 0.359788715839386 Loss_D_B: 0.05758398771286011 Loss_G: 0.23489755392074585\n",
      "Epoch [6/10] Batch [13700/20655] Loss_D_A: 0.766857385635376 Loss_D_B: 0.9997664093971252 Loss_G: 0.7150976061820984\n",
      "Epoch [6/10] Batch [13800/20655] Loss_D_A: 0.3566970229148865 Loss_D_B: 0.23459899425506592 Loss_G: 0.8445490598678589\n",
      "Epoch [6/10] Batch [13900/20655] Loss_D_A: 0.8985635638237 Loss_D_B: 0.8679825067520142 Loss_G: 0.649787962436676\n",
      "Epoch [6/10] Batch [14000/20655] Loss_D_A: 0.2693061828613281 Loss_D_B: 0.38315701484680176 Loss_G: 0.4884569048881531\n",
      "Epoch [6/10] Batch [14100/20655] Loss_D_A: 0.5985609889030457 Loss_D_B: 0.6928851008415222 Loss_G: 0.5838884115219116\n",
      "Epoch [6/10] Batch [14200/20655] Loss_D_A: 0.845133900642395 Loss_D_B: 0.8084304332733154 Loss_G: 0.20299971103668213\n",
      "Epoch [6/10] Batch [14300/20655] Loss_D_A: 0.6231945157051086 Loss_D_B: 0.46847349405288696 Loss_G: 0.5146675109863281\n",
      "Epoch [6/10] Batch [14400/20655] Loss_D_A: 0.3219413161277771 Loss_D_B: 0.7069094777107239 Loss_G: 0.91116863489151\n",
      "Epoch [6/10] Batch [14500/20655] Loss_D_A: 0.6732735633850098 Loss_D_B: 0.36264568567276 Loss_G: 0.05962777137756348\n",
      "Epoch [6/10] Batch [14600/20655] Loss_D_A: 0.06676644086837769 Loss_D_B: 0.8441645503044128 Loss_G: 0.953589141368866\n",
      "Epoch [6/10] Batch [14700/20655] Loss_D_A: 0.008217453956604004 Loss_D_B: 0.7436515092849731 Loss_G: 0.6835801005363464\n",
      "Epoch [6/10] Batch [14800/20655] Loss_D_A: 0.15083056688308716 Loss_D_B: 0.9597911834716797 Loss_G: 0.510039746761322\n",
      "Epoch [6/10] Batch [14900/20655] Loss_D_A: 0.7145087122917175 Loss_D_B: 0.31438589096069336 Loss_G: 0.7197414636611938\n",
      "Epoch [6/10] Batch [15000/20655] Loss_D_A: 0.5951903462409973 Loss_D_B: 0.5743973255157471 Loss_G: 0.4380289912223816\n",
      "Epoch [6/10] Batch [15100/20655] Loss_D_A: 0.42587608098983765 Loss_D_B: 0.8313844799995422 Loss_G: 0.07287812232971191\n",
      "Epoch [6/10] Batch [15200/20655] Loss_D_A: 0.8500118851661682 Loss_D_B: 0.9681670069694519 Loss_G: 0.48506754636764526\n",
      "Epoch [6/10] Batch [15300/20655] Loss_D_A: 0.7368378043174744 Loss_D_B: 0.39088404178619385 Loss_G: 0.6547057032585144\n",
      "Epoch [6/10] Batch [15400/20655] Loss_D_A: 0.48133790493011475 Loss_D_B: 0.9239919781684875 Loss_G: 0.6958666443824768\n",
      "Epoch [6/10] Batch [15500/20655] Loss_D_A: 0.30970537662506104 Loss_D_B: 0.8955536484718323 Loss_G: 0.39356809854507446\n",
      "Epoch [6/10] Batch [15600/20655] Loss_D_A: 0.33364731073379517 Loss_D_B: 0.8895734548568726 Loss_G: 0.003787994384765625\n",
      "Epoch [6/10] Batch [15700/20655] Loss_D_A: 0.3402135968208313 Loss_D_B: 0.32829588651657104 Loss_G: 0.4020634889602661\n",
      "Epoch [6/10] Batch [15800/20655] Loss_D_A: 0.7519247531890869 Loss_D_B: 0.5043722987174988 Loss_G: 0.3326556086540222\n",
      "Epoch [6/10] Batch [15900/20655] Loss_D_A: 0.7486135959625244 Loss_D_B: 0.19516849517822266 Loss_G: 0.759834349155426\n",
      "Epoch [6/10] Batch [16000/20655] Loss_D_A: 0.28283369541168213 Loss_D_B: 0.17360275983810425 Loss_G: 0.3199474811553955\n",
      "Epoch [6/10] Batch [16100/20655] Loss_D_A: 0.9751431941986084 Loss_D_B: 0.9831216931343079 Loss_G: 0.19449180364608765\n",
      "Epoch [6/10] Batch [16200/20655] Loss_D_A: 0.06256753206253052 Loss_D_B: 0.6056208610534668 Loss_G: 0.3983479142189026\n",
      "Epoch [6/10] Batch [16300/20655] Loss_D_A: 0.5405452251434326 Loss_D_B: 0.8283003568649292 Loss_G: 0.9232532382011414\n",
      "Epoch [6/10] Batch [16400/20655] Loss_D_A: 0.4800839424133301 Loss_D_B: 0.7581502199172974 Loss_G: 0.26640164852142334\n",
      "Epoch [6/10] Batch [16500/20655] Loss_D_A: 0.26074570417404175 Loss_D_B: 0.40688586235046387 Loss_G: 0.03708481788635254\n",
      "Epoch [6/10] Batch [16600/20655] Loss_D_A: 0.40279269218444824 Loss_D_B: 0.4018225073814392 Loss_G: 0.13523608446121216\n",
      "Epoch [6/10] Batch [16700/20655] Loss_D_A: 0.4816436767578125 Loss_D_B: 0.4145777225494385 Loss_G: 0.5535114407539368\n",
      "Epoch [6/10] Batch [16800/20655] Loss_D_A: 0.19036638736724854 Loss_D_B: 0.8096426725387573 Loss_G: 0.0560145378112793\n",
      "Epoch [6/10] Batch [16900/20655] Loss_D_A: 0.39155280590057373 Loss_D_B: 0.8748828768730164 Loss_G: 0.4909874200820923\n",
      "Epoch [6/10] Batch [17000/20655] Loss_D_A: 0.045126259326934814 Loss_D_B: 0.6013451218605042 Loss_G: 0.8525887727737427\n",
      "Epoch [6/10] Batch [17100/20655] Loss_D_A: 0.32993727922439575 Loss_D_B: 0.0735369324684143 Loss_G: 0.3935682773590088\n",
      "Epoch [6/10] Batch [17200/20655] Loss_D_A: 0.5584025382995605 Loss_D_B: 0.8826448917388916 Loss_G: 0.5895949602127075\n",
      "Epoch [6/10] Batch [17300/20655] Loss_D_A: 0.3877851366996765 Loss_D_B: 0.6413016319274902 Loss_G: 0.9469351172447205\n",
      "Epoch [6/10] Batch [17400/20655] Loss_D_A: 0.8348625898361206 Loss_D_B: 0.14848828315734863 Loss_G: 0.3497580885887146\n",
      "Epoch [6/10] Batch [17500/20655] Loss_D_A: 0.04510241746902466 Loss_D_B: 0.4262995719909668 Loss_G: 0.7122625708580017\n",
      "Epoch [6/10] Batch [17600/20655] Loss_D_A: 0.8303471207618713 Loss_D_B: 0.8748742341995239 Loss_G: 0.28595614433288574\n",
      "Epoch [6/10] Batch [17700/20655] Loss_D_A: 0.9507063031196594 Loss_D_B: 0.31277668476104736 Loss_G: 0.008656799793243408\n",
      "Epoch [6/10] Batch [17800/20655] Loss_D_A: 0.4198880195617676 Loss_D_B: 0.27031880617141724 Loss_G: 0.7003074288368225\n",
      "Epoch [6/10] Batch [17900/20655] Loss_D_A: 0.12167418003082275 Loss_D_B: 0.4924091100692749 Loss_G: 0.3436042070388794\n",
      "Epoch [6/10] Batch [18000/20655] Loss_D_A: 0.18294036388397217 Loss_D_B: 0.45694398880004883 Loss_G: 0.4374110698699951\n",
      "Epoch [6/10] Batch [18100/20655] Loss_D_A: 0.8540330529212952 Loss_D_B: 0.5571363568305969 Loss_G: 0.016312897205352783\n",
      "Epoch [6/10] Batch [18200/20655] Loss_D_A: 0.3580554127693176 Loss_D_B: 0.9124183058738708 Loss_G: 0.7520287036895752\n",
      "Epoch [6/10] Batch [18300/20655] Loss_D_A: 0.4041748046875 Loss_D_B: 0.8818422555923462 Loss_G: 0.1849682331085205\n",
      "Epoch [6/10] Batch [18400/20655] Loss_D_A: 0.42074018716812134 Loss_D_B: 0.32278650999069214 Loss_G: 0.979296088218689\n",
      "Epoch [6/10] Batch [18500/20655] Loss_D_A: 0.41008424758911133 Loss_D_B: 0.3070496916770935 Loss_G: 0.5208507180213928\n",
      "Epoch [6/10] Batch [18600/20655] Loss_D_A: 0.8763565421104431 Loss_D_B: 0.9605734348297119 Loss_G: 0.936336874961853\n",
      "Epoch [6/10] Batch [18700/20655] Loss_D_A: 0.7903674840927124 Loss_D_B: 0.29990971088409424 Loss_G: 0.8730278611183167\n",
      "Epoch [6/10] Batch [18800/20655] Loss_D_A: 0.9822323322296143 Loss_D_B: 0.6717926859855652 Loss_G: 0.5085583925247192\n",
      "Epoch [6/10] Batch [18900/20655] Loss_D_A: 0.802875816822052 Loss_D_B: 0.14450722932815552 Loss_G: 0.7947050333023071\n",
      "Epoch [6/10] Batch [19000/20655] Loss_D_A: 0.7797360420227051 Loss_D_B: 0.4439433217048645 Loss_G: 0.14405900239944458\n",
      "Epoch [6/10] Batch [19100/20655] Loss_D_A: 0.10186415910720825 Loss_D_B: 0.06190991401672363 Loss_G: 0.5432742238044739\n",
      "Epoch [6/10] Batch [19200/20655] Loss_D_A: 0.2758846879005432 Loss_D_B: 0.7382493019104004 Loss_G: 0.5149399042129517\n",
      "Epoch [6/10] Batch [19300/20655] Loss_D_A: 0.33223479986190796 Loss_D_B: 0.6670053005218506 Loss_G: 0.41012483835220337\n",
      "Epoch [6/10] Batch [19400/20655] Loss_D_A: 0.2857148051261902 Loss_D_B: 0.5597555637359619 Loss_G: 0.9233235716819763\n",
      "Epoch [6/10] Batch [19500/20655] Loss_D_A: 0.8471878170967102 Loss_D_B: 0.7433299422264099 Loss_G: 0.45148468017578125\n",
      "Epoch [6/10] Batch [19600/20655] Loss_D_A: 0.2721337080001831 Loss_D_B: 0.7525210380554199 Loss_G: 0.2284855842590332\n",
      "Epoch [6/10] Batch [19700/20655] Loss_D_A: 0.5510247349739075 Loss_D_B: 0.7293767333030701 Loss_G: 0.15411579608917236\n",
      "Epoch [6/10] Batch [19800/20655] Loss_D_A: 0.3087211847305298 Loss_D_B: 0.6398691534996033 Loss_G: 0.015214323997497559\n",
      "Epoch [6/10] Batch [19900/20655] Loss_D_A: 0.13448697328567505 Loss_D_B: 0.15362852811813354 Loss_G: 0.5572341680526733\n",
      "Epoch [6/10] Batch [20000/20655] Loss_D_A: 0.8170840740203857 Loss_D_B: 0.5030962824821472 Loss_G: 0.6806932091712952\n",
      "Epoch [6/10] Batch [20100/20655] Loss_D_A: 0.994371771812439 Loss_D_B: 0.2793962359428406 Loss_G: 0.2172139286994934\n",
      "Epoch [6/10] Batch [20200/20655] Loss_D_A: 0.10176998376846313 Loss_D_B: 0.6239761710166931 Loss_G: 0.26140332221984863\n",
      "Epoch [6/10] Batch [20300/20655] Loss_D_A: 0.6564726233482361 Loss_D_B: 0.022943973541259766 Loss_G: 0.44283074140548706\n",
      "Epoch [6/10] Batch [20400/20655] Loss_D_A: 0.08012747764587402 Loss_D_B: 0.9657047390937805 Loss_G: 0.8538584113121033\n",
      "Epoch [6/10] Batch [20500/20655] Loss_D_A: 0.7657700181007385 Loss_D_B: 0.9061875939369202 Loss_G: 0.5994250774383545\n",
      "Epoch [6/10] Batch [20600/20655] Loss_D_A: 0.02874833345413208 Loss_D_B: 0.21209698915481567 Loss_G: 0.7688124775886536\n",
      "Epoch [7/10] Batch [0/20655] Loss_D_A: 0.15960389375686646 Loss_D_B: 0.5763391852378845 Loss_G: 0.7755647897720337\n",
      "Epoch [7/10] Batch [0/20655] Loss_D_A: 0.5708022117614746 Loss_D_B: 0.8214590549468994 Loss_G: 0.02139735221862793\n",
      "Epoch [7/10] Batch [100/20655] Loss_D_A: 0.6632921099662781 Loss_D_B: 0.9386906027793884 Loss_G: 0.2677820920944214\n",
      "Epoch [7/10] Batch [200/20655] Loss_D_A: 0.08317160606384277 Loss_D_B: 0.9135867953300476 Loss_G: 0.633490264415741\n",
      "Epoch [7/10] Batch [300/20655] Loss_D_A: 0.9291186332702637 Loss_D_B: 0.6995759606361389 Loss_G: 0.9876313805580139\n",
      "Epoch [7/10] Batch [400/20655] Loss_D_A: 0.18293017148971558 Loss_D_B: 0.004506230354309082 Loss_G: 0.8635122179985046\n",
      "Epoch [7/10] Batch [500/20655] Loss_D_A: 0.12357538938522339 Loss_D_B: 0.9264885783195496 Loss_G: 0.9107426404953003\n",
      "Epoch [7/10] Batch [600/20655] Loss_D_A: 0.45392948389053345 Loss_D_B: 0.5055983662605286 Loss_G: 0.3374539613723755\n",
      "Epoch [7/10] Batch [700/20655] Loss_D_A: 0.31838393211364746 Loss_D_B: 0.900294840335846 Loss_G: 0.7676869034767151\n",
      "Epoch [7/10] Batch [800/20655] Loss_D_A: 0.8511564135551453 Loss_D_B: 0.08950293064117432 Loss_G: 0.9039453864097595\n",
      "Epoch [7/10] Batch [900/20655] Loss_D_A: 0.28148144483566284 Loss_D_B: 0.7757914662361145 Loss_G: 0.698036253452301\n",
      "Epoch [7/10] Batch [1000/20655] Loss_D_A: 0.14830881357192993 Loss_D_B: 0.32102257013320923 Loss_G: 0.7981030941009521\n",
      "Epoch [7/10] Batch [1100/20655] Loss_D_A: 0.3977143168449402 Loss_D_B: 0.9108187556266785 Loss_G: 0.07736587524414062\n",
      "Epoch [7/10] Batch [1200/20655] Loss_D_A: 0.3458312749862671 Loss_D_B: 0.9023165106773376 Loss_G: 0.5903981328010559\n",
      "Epoch [7/10] Batch [1300/20655] Loss_D_A: 0.1367584466934204 Loss_D_B: 0.12176012992858887 Loss_G: 0.02389127016067505\n",
      "Epoch [7/10] Batch [1400/20655] Loss_D_A: 0.5305862426757812 Loss_D_B: 0.19166302680969238 Loss_G: 0.5312808156013489\n",
      "Epoch [7/10] Batch [1500/20655] Loss_D_A: 0.27950340509414673 Loss_D_B: 0.1518370509147644 Loss_G: 0.6848492622375488\n",
      "Epoch [7/10] Batch [1600/20655] Loss_D_A: 0.6837310194969177 Loss_D_B: 0.06022089719772339 Loss_G: 0.049047231674194336\n",
      "Epoch [7/10] Batch [1700/20655] Loss_D_A: 0.6426056623458862 Loss_D_B: 0.3851192593574524 Loss_G: 0.2386224865913391\n",
      "Epoch [7/10] Batch [1800/20655] Loss_D_A: 0.8391537070274353 Loss_D_B: 0.4879881739616394 Loss_G: 0.3880344033241272\n",
      "Epoch [7/10] Batch [1900/20655] Loss_D_A: 0.3172459602355957 Loss_D_B: 0.25463300943374634 Loss_G: 0.2035800814628601\n",
      "Epoch [7/10] Batch [2000/20655] Loss_D_A: 0.17930370569229126 Loss_D_B: 0.36486560106277466 Loss_G: 0.49852192401885986\n",
      "Epoch [7/10] Batch [2100/20655] Loss_D_A: 0.3498455882072449 Loss_D_B: 0.39670008420944214 Loss_G: 0.31039977073669434\n",
      "Epoch [7/10] Batch [2200/20655] Loss_D_A: 0.47701555490493774 Loss_D_B: 0.46341413259506226 Loss_G: 0.2796352505683899\n",
      "Epoch [7/10] Batch [2300/20655] Loss_D_A: 0.8238535523414612 Loss_D_B: 0.9579062461853027 Loss_G: 0.041274845600128174\n",
      "Epoch [7/10] Batch [2400/20655] Loss_D_A: 0.9810027480125427 Loss_D_B: 0.7304508686065674 Loss_G: 0.4754515290260315\n",
      "Epoch [7/10] Batch [2500/20655] Loss_D_A: 0.748117983341217 Loss_D_B: 0.7980689406394958 Loss_G: 0.31818056106567383\n",
      "Epoch [7/10] Batch [2600/20655] Loss_D_A: 0.970031201839447 Loss_D_B: 0.05786782503128052 Loss_G: 0.18429988622665405\n",
      "Epoch [7/10] Batch [2700/20655] Loss_D_A: 0.18106365203857422 Loss_D_B: 0.039148569107055664 Loss_G: 0.493930459022522\n",
      "Epoch [7/10] Batch [2800/20655] Loss_D_A: 0.9282796382904053 Loss_D_B: 0.2636459469795227 Loss_G: 0.5961682796478271\n",
      "Epoch [7/10] Batch [2900/20655] Loss_D_A: 0.6055006384849548 Loss_D_B: 0.27204930782318115 Loss_G: 0.6769618391990662\n",
      "Epoch [7/10] Batch [3000/20655] Loss_D_A: 0.7969850301742554 Loss_D_B: 0.7263908982276917 Loss_G: 0.5011129379272461\n",
      "Epoch [7/10] Batch [3100/20655] Loss_D_A: 0.2904999256134033 Loss_D_B: 0.5493805408477783 Loss_G: 0.05898386240005493\n",
      "Epoch [7/10] Batch [3200/20655] Loss_D_A: 0.05923110246658325 Loss_D_B: 0.2581174373626709 Loss_G: 0.6306964755058289\n",
      "Epoch [7/10] Batch [3300/20655] Loss_D_A: 0.9399668574333191 Loss_D_B: 0.25501441955566406 Loss_G: 0.39384424686431885\n",
      "Epoch [7/10] Batch [3400/20655] Loss_D_A: 0.7132179737091064 Loss_D_B: 0.09957826137542725 Loss_G: 0.9415478706359863\n",
      "Epoch [7/10] Batch [3500/20655] Loss_D_A: 0.045249223709106445 Loss_D_B: 0.9801139831542969 Loss_G: 0.07470482587814331\n",
      "Epoch [7/10] Batch [3600/20655] Loss_D_A: 0.6473677754402161 Loss_D_B: 0.45532190799713135 Loss_G: 0.8635063767433167\n",
      "Epoch [7/10] Batch [3700/20655] Loss_D_A: 0.40730035305023193 Loss_D_B: 0.16167902946472168 Loss_G: 0.3937363624572754\n",
      "Epoch [7/10] Batch [3800/20655] Loss_D_A: 0.8606883883476257 Loss_D_B: 0.09930741786956787 Loss_G: 0.6537035703659058\n",
      "Epoch [7/10] Batch [3900/20655] Loss_D_A: 0.09610813856124878 Loss_D_B: 0.6771843433380127 Loss_G: 0.5241737365722656\n",
      "Epoch [7/10] Batch [4000/20655] Loss_D_A: 0.8831039667129517 Loss_D_B: 0.7928656935691833 Loss_G: 0.03763538599014282\n",
      "Epoch [7/10] Batch [4100/20655] Loss_D_A: 0.9022841453552246 Loss_D_B: 0.8415864706039429 Loss_G: 0.39461350440979004\n",
      "Epoch [7/10] Batch [4200/20655] Loss_D_A: 0.04207426309585571 Loss_D_B: 0.6436736583709717 Loss_G: 0.20134472846984863\n",
      "Epoch [7/10] Batch [4300/20655] Loss_D_A: 0.7149183750152588 Loss_D_B: 0.7712715268135071 Loss_G: 0.5669746994972229\n",
      "Epoch [7/10] Batch [4400/20655] Loss_D_A: 0.6960312128067017 Loss_D_B: 0.03707844018936157 Loss_G: 0.7996410727500916\n",
      "Epoch [7/10] Batch [4500/20655] Loss_D_A: 0.4104819893836975 Loss_D_B: 0.030594706535339355 Loss_G: 0.17186594009399414\n",
      "Epoch [7/10] Batch [4600/20655] Loss_D_A: 0.45276451110839844 Loss_D_B: 0.7880488038063049 Loss_G: 0.8903200030326843\n",
      "Epoch [7/10] Batch [4700/20655] Loss_D_A: 0.5062158107757568 Loss_D_B: 0.6824049353599548 Loss_G: 0.3364561200141907\n",
      "Epoch [7/10] Batch [4800/20655] Loss_D_A: 0.5046780109405518 Loss_D_B: 0.9421936869621277 Loss_G: 0.6010699272155762\n",
      "Epoch [7/10] Batch [4900/20655] Loss_D_A: 0.98219233751297 Loss_D_B: 0.42499780654907227 Loss_G: 0.731168270111084\n",
      "Epoch [7/10] Batch [5000/20655] Loss_D_A: 0.016003549098968506 Loss_D_B: 0.9495211243629456 Loss_G: 0.4944499731063843\n",
      "Epoch [7/10] Batch [5100/20655] Loss_D_A: 0.9415173530578613 Loss_D_B: 0.3190529942512512 Loss_G: 0.33168983459472656\n",
      "Epoch [7/10] Batch [5200/20655] Loss_D_A: 0.3405763506889343 Loss_D_B: 0.21460986137390137 Loss_G: 0.6519461274147034\n",
      "Epoch [7/10] Batch [5300/20655] Loss_D_A: 0.48129183053970337 Loss_D_B: 0.12447810173034668 Loss_G: 0.7418068647384644\n",
      "Epoch [7/10] Batch [5400/20655] Loss_D_A: 0.6101988554000854 Loss_D_B: 0.91133052110672 Loss_G: 0.9203962683677673\n",
      "Epoch [7/10] Batch [5500/20655] Loss_D_A: 0.8680269122123718 Loss_D_B: 0.8698306679725647 Loss_G: 0.8046608567237854\n",
      "Epoch [7/10] Batch [5600/20655] Loss_D_A: 0.7281151413917542 Loss_D_B: 0.6805630922317505 Loss_G: 0.661713719367981\n",
      "Epoch [7/10] Batch [5700/20655] Loss_D_A: 0.5654063820838928 Loss_D_B: 0.6826257109642029 Loss_G: 0.8849826455116272\n",
      "Epoch [7/10] Batch [5800/20655] Loss_D_A: 0.7586520910263062 Loss_D_B: 0.43364423513412476 Loss_G: 0.31416094303131104\n",
      "Epoch [7/10] Batch [5900/20655] Loss_D_A: 0.4502856135368347 Loss_D_B: 0.4298751950263977 Loss_G: 0.12265467643737793\n",
      "Epoch [7/10] Batch [6000/20655] Loss_D_A: 0.9780080318450928 Loss_D_B: 0.4479849934577942 Loss_G: 0.10517382621765137\n",
      "Epoch [7/10] Batch [6100/20655] Loss_D_A: 0.3358743190765381 Loss_D_B: 0.5815730690956116 Loss_G: 0.7235491871833801\n",
      "Epoch [7/10] Batch [6200/20655] Loss_D_A: 0.4525582790374756 Loss_D_B: 0.07478421926498413 Loss_G: 0.47381591796875\n",
      "Epoch [7/10] Batch [6300/20655] Loss_D_A: 0.8841435313224792 Loss_D_B: 0.846234917640686 Loss_G: 0.38083600997924805\n",
      "Epoch [7/10] Batch [6400/20655] Loss_D_A: 0.8799861669540405 Loss_D_B: 0.4705246686935425 Loss_G: 0.845682680606842\n",
      "Epoch [7/10] Batch [6500/20655] Loss_D_A: 0.6059959530830383 Loss_D_B: 0.5909284353256226 Loss_G: 0.680083155632019\n",
      "Epoch [7/10] Batch [6600/20655] Loss_D_A: 0.05552363395690918 Loss_D_B: 0.8545286655426025 Loss_G: 0.44527536630630493\n",
      "Epoch [7/10] Batch [6700/20655] Loss_D_A: 0.848721444606781 Loss_D_B: 0.8451518416404724 Loss_G: 0.7156366109848022\n",
      "Epoch [7/10] Batch [6800/20655] Loss_D_A: 0.5925509929656982 Loss_D_B: 0.9540546536445618 Loss_G: 0.35258668661117554\n",
      "Epoch [7/10] Batch [6900/20655] Loss_D_A: 0.209114670753479 Loss_D_B: 0.7356542944908142 Loss_G: 0.8178467750549316\n",
      "Epoch [7/10] Batch [7000/20655] Loss_D_A: 0.9552002549171448 Loss_D_B: 0.8668060302734375 Loss_G: 0.41838300228118896\n",
      "Epoch [7/10] Batch [7100/20655] Loss_D_A: 0.502350389957428 Loss_D_B: 0.677976667881012 Loss_G: 0.18591541051864624\n",
      "Epoch [7/10] Batch [7200/20655] Loss_D_A: 0.7442817687988281 Loss_D_B: 0.748202383518219 Loss_G: 0.9277055263519287\n",
      "Epoch [7/10] Batch [7300/20655] Loss_D_A: 0.6656359434127808 Loss_D_B: 0.6282253265380859 Loss_G: 0.3756979703903198\n",
      "Epoch [7/10] Batch [7400/20655] Loss_D_A: 0.34652572870254517 Loss_D_B: 0.7106153964996338 Loss_G: 0.89906245470047\n",
      "Epoch [7/10] Batch [7500/20655] Loss_D_A: 0.8206900954246521 Loss_D_B: 0.8848044872283936 Loss_G: 0.7126262784004211\n",
      "Epoch [7/10] Batch [7600/20655] Loss_D_A: 0.5260167717933655 Loss_D_B: 0.7061418294906616 Loss_G: 0.9406814575195312\n",
      "Epoch [7/10] Batch [7700/20655] Loss_D_A: 0.5208407044410706 Loss_D_B: 0.6976025700569153 Loss_G: 0.808040201663971\n",
      "Epoch [7/10] Batch [7800/20655] Loss_D_A: 0.9158100485801697 Loss_D_B: 0.16135835647583008 Loss_G: 0.5545671582221985\n",
      "Epoch [7/10] Batch [7900/20655] Loss_D_A: 0.6320749521255493 Loss_D_B: 0.4331769347190857 Loss_G: 0.5234037041664124\n",
      "Epoch [7/10] Batch [8000/20655] Loss_D_A: 0.8930652737617493 Loss_D_B: 0.9388352632522583 Loss_G: 0.27969270944595337\n",
      "Epoch [7/10] Batch [8100/20655] Loss_D_A: 0.817557156085968 Loss_D_B: 0.3674049973487854 Loss_G: 0.8924169540405273\n",
      "Epoch [7/10] Batch [8200/20655] Loss_D_A: 0.4318639039993286 Loss_D_B: 0.4837576746940613 Loss_G: 0.37138831615448\n",
      "Epoch [7/10] Batch [8300/20655] Loss_D_A: 0.02836155891418457 Loss_D_B: 0.5134671926498413 Loss_G: 0.40363675355911255\n",
      "Epoch [7/10] Batch [8400/20655] Loss_D_A: 0.9350346326828003 Loss_D_B: 0.32700085639953613 Loss_G: 0.36344432830810547\n",
      "Epoch [7/10] Batch [8500/20655] Loss_D_A: 0.557490348815918 Loss_D_B: 0.4518292546272278 Loss_G: 0.7546617388725281\n",
      "Epoch [7/10] Batch [8600/20655] Loss_D_A: 0.933643102645874 Loss_D_B: 0.3935038447380066 Loss_G: 0.511976957321167\n",
      "Epoch [7/10] Batch [8700/20655] Loss_D_A: 0.3912912607192993 Loss_D_B: 0.021018803119659424 Loss_G: 0.9938492178916931\n",
      "Epoch [7/10] Batch [8800/20655] Loss_D_A: 0.32843613624572754 Loss_D_B: 0.8508234024047852 Loss_G: 0.6726944446563721\n",
      "Epoch [7/10] Batch [8900/20655] Loss_D_A: 0.4011960029602051 Loss_D_B: 0.2932606339454651 Loss_G: 0.4497867226600647\n",
      "Epoch [7/10] Batch [9000/20655] Loss_D_A: 0.8861634135246277 Loss_D_B: 0.3039690852165222 Loss_G: 0.6920410394668579\n",
      "Epoch [7/10] Batch [9100/20655] Loss_D_A: 0.34333765506744385 Loss_D_B: 0.26517432928085327 Loss_G: 0.12924468517303467\n",
      "Epoch [7/10] Batch [9200/20655] Loss_D_A: 0.6699180006980896 Loss_D_B: 0.4508540630340576 Loss_G: 0.0018916726112365723\n",
      "Epoch [7/10] Batch [9300/20655] Loss_D_A: 0.1820920705795288 Loss_D_B: 0.34634870290756226 Loss_G: 0.589096188545227\n",
      "Epoch [7/10] Batch [9400/20655] Loss_D_A: 0.0071305036544799805 Loss_D_B: 0.7154349088668823 Loss_G: 0.824983537197113\n",
      "Epoch [7/10] Batch [9500/20655] Loss_D_A: 0.7260108590126038 Loss_D_B: 0.5690193176269531 Loss_G: 0.4553605914115906\n",
      "Epoch [7/10] Batch [9600/20655] Loss_D_A: 0.24622023105621338 Loss_D_B: 0.3855268359184265 Loss_G: 0.19558823108673096\n",
      "Epoch [7/10] Batch [9700/20655] Loss_D_A: 0.27923816442489624 Loss_D_B: 0.12229233980178833 Loss_G: 0.406050443649292\n",
      "Epoch [7/10] Batch [9800/20655] Loss_D_A: 0.4965950846672058 Loss_D_B: 0.7262073755264282 Loss_G: 0.855651319026947\n",
      "Epoch [7/10] Batch [9900/20655] Loss_D_A: 0.5159459114074707 Loss_D_B: 0.9838370084762573 Loss_G: 0.4515594244003296\n",
      "Epoch [7/10] Batch [10000/20655] Loss_D_A: 0.10814058780670166 Loss_D_B: 0.6698559522628784 Loss_G: 0.07492399215698242\n",
      "Epoch [7/10] Batch [10100/20655] Loss_D_A: 0.666984498500824 Loss_D_B: 0.8157833218574524 Loss_G: 0.21704119443893433\n",
      "Epoch [7/10] Batch [10200/20655] Loss_D_A: 0.35533422231674194 Loss_D_B: 0.23383933305740356 Loss_G: 0.6181182265281677\n",
      "Epoch [7/10] Batch [10300/20655] Loss_D_A: 0.7100926041603088 Loss_D_B: 0.4694700837135315 Loss_G: 0.6591138243675232\n",
      "Epoch [7/10] Batch [10400/20655] Loss_D_A: 0.366687536239624 Loss_D_B: 0.3923521041870117 Loss_G: 0.5420339107513428\n",
      "Epoch [7/10] Batch [10500/20655] Loss_D_A: 0.6505438089370728 Loss_D_B: 0.1642560362815857 Loss_G: 0.08814847469329834\n",
      "Epoch [7/10] Batch [10600/20655] Loss_D_A: 0.5633897185325623 Loss_D_B: 0.23842889070510864 Loss_G: 0.20326733589172363\n",
      "Epoch [7/10] Batch [10700/20655] Loss_D_A: 0.6644410490989685 Loss_D_B: 0.46191179752349854 Loss_G: 0.47218620777130127\n",
      "Epoch [7/10] Batch [10800/20655] Loss_D_A: 0.1327720284461975 Loss_D_B: 0.8030428886413574 Loss_G: 0.9512616991996765\n",
      "Epoch [7/10] Batch [10900/20655] Loss_D_A: 0.7960787415504456 Loss_D_B: 0.33645379543304443 Loss_G: 0.33364778757095337\n",
      "Epoch [7/10] Batch [11000/20655] Loss_D_A: 0.8426958322525024 Loss_D_B: 0.5097255110740662 Loss_G: 0.4958581328392029\n",
      "Epoch [7/10] Batch [11100/20655] Loss_D_A: 0.27189701795578003 Loss_D_B: 0.007316648960113525 Loss_G: 0.06347227096557617\n",
      "Epoch [7/10] Batch [11200/20655] Loss_D_A: 0.288993239402771 Loss_D_B: 0.8046894669532776 Loss_G: 0.27810460329055786\n",
      "Epoch [7/10] Batch [11300/20655] Loss_D_A: 0.9050419330596924 Loss_D_B: 0.4183972477912903 Loss_G: 0.4665430188179016\n",
      "Epoch [7/10] Batch [11400/20655] Loss_D_A: 0.5901346206665039 Loss_D_B: 0.20935362577438354 Loss_G: 0.8277274370193481\n",
      "Epoch [7/10] Batch [11500/20655] Loss_D_A: 0.5818564295768738 Loss_D_B: 0.21796220541000366 Loss_G: 0.9449703097343445\n",
      "Epoch [7/10] Batch [11600/20655] Loss_D_A: 0.8689661622047424 Loss_D_B: 0.7733426690101624 Loss_G: 0.2550554871559143\n",
      "Epoch [7/10] Batch [11700/20655] Loss_D_A: 0.9512393474578857 Loss_D_B: 0.13569235801696777 Loss_G: 0.3600934147834778\n",
      "Epoch [7/10] Batch [11800/20655] Loss_D_A: 0.2713838219642639 Loss_D_B: 0.6272231340408325 Loss_G: 0.6860926747322083\n",
      "Epoch [7/10] Batch [11900/20655] Loss_D_A: 0.9005596041679382 Loss_D_B: 0.6602073907852173 Loss_G: 0.5188649296760559\n",
      "Epoch [7/10] Batch [12000/20655] Loss_D_A: 0.09648537635803223 Loss_D_B: 0.06050455570220947 Loss_G: 0.5434685349464417\n",
      "Epoch [7/10] Batch [12100/20655] Loss_D_A: 0.8740782141685486 Loss_D_B: 0.8075926303863525 Loss_G: 0.14491403102874756\n",
      "Epoch [7/10] Batch [12200/20655] Loss_D_A: 0.747747540473938 Loss_D_B: 0.4828091859817505 Loss_G: 0.3551846742630005\n",
      "Epoch [7/10] Batch [12300/20655] Loss_D_A: 0.9721744060516357 Loss_D_B: 0.8512060642242432 Loss_G: 0.28975141048431396\n",
      "Epoch [7/10] Batch [12400/20655] Loss_D_A: 0.47994643449783325 Loss_D_B: 0.439516544342041 Loss_G: 0.26602256298065186\n",
      "Epoch [7/10] Batch [12500/20655] Loss_D_A: 0.9779371023178101 Loss_D_B: 0.28332680463790894 Loss_G: 0.40991002321243286\n",
      "Epoch [7/10] Batch [12600/20655] Loss_D_A: 0.3785771131515503 Loss_D_B: 0.16088074445724487 Loss_G: 0.04776918888092041\n",
      "Epoch [7/10] Batch [12700/20655] Loss_D_A: 0.74746173620224 Loss_D_B: 0.9358834624290466 Loss_G: 0.6308547258377075\n",
      "Epoch [7/10] Batch [12800/20655] Loss_D_A: 0.36255502700805664 Loss_D_B: 0.18397563695907593 Loss_G: 0.5528252124786377\n",
      "Epoch [7/10] Batch [12900/20655] Loss_D_A: 0.6542739868164062 Loss_D_B: 0.7571437358856201 Loss_G: 0.791392982006073\n",
      "Epoch [7/10] Batch [13000/20655] Loss_D_A: 0.4600537419319153 Loss_D_B: 0.6373944282531738 Loss_G: 0.9873967170715332\n",
      "Epoch [7/10] Batch [13100/20655] Loss_D_A: 0.8318158388137817 Loss_D_B: 0.639209508895874 Loss_G: 0.2818197011947632\n",
      "Epoch [7/10] Batch [13200/20655] Loss_D_A: 0.6565812230110168 Loss_D_B: 0.8038086891174316 Loss_G: 0.5667794346809387\n",
      "Epoch [7/10] Batch [13300/20655] Loss_D_A: 0.7357519268989563 Loss_D_B: 0.34591901302337646 Loss_G: 0.04886770248413086\n",
      "Epoch [7/10] Batch [13400/20655] Loss_D_A: 0.34066057205200195 Loss_D_B: 0.35979199409484863 Loss_G: 0.4459657669067383\n",
      "Epoch [7/10] Batch [13500/20655] Loss_D_A: 0.4247099757194519 Loss_D_B: 0.7705225348472595 Loss_G: 0.7337836623191833\n",
      "Epoch [7/10] Batch [13600/20655] Loss_D_A: 0.17607522010803223 Loss_D_B: 0.9827474355697632 Loss_G: 0.9313688278198242\n",
      "Epoch [7/10] Batch [13700/20655] Loss_D_A: 0.4553413391113281 Loss_D_B: 0.48822933435440063 Loss_G: 0.8305903077125549\n",
      "Epoch [7/10] Batch [13800/20655] Loss_D_A: 0.7111122608184814 Loss_D_B: 0.4106817841529846 Loss_G: 0.1495891809463501\n",
      "Epoch [7/10] Batch [13900/20655] Loss_D_A: 0.8715739846229553 Loss_D_B: 0.1818535327911377 Loss_G: 0.9855657815933228\n",
      "Epoch [7/10] Batch [14000/20655] Loss_D_A: 0.40544891357421875 Loss_D_B: 0.5403102040290833 Loss_G: 0.6382002234458923\n",
      "Epoch [7/10] Batch [14100/20655] Loss_D_A: 0.7930283546447754 Loss_D_B: 0.540505588054657 Loss_G: 0.5883594751358032\n",
      "Epoch [7/10] Batch [14200/20655] Loss_D_A: 0.0072397589683532715 Loss_D_B: 0.977539598941803 Loss_G: 0.07255995273590088\n",
      "Epoch [7/10] Batch [14300/20655] Loss_D_A: 0.6221554279327393 Loss_D_B: 0.8805776834487915 Loss_G: 0.5261811017990112\n",
      "Epoch [7/10] Batch [14400/20655] Loss_D_A: 0.14147192239761353 Loss_D_B: 0.9764763116836548 Loss_G: 0.34022200107574463\n",
      "Epoch [7/10] Batch [14500/20655] Loss_D_A: 0.6300560832023621 Loss_D_B: 0.9496719837188721 Loss_G: 0.8157585859298706\n",
      "Epoch [7/10] Batch [14600/20655] Loss_D_A: 0.7026064395904541 Loss_D_B: 0.8296442627906799 Loss_G: 0.8902803659439087\n",
      "Epoch [7/10] Batch [14700/20655] Loss_D_A: 0.207045316696167 Loss_D_B: 0.6669911742210388 Loss_G: 0.07470512390136719\n",
      "Epoch [7/10] Batch [14800/20655] Loss_D_A: 0.2233688235282898 Loss_D_B: 0.6622551083564758 Loss_G: 0.05942511558532715\n",
      "Epoch [7/10] Batch [14900/20655] Loss_D_A: 0.5629849433898926 Loss_D_B: 0.2557395100593567 Loss_G: 0.8608019948005676\n",
      "Epoch [7/10] Batch [15000/20655] Loss_D_A: 0.00022608041763305664 Loss_D_B: 0.6210253238677979 Loss_G: 0.4916346073150635\n",
      "Epoch [7/10] Batch [15100/20655] Loss_D_A: 0.6543613076210022 Loss_D_B: 0.14713609218597412 Loss_G: 0.8897336721420288\n",
      "Epoch [7/10] Batch [15200/20655] Loss_D_A: 0.6405888795852661 Loss_D_B: 0.4415077567100525 Loss_G: 0.8025392293930054\n",
      "Epoch [7/10] Batch [15300/20655] Loss_D_A: 0.2457757592201233 Loss_D_B: 0.791159987449646 Loss_G: 0.6611860394477844\n",
      "Epoch [7/10] Batch [15400/20655] Loss_D_A: 0.052668869495391846 Loss_D_B: 0.041342854499816895 Loss_G: 0.801715612411499\n",
      "Epoch [7/10] Batch [15500/20655] Loss_D_A: 0.3533326983451843 Loss_D_B: 0.23230093717575073 Loss_G: 0.9140895009040833\n",
      "Epoch [7/10] Batch [15600/20655] Loss_D_A: 0.17930269241333008 Loss_D_B: 0.22966516017913818 Loss_G: 0.06792449951171875\n",
      "Epoch [7/10] Batch [15700/20655] Loss_D_A: 0.18948602676391602 Loss_D_B: 0.5524004697799683 Loss_G: 0.23511868715286255\n",
      "Epoch [7/10] Batch [15800/20655] Loss_D_A: 0.1343504786491394 Loss_D_B: 0.012918651103973389 Loss_G: 0.899374783039093\n",
      "Epoch [7/10] Batch [15900/20655] Loss_D_A: 0.1574181318283081 Loss_D_B: 0.5883428454399109 Loss_G: 0.8212501406669617\n",
      "Epoch [7/10] Batch [16000/20655] Loss_D_A: 0.1415867805480957 Loss_D_B: 0.8416545987129211 Loss_G: 0.3663598895072937\n",
      "Epoch [7/10] Batch [16100/20655] Loss_D_A: 0.6305804252624512 Loss_D_B: 0.7526503205299377 Loss_G: 0.2664420008659363\n",
      "Epoch [7/10] Batch [16200/20655] Loss_D_A: 0.7454509139060974 Loss_D_B: 0.647257924079895 Loss_G: 0.670428454875946\n",
      "Epoch [7/10] Batch [16300/20655] Loss_D_A: 0.7531489729881287 Loss_D_B: 0.1540502905845642 Loss_G: 0.6504178047180176\n",
      "Epoch [7/10] Batch [16400/20655] Loss_D_A: 0.3796008825302124 Loss_D_B: 0.6910101771354675 Loss_G: 0.7571964859962463\n",
      "Epoch [7/10] Batch [16500/20655] Loss_D_A: 0.9877200126647949 Loss_D_B: 0.05301576852798462 Loss_G: 0.22738009691238403\n",
      "Epoch [7/10] Batch [16600/20655] Loss_D_A: 0.947812020778656 Loss_D_B: 0.5280894041061401 Loss_G: 0.5928587317466736\n",
      "Epoch [7/10] Batch [16700/20655] Loss_D_A: 0.1253068447113037 Loss_D_B: 0.6147765517234802 Loss_G: 0.2656633257865906\n",
      "Epoch [7/10] Batch [16800/20655] Loss_D_A: 0.6551231741905212 Loss_D_B: 0.28640514612197876 Loss_G: 0.6337234377861023\n",
      "Epoch [7/10] Batch [16900/20655] Loss_D_A: 0.42207300662994385 Loss_D_B: 0.3165956735610962 Loss_G: 0.9337407946586609\n",
      "Epoch [7/10] Batch [17000/20655] Loss_D_A: 0.4053650498390198 Loss_D_B: 0.5862195491790771 Loss_G: 0.39480650424957275\n",
      "Epoch [7/10] Batch [17100/20655] Loss_D_A: 0.37323522567749023 Loss_D_B: 0.2164163589477539 Loss_G: 0.20297092199325562\n",
      "Epoch [7/10] Batch [17200/20655] Loss_D_A: 0.7963883280754089 Loss_D_B: 0.06558352708816528 Loss_G: 0.13849598169326782\n",
      "Epoch [7/10] Batch [17300/20655] Loss_D_A: 0.857269823551178 Loss_D_B: 0.02372145652770996 Loss_G: 0.32502061128616333\n",
      "Epoch [7/10] Batch [17400/20655] Loss_D_A: 0.7791150808334351 Loss_D_B: 0.06680440902709961 Loss_G: 0.08913230895996094\n",
      "Epoch [7/10] Batch [17500/20655] Loss_D_A: 0.6224749684333801 Loss_D_B: 0.7713698148727417 Loss_G: 0.5409929156303406\n",
      "Epoch [7/10] Batch [17600/20655] Loss_D_A: 0.7344037890434265 Loss_D_B: 0.5476239919662476 Loss_G: 0.5538994669914246\n",
      "Epoch [7/10] Batch [17700/20655] Loss_D_A: 0.3397635221481323 Loss_D_B: 0.11207377910614014 Loss_G: 0.45028990507125854\n",
      "Epoch [7/10] Batch [17800/20655] Loss_D_A: 0.5571869015693665 Loss_D_B: 0.36924946308135986 Loss_G: 0.0303075909614563\n",
      "Epoch [7/10] Batch [17900/20655] Loss_D_A: 0.5405839681625366 Loss_D_B: 0.5880811810493469 Loss_G: 0.5269196033477783\n",
      "Epoch [7/10] Batch [18000/20655] Loss_D_A: 0.39751750230789185 Loss_D_B: 0.745892345905304 Loss_G: 0.5795659422874451\n",
      "Epoch [7/10] Batch [18100/20655] Loss_D_A: 0.25826287269592285 Loss_D_B: 0.09058469533920288 Loss_G: 0.6952869892120361\n",
      "Epoch [7/10] Batch [18200/20655] Loss_D_A: 0.5367627739906311 Loss_D_B: 0.34349119663238525 Loss_G: 0.9490004777908325\n",
      "Epoch [7/10] Batch [18300/20655] Loss_D_A: 0.5120705962181091 Loss_D_B: 0.9280986189842224 Loss_G: 0.3754214644432068\n",
      "Epoch [7/10] Batch [18400/20655] Loss_D_A: 0.7376052141189575 Loss_D_B: 0.2263699173927307 Loss_G: 0.6613296866416931\n",
      "Epoch [7/10] Batch [18500/20655] Loss_D_A: 0.8013141751289368 Loss_D_B: 0.28483498096466064 Loss_G: 0.7170825600624084\n",
      "Epoch [7/10] Batch [18600/20655] Loss_D_A: 0.06360828876495361 Loss_D_B: 0.73866206407547 Loss_G: 0.8732401132583618\n",
      "Epoch [7/10] Batch [18700/20655] Loss_D_A: 0.9662213921546936 Loss_D_B: 0.09881788492202759 Loss_G: 0.7335944771766663\n",
      "Epoch [7/10] Batch [18800/20655] Loss_D_A: 0.9673717617988586 Loss_D_B: 0.471560001373291 Loss_G: 0.25585466623306274\n",
      "Epoch [7/10] Batch [18900/20655] Loss_D_A: 0.23135453462600708 Loss_D_B: 0.6347517371177673 Loss_G: 0.017315208911895752\n",
      "Epoch [7/10] Batch [19000/20655] Loss_D_A: 0.8541245460510254 Loss_D_B: 0.7382248640060425 Loss_G: 0.12488657236099243\n",
      "Epoch [7/10] Batch [19100/20655] Loss_D_A: 0.25152039527893066 Loss_D_B: 0.05250883102416992 Loss_G: 0.3152387738227844\n",
      "Epoch [7/10] Batch [19200/20655] Loss_D_A: 0.4805675745010376 Loss_D_B: 0.42120200395584106 Loss_G: 0.08281874656677246\n",
      "Epoch [7/10] Batch [19300/20655] Loss_D_A: 0.8699755072593689 Loss_D_B: 0.7149850726127625 Loss_G: 0.09336155652999878\n",
      "Epoch [7/10] Batch [19400/20655] Loss_D_A: 0.8447197079658508 Loss_D_B: 0.5203114748001099 Loss_G: 0.39915674924850464\n",
      "Epoch [7/10] Batch [19500/20655] Loss_D_A: 0.9245776534080505 Loss_D_B: 0.4746876358985901 Loss_G: 0.383012056350708\n",
      "Epoch [7/10] Batch [19600/20655] Loss_D_A: 0.4863281846046448 Loss_D_B: 0.4417186975479126 Loss_G: 0.016498088836669922\n",
      "Epoch [7/10] Batch [19700/20655] Loss_D_A: 0.200433611869812 Loss_D_B: 0.4353511929512024 Loss_G: 0.050667762756347656\n",
      "Epoch [7/10] Batch [19800/20655] Loss_D_A: 0.4198271632194519 Loss_D_B: 0.21431171894073486 Loss_G: 0.9641213417053223\n",
      "Epoch [7/10] Batch [19900/20655] Loss_D_A: 0.7416319847106934 Loss_D_B: 0.07417851686477661 Loss_G: 0.42210233211517334\n",
      "Epoch [7/10] Batch [20000/20655] Loss_D_A: 0.2995765805244446 Loss_D_B: 0.49555253982543945 Loss_G: 0.8967366218566895\n",
      "Epoch [7/10] Batch [20100/20655] Loss_D_A: 0.2344033122062683 Loss_D_B: 0.416179358959198 Loss_G: 0.0012913942337036133\n",
      "Epoch [7/10] Batch [20200/20655] Loss_D_A: 0.9396525621414185 Loss_D_B: 0.9372773170471191 Loss_G: 0.669124186038971\n",
      "Epoch [7/10] Batch [20300/20655] Loss_D_A: 0.6357652544975281 Loss_D_B: 0.5690984129905701 Loss_G: 0.7034927010536194\n",
      "Epoch [7/10] Batch [20400/20655] Loss_D_A: 0.2329232096672058 Loss_D_B: 0.22542768716812134 Loss_G: 0.787596583366394\n",
      "Epoch [7/10] Batch [20500/20655] Loss_D_A: 0.2902582883834839 Loss_D_B: 0.707739531993866 Loss_G: 0.3638414740562439\n",
      "Epoch [7/10] Batch [20600/20655] Loss_D_A: 0.48649299144744873 Loss_D_B: 0.024009764194488525 Loss_G: 0.6488000154495239\n",
      "Epoch [8/10] Batch [0/20655] Loss_D_A: 0.5708022117614746 Loss_D_B: 0.8214590549468994 Loss_G: 0.02139735221862793\n",
      "Epoch [8/10] Batch [0/20655] Loss_D_A: 0.13780421018600464 Loss_D_B: 0.9845083951950073 Loss_G: 0.6054011583328247\n",
      "Epoch [8/10] Batch [100/20655] Loss_D_A: 0.9107910394668579 Loss_D_B: 0.49640101194381714 Loss_G: 0.18297696113586426\n",
      "Epoch [8/10] Batch [200/20655] Loss_D_A: 0.12089908123016357 Loss_D_B: 0.5504465699195862 Loss_G: 0.42054545879364014\n",
      "Epoch [8/10] Batch [300/20655] Loss_D_A: 0.20889025926589966 Loss_D_B: 0.9550793766975403 Loss_G: 0.9169038534164429\n",
      "Epoch [8/10] Batch [400/20655] Loss_D_A: 0.6115859150886536 Loss_D_B: 0.9903565049171448 Loss_G: 0.5523858070373535\n",
      "Epoch [8/10] Batch [500/20655] Loss_D_A: 0.3084831237792969 Loss_D_B: 0.9810227155685425 Loss_G: 0.865212082862854\n",
      "Epoch [8/10] Batch [600/20655] Loss_D_A: 0.1184462308883667 Loss_D_B: 0.7872015237808228 Loss_G: 0.17893356084823608\n",
      "Epoch [8/10] Batch [700/20655] Loss_D_A: 0.1069527268409729 Loss_D_B: 0.48146963119506836 Loss_G: 0.12858229875564575\n",
      "Epoch [8/10] Batch [800/20655] Loss_D_A: 0.8581745624542236 Loss_D_B: 0.43093442916870117 Loss_G: 0.9067412614822388\n",
      "Epoch [8/10] Batch [900/20655] Loss_D_A: 0.0677136778831482 Loss_D_B: 0.43204957246780396 Loss_G: 0.6374002695083618\n",
      "Epoch [8/10] Batch [1000/20655] Loss_D_A: 0.7408623099327087 Loss_D_B: 0.27268922328948975 Loss_G: 0.687123715877533\n",
      "Epoch [8/10] Batch [1100/20655] Loss_D_A: 0.8627691864967346 Loss_D_B: 0.5847634673118591 Loss_G: 0.03991144895553589\n",
      "Epoch [8/10] Batch [1200/20655] Loss_D_A: 0.9777076840400696 Loss_D_B: 0.5619878768920898 Loss_G: 0.5192768573760986\n",
      "Epoch [8/10] Batch [1300/20655] Loss_D_A: 0.5280396342277527 Loss_D_B: 0.8232733607292175 Loss_G: 0.5332794785499573\n",
      "Epoch [8/10] Batch [1400/20655] Loss_D_A: 0.746739387512207 Loss_D_B: 0.3001953959465027 Loss_G: 0.8127431869506836\n",
      "Epoch [8/10] Batch [1500/20655] Loss_D_A: 0.3805652856826782 Loss_D_B: 0.6343710422515869 Loss_G: 0.892479419708252\n",
      "Epoch [8/10] Batch [1600/20655] Loss_D_A: 0.31434935331344604 Loss_D_B: 0.09411227703094482 Loss_G: 0.9404075741767883\n",
      "Epoch [8/10] Batch [1700/20655] Loss_D_A: 0.636694610118866 Loss_D_B: 0.8108760714530945 Loss_G: 0.5670344829559326\n",
      "Epoch [8/10] Batch [1800/20655] Loss_D_A: 0.8217846155166626 Loss_D_B: 0.419944703578949 Loss_G: 0.31739455461502075\n",
      "Epoch [8/10] Batch [1900/20655] Loss_D_A: 0.058494627475738525 Loss_D_B: 0.2653615474700928 Loss_G: 0.42247265577316284\n",
      "Epoch [8/10] Batch [2000/20655] Loss_D_A: 0.0886569619178772 Loss_D_B: 0.6842359900474548 Loss_G: 0.9503573179244995\n",
      "Epoch [8/10] Batch [2100/20655] Loss_D_A: 0.3798004388809204 Loss_D_B: 0.508560061454773 Loss_G: 0.39175742864608765\n",
      "Epoch [8/10] Batch [2200/20655] Loss_D_A: 0.3439633250236511 Loss_D_B: 0.07677507400512695 Loss_G: 0.4305296540260315\n",
      "Epoch [8/10] Batch [2300/20655] Loss_D_A: 0.5195783972740173 Loss_D_B: 0.8102768063545227 Loss_G: 0.4934282898902893\n",
      "Epoch [8/10] Batch [2400/20655] Loss_D_A: 0.44632911682128906 Loss_D_B: 0.5612311363220215 Loss_G: 0.8745875358581543\n",
      "Epoch [8/10] Batch [2500/20655] Loss_D_A: 0.7954525947570801 Loss_D_B: 0.4895734190940857 Loss_G: 0.26154083013534546\n",
      "Epoch [8/10] Batch [2600/20655] Loss_D_A: 0.9976248145103455 Loss_D_B: 0.7166367173194885 Loss_G: 0.10385864973068237\n",
      "Epoch [8/10] Batch [2700/20655] Loss_D_A: 0.5179190635681152 Loss_D_B: 0.7911642789840698 Loss_G: 0.6661889553070068\n",
      "Epoch [8/10] Batch [2800/20655] Loss_D_A: 0.5956816077232361 Loss_D_B: 0.06958872079849243 Loss_G: 0.6407661437988281\n",
      "Epoch [8/10] Batch [2900/20655] Loss_D_A: 0.23361808061599731 Loss_D_B: 0.3307356834411621 Loss_G: 0.8398450016975403\n",
      "Epoch [8/10] Batch [3000/20655] Loss_D_A: 0.8167214393615723 Loss_D_B: 0.4322664737701416 Loss_G: 0.552661120891571\n",
      "Epoch [8/10] Batch [3100/20655] Loss_D_A: 0.07442688941955566 Loss_D_B: 0.9140002727508545 Loss_G: 0.2570357918739319\n",
      "Epoch [8/10] Batch [3200/20655] Loss_D_A: 0.207325279712677 Loss_D_B: 0.49819815158843994 Loss_G: 0.9060643315315247\n",
      "Epoch [8/10] Batch [3300/20655] Loss_D_A: 0.7358583807945251 Loss_D_B: 0.12675148248672485 Loss_G: 0.4721229672431946\n",
      "Epoch [8/10] Batch [3400/20655] Loss_D_A: 0.34654945135116577 Loss_D_B: 0.7828301787376404 Loss_G: 0.3002500534057617\n",
      "Epoch [8/10] Batch [3500/20655] Loss_D_A: 0.9801217913627625 Loss_D_B: 0.4016687870025635 Loss_G: 0.7492139339447021\n",
      "Epoch [8/10] Batch [3600/20655] Loss_D_A: 0.47662168741226196 Loss_D_B: 0.74578857421875 Loss_G: 0.13527363538742065\n",
      "Epoch [8/10] Batch [3700/20655] Loss_D_A: 0.3165099620819092 Loss_D_B: 0.19719016551971436 Loss_G: 0.7009040713310242\n",
      "Epoch [8/10] Batch [3800/20655] Loss_D_A: 0.5008898377418518 Loss_D_B: 0.7133821845054626 Loss_G: 0.2435324788093567\n",
      "Epoch [8/10] Batch [3900/20655] Loss_D_A: 0.601361870765686 Loss_D_B: 0.4460577964782715 Loss_G: 0.0014445185661315918\n",
      "Epoch [8/10] Batch [4000/20655] Loss_D_A: 0.6809368133544922 Loss_D_B: 0.8468431830406189 Loss_G: 0.6719474792480469\n",
      "Epoch [8/10] Batch [4100/20655] Loss_D_A: 0.21970993280410767 Loss_D_B: 0.17093753814697266 Loss_G: 0.9699580073356628\n",
      "Epoch [8/10] Batch [4200/20655] Loss_D_A: 0.378106951713562 Loss_D_B: 0.25823795795440674 Loss_G: 0.8741850256919861\n",
      "Epoch [8/10] Batch [4300/20655] Loss_D_A: 0.7079957127571106 Loss_D_B: 0.1719873547554016 Loss_G: 0.21015751361846924\n",
      "Epoch [8/10] Batch [4400/20655] Loss_D_A: 0.1454525589942932 Loss_D_B: 0.6903396844863892 Loss_G: 0.8932837247848511\n",
      "Epoch [8/10] Batch [4500/20655] Loss_D_A: 0.41906654834747314 Loss_D_B: 0.16731977462768555 Loss_G: 0.42535215616226196\n",
      "Epoch [8/10] Batch [4600/20655] Loss_D_A: 0.4643266797065735 Loss_D_B: 0.4147558808326721 Loss_G: 0.15050721168518066\n",
      "Epoch [8/10] Batch [4700/20655] Loss_D_A: 0.20665335655212402 Loss_D_B: 0.08903145790100098 Loss_G: 0.21937745809555054\n",
      "Epoch [8/10] Batch [4800/20655] Loss_D_A: 0.3066561818122864 Loss_D_B: 0.8148024678230286 Loss_G: 0.08552706241607666\n",
      "Epoch [8/10] Batch [4900/20655] Loss_D_A: 0.5824665427207947 Loss_D_B: 0.02194422483444214 Loss_G: 0.3808826804161072\n",
      "Epoch [8/10] Batch [5000/20655] Loss_D_A: 0.119687020778656 Loss_D_B: 0.2301654815673828 Loss_G: 0.16493117809295654\n",
      "Epoch [8/10] Batch [5100/20655] Loss_D_A: 0.07392513751983643 Loss_D_B: 0.46293288469314575 Loss_G: 0.7434890270233154\n",
      "Epoch [8/10] Batch [5200/20655] Loss_D_A: 0.5944859385490417 Loss_D_B: 0.8447876572608948 Loss_G: 0.47003066539764404\n",
      "Epoch [8/10] Batch [5300/20655] Loss_D_A: 0.6657166481018066 Loss_D_B: 0.1412385106086731 Loss_G: 0.637433648109436\n",
      "Epoch [8/10] Batch [5400/20655] Loss_D_A: 0.6767653226852417 Loss_D_B: 0.15066468715667725 Loss_G: 0.6318932771682739\n",
      "Epoch [8/10] Batch [5500/20655] Loss_D_A: 0.19787442684173584 Loss_D_B: 0.539048969745636 Loss_G: 0.6186731457710266\n",
      "Epoch [8/10] Batch [5600/20655] Loss_D_A: 0.7773058414459229 Loss_D_B: 0.7606970071792603 Loss_G: 0.7077237367630005\n",
      "Epoch [8/10] Batch [5700/20655] Loss_D_A: 0.8808180093765259 Loss_D_B: 0.23457598686218262 Loss_G: 0.4437212347984314\n",
      "Epoch [8/10] Batch [5800/20655] Loss_D_A: 0.5013770461082458 Loss_D_B: 0.024670720100402832 Loss_G: 0.9922202825546265\n",
      "Epoch [8/10] Batch [5900/20655] Loss_D_A: 0.6063717007637024 Loss_D_B: 0.102486252784729 Loss_G: 0.906447172164917\n",
      "Epoch [8/10] Batch [6000/20655] Loss_D_A: 0.5602604746818542 Loss_D_B: 0.755409300327301 Loss_G: 0.33420127630233765\n",
      "Epoch [8/10] Batch [6100/20655] Loss_D_A: 0.31070923805236816 Loss_D_B: 0.7032850384712219 Loss_G: 0.16755449771881104\n",
      "Epoch [8/10] Batch [6200/20655] Loss_D_A: 0.16464930772781372 Loss_D_B: 0.21530401706695557 Loss_G: 0.3483288288116455\n",
      "Epoch [8/10] Batch [6300/20655] Loss_D_A: 0.11628842353820801 Loss_D_B: 0.7396594285964966 Loss_G: 0.3738046884536743\n",
      "Epoch [8/10] Batch [6400/20655] Loss_D_A: 0.6138114333152771 Loss_D_B: 0.7005947828292847 Loss_G: 0.5916697978973389\n",
      "Epoch [8/10] Batch [6500/20655] Loss_D_A: 0.28909194469451904 Loss_D_B: 0.8985791206359863 Loss_G: 0.7661576867103577\n",
      "Epoch [8/10] Batch [6600/20655] Loss_D_A: 0.6044673919677734 Loss_D_B: 0.8005496263504028 Loss_G: 0.5480884909629822\n",
      "Epoch [8/10] Batch [6700/20655] Loss_D_A: 0.22954130172729492 Loss_D_B: 0.17071282863616943 Loss_G: 0.16967391967773438\n",
      "Epoch [8/10] Batch [6800/20655] Loss_D_A: 0.981435239315033 Loss_D_B: 0.06782853603363037 Loss_G: 0.595781147480011\n",
      "Epoch [8/10] Batch [6900/20655] Loss_D_A: 0.1982729434967041 Loss_D_B: 0.6502582430839539 Loss_G: 0.6367962956428528\n",
      "Epoch [8/10] Batch [7000/20655] Loss_D_A: 0.7700814604759216 Loss_D_B: 0.30494070053100586 Loss_G: 0.563164234161377\n",
      "Epoch [8/10] Batch [7100/20655] Loss_D_A: 0.21637839078903198 Loss_D_B: 0.6968203783035278 Loss_G: 0.7438865303993225\n",
      "Epoch [8/10] Batch [7200/20655] Loss_D_A: 0.1521129012107849 Loss_D_B: 0.7267583608627319 Loss_G: 0.7940576076507568\n",
      "Epoch [8/10] Batch [7300/20655] Loss_D_A: 0.30856454372406006 Loss_D_B: 0.8258476853370667 Loss_G: 0.5739941000938416\n",
      "Epoch [8/10] Batch [7400/20655] Loss_D_A: 0.9220483899116516 Loss_D_B: 0.26596033573150635 Loss_G: 0.7635359168052673\n",
      "Epoch [8/10] Batch [7500/20655] Loss_D_A: 0.5533455610275269 Loss_D_B: 0.6713170409202576 Loss_G: 0.5230531096458435\n",
      "Epoch [8/10] Batch [7600/20655] Loss_D_A: 0.32791489362716675 Loss_D_B: 0.13849681615829468 Loss_G: 0.3970012068748474\n",
      "Epoch [8/10] Batch [7700/20655] Loss_D_A: 0.24019509553909302 Loss_D_B: 0.6075213551521301 Loss_G: 0.786081850528717\n",
      "Epoch [8/10] Batch [7800/20655] Loss_D_A: 0.7484124898910522 Loss_D_B: 0.49106884002685547 Loss_G: 0.9364782571792603\n",
      "Epoch [8/10] Batch [7900/20655] Loss_D_A: 0.747783899307251 Loss_D_B: 0.33134812116622925 Loss_G: 0.6114808917045593\n",
      "Epoch [8/10] Batch [8000/20655] Loss_D_A: 0.7368541359901428 Loss_D_B: 0.6666281223297119 Loss_G: 0.22178161144256592\n",
      "Epoch [8/10] Batch [8100/20655] Loss_D_A: 0.10295778512954712 Loss_D_B: 0.18045306205749512 Loss_G: 0.9085677862167358\n",
      "Epoch [8/10] Batch [8200/20655] Loss_D_A: 0.44027090072631836 Loss_D_B: 0.9640777707099915 Loss_G: 0.1591254472732544\n",
      "Epoch [8/10] Batch [8300/20655] Loss_D_A: 0.18333971500396729 Loss_D_B: 0.6639078259468079 Loss_G: 0.9225943088531494\n",
      "Epoch [8/10] Batch [8400/20655] Loss_D_A: 0.843285322189331 Loss_D_B: 0.518079936504364 Loss_G: 0.31771254539489746\n",
      "Epoch [8/10] Batch [8500/20655] Loss_D_A: 0.7284049391746521 Loss_D_B: 0.5564768314361572 Loss_G: 0.9546045660972595\n",
      "Epoch [8/10] Batch [8600/20655] Loss_D_A: 0.9023260474205017 Loss_D_B: 0.12327104806900024 Loss_G: 0.9670539498329163\n",
      "Epoch [8/10] Batch [8700/20655] Loss_D_A: 0.34448176622390747 Loss_D_B: 0.39017391204833984 Loss_G: 0.43614083528518677\n",
      "Epoch [8/10] Batch [8800/20655] Loss_D_A: 0.3512697219848633 Loss_D_B: 0.9180631637573242 Loss_G: 0.6094914078712463\n",
      "Epoch [8/10] Batch [8900/20655] Loss_D_A: 0.06900131702423096 Loss_D_B: 0.7768215537071228 Loss_G: 0.5057641267776489\n",
      "Epoch [8/10] Batch [9000/20655] Loss_D_A: 0.8429293632507324 Loss_D_B: 0.3632141351699829 Loss_G: 0.960993230342865\n",
      "Epoch [8/10] Batch [9100/20655] Loss_D_A: 0.30095016956329346 Loss_D_B: 0.7944177389144897 Loss_G: 0.5070006847381592\n",
      "Epoch [8/10] Batch [9200/20655] Loss_D_A: 0.7722254395484924 Loss_D_B: 0.34948939085006714 Loss_G: 0.21542513370513916\n",
      "Epoch [8/10] Batch [9300/20655] Loss_D_A: 0.1422380805015564 Loss_D_B: 0.13235729932785034 Loss_G: 0.6703113317489624\n",
      "Epoch [8/10] Batch [9400/20655] Loss_D_A: 0.32032591104507446 Loss_D_B: 0.9078412055969238 Loss_G: 0.5100361704826355\n",
      "Epoch [8/10] Batch [9500/20655] Loss_D_A: 0.8612082600593567 Loss_D_B: 0.4420527219772339 Loss_G: 0.12375462055206299\n",
      "Epoch [8/10] Batch [9600/20655] Loss_D_A: 0.5160052180290222 Loss_D_B: 0.7863169312477112 Loss_G: 0.14423364400863647\n",
      "Epoch [8/10] Batch [9700/20655] Loss_D_A: 0.21789616346359253 Loss_D_B: 0.46630245447158813 Loss_G: 0.2006794810295105\n",
      "Epoch [8/10] Batch [9800/20655] Loss_D_A: 0.21272939443588257 Loss_D_B: 0.4125670790672302 Loss_G: 0.44317781925201416\n",
      "Epoch [8/10] Batch [9900/20655] Loss_D_A: 0.7496960759162903 Loss_D_B: 0.2208089828491211 Loss_G: 0.7495913505554199\n",
      "Epoch [8/10] Batch [10000/20655] Loss_D_A: 0.9339444637298584 Loss_D_B: 0.3057640790939331 Loss_G: 0.7989843487739563\n",
      "Epoch [8/10] Batch [10100/20655] Loss_D_A: 0.6181131601333618 Loss_D_B: 0.6882190704345703 Loss_G: 0.5320566296577454\n",
      "Epoch [8/10] Batch [10200/20655] Loss_D_A: 0.016956806182861328 Loss_D_B: 0.27045297622680664 Loss_G: 0.05691242218017578\n",
      "Epoch [8/10] Batch [10300/20655] Loss_D_A: 0.5099332332611084 Loss_D_B: 0.18125730752944946 Loss_G: 0.11780834197998047\n",
      "Epoch [8/10] Batch [10400/20655] Loss_D_A: 0.5469278693199158 Loss_D_B: 0.283080518245697 Loss_G: 0.8320814967155457\n",
      "Epoch [8/10] Batch [10500/20655] Loss_D_A: 0.024631619453430176 Loss_D_B: 0.7189257144927979 Loss_G: 0.6731253862380981\n",
      "Epoch [8/10] Batch [10600/20655] Loss_D_A: 0.16231769323349 Loss_D_B: 0.49731552600860596 Loss_G: 0.63753342628479\n",
      "Epoch [8/10] Batch [10700/20655] Loss_D_A: 0.35759419202804565 Loss_D_B: 0.16635215282440186 Loss_G: 0.6418627500534058\n",
      "Epoch [8/10] Batch [10800/20655] Loss_D_A: 0.9117686152458191 Loss_D_B: 0.10641604661941528 Loss_G: 0.9785473346710205\n",
      "Epoch [8/10] Batch [10900/20655] Loss_D_A: 0.9695091843605042 Loss_D_B: 0.14637833833694458 Loss_G: 0.6750687956809998\n",
      "Epoch [8/10] Batch [11000/20655] Loss_D_A: 0.8332281708717346 Loss_D_B: 0.1508961319923401 Loss_G: 0.2862789034843445\n",
      "Epoch [8/10] Batch [11100/20655] Loss_D_A: 0.39977043867111206 Loss_D_B: 0.3957222104072571 Loss_G: 0.5577089190483093\n",
      "Epoch [8/10] Batch [11200/20655] Loss_D_A: 0.08573043346405029 Loss_D_B: 0.33848804235458374 Loss_G: 0.2527560591697693\n",
      "Epoch [8/10] Batch [11300/20655] Loss_D_A: 0.5595086216926575 Loss_D_B: 0.3266165852546692 Loss_G: 0.6596226692199707\n",
      "Epoch [8/10] Batch [11400/20655] Loss_D_A: 0.2560781240463257 Loss_D_B: 0.607967734336853 Loss_G: 0.7382091283798218\n",
      "Epoch [8/10] Batch [11500/20655] Loss_D_A: 0.760679304599762 Loss_D_B: 0.004868149757385254 Loss_G: 0.7551170587539673\n",
      "Epoch [8/10] Batch [11600/20655] Loss_D_A: 0.8651089072227478 Loss_D_B: 0.6216487288475037 Loss_G: 0.496493399143219\n",
      "Epoch [8/10] Batch [11700/20655] Loss_D_A: 0.2009097933769226 Loss_D_B: 0.9176818132400513 Loss_G: 0.04389470815658569\n",
      "Epoch [8/10] Batch [11800/20655] Loss_D_A: 0.36791056394577026 Loss_D_B: 0.7544522881507874 Loss_G: 0.6803065538406372\n",
      "Epoch [8/10] Batch [11900/20655] Loss_D_A: 0.05340844392776489 Loss_D_B: 0.2939707040786743 Loss_G: 0.14837193489074707\n",
      "Epoch [8/10] Batch [12000/20655] Loss_D_A: 0.9082854390144348 Loss_D_B: 0.030809223651885986 Loss_G: 0.2733411192893982\n",
      "Epoch [8/10] Batch [12100/20655] Loss_D_A: 0.5040799379348755 Loss_D_B: 0.2893867492675781 Loss_G: 0.3350955843925476\n",
      "Epoch [8/10] Batch [12200/20655] Loss_D_A: 0.4021138548851013 Loss_D_B: 0.18351203203201294 Loss_G: 0.18917137384414673\n",
      "Epoch [8/10] Batch [12300/20655] Loss_D_A: 0.6451602578163147 Loss_D_B: 0.31775814294815063 Loss_G: 0.21734237670898438\n",
      "Epoch [8/10] Batch [12400/20655] Loss_D_A: 0.859129011631012 Loss_D_B: 0.45566290616989136 Loss_G: 0.14095747470855713\n",
      "Epoch [8/10] Batch [12500/20655] Loss_D_A: 0.9878647923469543 Loss_D_B: 0.7822444438934326 Loss_G: 0.22558492422103882\n",
      "Epoch [8/10] Batch [12600/20655] Loss_D_A: 0.9056397676467896 Loss_D_B: 0.21719300746917725 Loss_G: 0.2548450231552124\n",
      "Epoch [8/10] Batch [12700/20655] Loss_D_A: 0.2389899492263794 Loss_D_B: 0.3986305594444275 Loss_G: 0.99642014503479\n",
      "Epoch [8/10] Batch [12800/20655] Loss_D_A: 0.07327932119369507 Loss_D_B: 0.47486329078674316 Loss_G: 0.7652082443237305\n",
      "Epoch [8/10] Batch [12900/20655] Loss_D_A: 0.785864531993866 Loss_D_B: 0.6746768355369568 Loss_G: 0.6711013913154602\n",
      "Epoch [8/10] Batch [13000/20655] Loss_D_A: 0.43917346000671387 Loss_D_B: 0.25100356340408325 Loss_G: 0.7750284075737\n",
      "Epoch [8/10] Batch [13100/20655] Loss_D_A: 0.3791501522064209 Loss_D_B: 0.30966460704803467 Loss_G: 0.259727418422699\n",
      "Epoch [8/10] Batch [13200/20655] Loss_D_A: 0.6973298192024231 Loss_D_B: 0.905547022819519 Loss_G: 0.5105671286582947\n",
      "Epoch [8/10] Batch [13300/20655] Loss_D_A: 0.1640567183494568 Loss_D_B: 0.8340367078781128 Loss_G: 0.7098382115364075\n",
      "Epoch [8/10] Batch [13400/20655] Loss_D_A: 0.9439679384231567 Loss_D_B: 0.7918456792831421 Loss_G: 0.6087687611579895\n",
      "Epoch [8/10] Batch [13500/20655] Loss_D_A: 0.4294949769973755 Loss_D_B: 0.1271870732307434 Loss_G: 0.8147847652435303\n",
      "Epoch [8/10] Batch [13600/20655] Loss_D_A: 0.09745997190475464 Loss_D_B: 0.8414113521575928 Loss_G: 0.6105768084526062\n",
      "Epoch [8/10] Batch [13700/20655] Loss_D_A: 0.6135403513908386 Loss_D_B: 0.42728447914123535 Loss_G: 0.5963162183761597\n",
      "Epoch [8/10] Batch [13800/20655] Loss_D_A: 0.2785381078720093 Loss_D_B: 0.6967663168907166 Loss_G: 0.35024070739746094\n",
      "Epoch [8/10] Batch [13900/20655] Loss_D_A: 0.20194190740585327 Loss_D_B: 0.7480001449584961 Loss_G: 0.7651973962783813\n",
      "Epoch [8/10] Batch [14000/20655] Loss_D_A: 0.9109973907470703 Loss_D_B: 0.6414969563484192 Loss_G: 0.6789715886116028\n",
      "Epoch [8/10] Batch [14100/20655] Loss_D_A: 0.15959644317626953 Loss_D_B: 0.46514374017715454 Loss_G: 0.37732362747192383\n",
      "Epoch [8/10] Batch [14200/20655] Loss_D_A: 0.7690116763114929 Loss_D_B: 0.38077276945114136 Loss_G: 0.21629005670547485\n",
      "Epoch [8/10] Batch [14300/20655] Loss_D_A: 0.3607252240180969 Loss_D_B: 0.10381531715393066 Loss_G: 0.9145347476005554\n",
      "Epoch [8/10] Batch [14400/20655] Loss_D_A: 0.376253604888916 Loss_D_B: 0.5328556895256042 Loss_G: 0.7465313076972961\n",
      "Epoch [8/10] Batch [14500/20655] Loss_D_A: 0.7653845548629761 Loss_D_B: 0.9594868421554565 Loss_G: 0.1590254306793213\n",
      "Epoch [8/10] Batch [14600/20655] Loss_D_A: 0.314347505569458 Loss_D_B: 0.21405529975891113 Loss_G: 0.12454915046691895\n",
      "Epoch [8/10] Batch [14700/20655] Loss_D_A: 0.45833849906921387 Loss_D_B: 0.774298369884491 Loss_G: 0.6847472786903381\n",
      "Epoch [8/10] Batch [14800/20655] Loss_D_A: 0.3900279402732849 Loss_D_B: 0.39109688997268677 Loss_G: 0.7733352780342102\n",
      "Epoch [8/10] Batch [14900/20655] Loss_D_A: 0.7310781478881836 Loss_D_B: 0.9617754220962524 Loss_G: 0.15087950229644775\n",
      "Epoch [8/10] Batch [15000/20655] Loss_D_A: 0.4925345778465271 Loss_D_B: 0.20763492584228516 Loss_G: 0.39040374755859375\n",
      "Epoch [8/10] Batch [15100/20655] Loss_D_A: 0.3600614070892334 Loss_D_B: 0.07055366039276123 Loss_G: 0.6719779372215271\n",
      "Epoch [8/10] Batch [15200/20655] Loss_D_A: 0.9802284240722656 Loss_D_B: 0.2576912045478821 Loss_G: 0.5500440001487732\n",
      "Epoch [8/10] Batch [15300/20655] Loss_D_A: 0.1140298843383789 Loss_D_B: 0.9878480434417725 Loss_G: 0.21182608604431152\n",
      "Epoch [8/10] Batch [15400/20655] Loss_D_A: 0.6621471047401428 Loss_D_B: 0.1947200894355774 Loss_G: 0.9341927170753479\n",
      "Epoch [8/10] Batch [15500/20655] Loss_D_A: 0.992825984954834 Loss_D_B: 0.2247444987297058 Loss_G: 0.3604949116706848\n",
      "Epoch [8/10] Batch [15600/20655] Loss_D_A: 0.4355212450027466 Loss_D_B: 0.7115300297737122 Loss_G: 0.6418882012367249\n",
      "Epoch [8/10] Batch [15700/20655] Loss_D_A: 0.7010945081710815 Loss_D_B: 0.6719900369644165 Loss_G: 0.055052876472473145\n",
      "Epoch [8/10] Batch [15800/20655] Loss_D_A: 0.5303234457969666 Loss_D_B: 0.33432984352111816 Loss_G: 0.6434596180915833\n",
      "Epoch [8/10] Batch [15900/20655] Loss_D_A: 0.30187904834747314 Loss_D_B: 0.5059210658073425 Loss_G: 0.33672577142715454\n",
      "Epoch [8/10] Batch [16000/20655] Loss_D_A: 0.7196994423866272 Loss_D_B: 0.33778536319732666 Loss_G: 0.7731455564498901\n",
      "Epoch [8/10] Batch [16100/20655] Loss_D_A: 0.1493505835533142 Loss_D_B: 0.5942703485488892 Loss_G: 0.22105848789215088\n",
      "Epoch [8/10] Batch [16200/20655] Loss_D_A: 0.013895690441131592 Loss_D_B: 0.7961406111717224 Loss_G: 0.8608459234237671\n",
      "Epoch [8/10] Batch [16300/20655] Loss_D_A: 0.7775289416313171 Loss_D_B: 0.28351354598999023 Loss_G: 0.1362835168838501\n",
      "Epoch [8/10] Batch [16400/20655] Loss_D_A: 0.710386335849762 Loss_D_B: 0.7820969223976135 Loss_G: 0.9974547028541565\n",
      "Epoch [8/10] Batch [16500/20655] Loss_D_A: 0.7480747103691101 Loss_D_B: 0.5033824443817139 Loss_G: 0.3512691259384155\n",
      "Epoch [8/10] Batch [16600/20655] Loss_D_A: 0.22461998462677002 Loss_D_B: 0.6504327058792114 Loss_G: 0.27246159315109253\n",
      "Epoch [8/10] Batch [16700/20655] Loss_D_A: 0.3158813714981079 Loss_D_B: 0.80876225233078 Loss_G: 0.26930558681488037\n",
      "Epoch [8/10] Batch [16800/20655] Loss_D_A: 0.875073254108429 Loss_D_B: 0.15754783153533936 Loss_G: 0.8713903427124023\n",
      "Epoch [8/10] Batch [16900/20655] Loss_D_A: 0.169253408908844 Loss_D_B: 0.97272127866745 Loss_G: 0.4182371497154236\n",
      "Epoch [8/10] Batch [17000/20655] Loss_D_A: 0.3854280114173889 Loss_D_B: 0.5838630199432373 Loss_G: 0.9010916352272034\n",
      "Epoch [8/10] Batch [17100/20655] Loss_D_A: 0.8072268962860107 Loss_D_B: 0.3593958616256714 Loss_G: 0.602977991104126\n",
      "Epoch [8/10] Batch [17200/20655] Loss_D_A: 0.9938267469406128 Loss_D_B: 0.8947469592094421 Loss_G: 0.11903685331344604\n",
      "Epoch [8/10] Batch [17300/20655] Loss_D_A: 0.13164454698562622 Loss_D_B: 0.7803138494491577 Loss_G: 0.0917620062828064\n",
      "Epoch [8/10] Batch [17400/20655] Loss_D_A: 0.9807008504867554 Loss_D_B: 0.9761800169944763 Loss_G: 0.3904145359992981\n",
      "Epoch [8/10] Batch [17500/20655] Loss_D_A: 0.3817061185836792 Loss_D_B: 0.46529918909072876 Loss_G: 0.5162302851676941\n",
      "Epoch [8/10] Batch [17600/20655] Loss_D_A: 0.11723089218139648 Loss_D_B: 0.7481597065925598 Loss_G: 0.4906381368637085\n",
      "Epoch [8/10] Batch [17700/20655] Loss_D_A: 0.8047313690185547 Loss_D_B: 0.13868719339370728 Loss_G: 0.7201660871505737\n",
      "Epoch [8/10] Batch [17800/20655] Loss_D_A: 0.04756951332092285 Loss_D_B: 0.8151375651359558 Loss_G: 0.07217812538146973\n",
      "Epoch [8/10] Batch [17900/20655] Loss_D_A: 0.00409245491027832 Loss_D_B: 0.07046878337860107 Loss_G: 0.4914558529853821\n",
      "Epoch [8/10] Batch [18000/20655] Loss_D_A: 0.9221579432487488 Loss_D_B: 0.7945937514305115 Loss_G: 0.4313046336174011\n",
      "Epoch [8/10] Batch [18100/20655] Loss_D_A: 0.04012829065322876 Loss_D_B: 0.6834145188331604 Loss_G: 0.2619848847389221\n",
      "Epoch [8/10] Batch [18200/20655] Loss_D_A: 0.5013153553009033 Loss_D_B: 0.5382676124572754 Loss_G: 0.0775829553604126\n",
      "Epoch [8/10] Batch [18300/20655] Loss_D_A: 0.4969344139099121 Loss_D_B: 0.14867252111434937 Loss_G: 0.28859180212020874\n",
      "Epoch [8/10] Batch [18400/20655] Loss_D_A: 0.154890775680542 Loss_D_B: 0.5475789904594421 Loss_G: 0.3859129548072815\n",
      "Epoch [8/10] Batch [18500/20655] Loss_D_A: 0.30243927240371704 Loss_D_B: 0.343055784702301 Loss_G: 0.7152096629142761\n",
      "Epoch [8/10] Batch [18600/20655] Loss_D_A: 0.7187785506248474 Loss_D_B: 0.6841191053390503 Loss_G: 0.7066305875778198\n",
      "Epoch [8/10] Batch [18700/20655] Loss_D_A: 0.6325113773345947 Loss_D_B: 0.41848134994506836 Loss_G: 0.9757052659988403\n",
      "Epoch [8/10] Batch [18800/20655] Loss_D_A: 0.7597755193710327 Loss_D_B: 0.08828330039978027 Loss_G: 0.5411844253540039\n",
      "Epoch [8/10] Batch [18900/20655] Loss_D_A: 0.9285240173339844 Loss_D_B: 0.3198004961013794 Loss_G: 0.9021530747413635\n",
      "Epoch [8/10] Batch [19000/20655] Loss_D_A: 0.8348252177238464 Loss_D_B: 0.6203593611717224 Loss_G: 0.5780786871910095\n",
      "Epoch [8/10] Batch [19100/20655] Loss_D_A: 0.19700491428375244 Loss_D_B: 0.4303620457649231 Loss_G: 0.8729744553565979\n",
      "Epoch [8/10] Batch [19200/20655] Loss_D_A: 0.42091459035873413 Loss_D_B: 0.39256417751312256 Loss_G: 0.4410364627838135\n",
      "Epoch [8/10] Batch [19300/20655] Loss_D_A: 0.7453615665435791 Loss_D_B: 0.5752193927764893 Loss_G: 0.48872894048690796\n",
      "Epoch [8/10] Batch [19400/20655] Loss_D_A: 0.37021559476852417 Loss_D_B: 0.09014064073562622 Loss_G: 0.5328736305236816\n",
      "Epoch [8/10] Batch [19500/20655] Loss_D_A: 0.2247365117073059 Loss_D_B: 0.23452341556549072 Loss_G: 0.7345758080482483\n",
      "Epoch [8/10] Batch [19600/20655] Loss_D_A: 0.8316825032234192 Loss_D_B: 0.6259308457374573 Loss_G: 0.9855756163597107\n",
      "Epoch [8/10] Batch [19700/20655] Loss_D_A: 0.426285982131958 Loss_D_B: 0.6869933009147644 Loss_G: 0.8336578607559204\n",
      "Epoch [8/10] Batch [19800/20655] Loss_D_A: 0.042552947998046875 Loss_D_B: 0.5349903702735901 Loss_G: 0.045604586601257324\n",
      "Epoch [8/10] Batch [19900/20655] Loss_D_A: 0.47888821363449097 Loss_D_B: 0.47859543561935425 Loss_G: 0.8912179470062256\n",
      "Epoch [8/10] Batch [20000/20655] Loss_D_A: 0.061218202114105225 Loss_D_B: 0.6825069189071655 Loss_G: 0.7254644632339478\n",
      "Epoch [8/10] Batch [20100/20655] Loss_D_A: 0.48360371589660645 Loss_D_B: 0.9251288175582886 Loss_G: 0.10824161767959595\n",
      "Epoch [8/10] Batch [20200/20655] Loss_D_A: 0.36166560649871826 Loss_D_B: 0.6862500309944153 Loss_G: 0.12688606977462769\n",
      "Epoch [8/10] Batch [20300/20655] Loss_D_A: 0.5098469257354736 Loss_D_B: 0.2892792820930481 Loss_G: 0.8015020489692688\n",
      "Epoch [8/10] Batch [20400/20655] Loss_D_A: 0.70380699634552 Loss_D_B: 0.20701658725738525 Loss_G: 0.10101497173309326\n",
      "Epoch [8/10] Batch [20500/20655] Loss_D_A: 0.8836441040039062 Loss_D_B: 0.5916426181793213 Loss_G: 0.0689399242401123\n",
      "Epoch [8/10] Batch [20600/20655] Loss_D_A: 0.4083760976791382 Loss_D_B: 0.12431842088699341 Loss_G: 0.41765153408050537\n",
      "Epoch [9/10] Batch [0/20655] Loss_D_A: 0.13780421018600464 Loss_D_B: 0.9845083951950073 Loss_G: 0.6054011583328247\n",
      "Epoch [9/10] Batch [0/20655] Loss_D_A: 0.7185193300247192 Loss_D_B: 5.2928924560546875e-05 Loss_G: 0.6681438684463501\n",
      "Epoch [9/10] Batch [100/20655] Loss_D_A: 0.957283616065979 Loss_D_B: 0.04005885124206543 Loss_G: 0.8467397093772888\n",
      "Epoch [9/10] Batch [200/20655] Loss_D_A: 0.5150416493415833 Loss_D_B: 0.21492493152618408 Loss_G: 0.9739770293235779\n",
      "Epoch [9/10] Batch [300/20655] Loss_D_A: 0.7805928587913513 Loss_D_B: 0.613284707069397 Loss_G: 0.8664222359657288\n",
      "Epoch [9/10] Batch [400/20655] Loss_D_A: 0.1044880747795105 Loss_D_B: 0.3380546569824219 Loss_G: 0.16503065824508667\n",
      "Epoch [9/10] Batch [500/20655] Loss_D_A: 0.5073515772819519 Loss_D_B: 0.8815646767616272 Loss_G: 0.29363417625427246\n",
      "Epoch [9/10] Batch [600/20655] Loss_D_A: 0.7476606965065002 Loss_D_B: 0.6742579340934753 Loss_G: 0.11630469560623169\n",
      "Epoch [9/10] Batch [700/20655] Loss_D_A: 0.19773584604263306 Loss_D_B: 0.1682942509651184 Loss_G: 0.455973744392395\n",
      "Epoch [9/10] Batch [800/20655] Loss_D_A: 0.9829715490341187 Loss_D_B: 0.48415911197662354 Loss_G: 0.577396810054779\n",
      "Epoch [9/10] Batch [900/20655] Loss_D_A: 0.8230155110359192 Loss_D_B: 0.6814515590667725 Loss_G: 0.3149522542953491\n",
      "Epoch [9/10] Batch [1000/20655] Loss_D_A: 0.42582303285598755 Loss_D_B: 0.4400821328163147 Loss_G: 0.21183264255523682\n",
      "Epoch [9/10] Batch [1100/20655] Loss_D_A: 0.650325357913971 Loss_D_B: 0.3627609610557556 Loss_G: 0.4968886971473694\n",
      "Epoch [9/10] Batch [1200/20655] Loss_D_A: 0.8442116379737854 Loss_D_B: 0.24505126476287842 Loss_G: 0.7993290424346924\n",
      "Epoch [9/10] Batch [1300/20655] Loss_D_A: 0.2930586338043213 Loss_D_B: 0.15920966863632202 Loss_G: 0.47383224964141846\n",
      "Epoch [9/10] Batch [1400/20655] Loss_D_A: 0.08194363117218018 Loss_D_B: 0.6300036907196045 Loss_G: 0.27165520191192627\n",
      "Epoch [9/10] Batch [1500/20655] Loss_D_A: 0.28970134258270264 Loss_D_B: 0.2811540365219116 Loss_G: 0.2243645191192627\n",
      "Epoch [9/10] Batch [1600/20655] Loss_D_A: 0.6242230534553528 Loss_D_B: 0.6171858906745911 Loss_G: 0.5066190958023071\n",
      "Epoch [9/10] Batch [1700/20655] Loss_D_A: 0.29675400257110596 Loss_D_B: 0.41453099250793457 Loss_G: 0.9703316688537598\n",
      "Epoch [9/10] Batch [1800/20655] Loss_D_A: 0.2634090781211853 Loss_D_B: 0.5451232194900513 Loss_G: 0.8860113620758057\n",
      "Epoch [9/10] Batch [1900/20655] Loss_D_A: 0.22333437204360962 Loss_D_B: 0.7121900916099548 Loss_G: 0.8250400424003601\n",
      "Epoch [9/10] Batch [2000/20655] Loss_D_A: 0.3836057782173157 Loss_D_B: 0.03130894899368286 Loss_G: 0.8706431984901428\n",
      "Epoch [9/10] Batch [2100/20655] Loss_D_A: 0.17674463987350464 Loss_D_B: 0.9432719349861145 Loss_G: 0.2288956642150879\n",
      "Epoch [9/10] Batch [2200/20655] Loss_D_A: 0.3050636053085327 Loss_D_B: 0.08768141269683838 Loss_G: 0.7190195918083191\n",
      "Epoch [9/10] Batch [2300/20655] Loss_D_A: 0.16036802530288696 Loss_D_B: 0.6109126210212708 Loss_G: 0.954393208026886\n",
      "Epoch [9/10] Batch [2400/20655] Loss_D_A: 0.9679669737815857 Loss_D_B: 0.8864091038703918 Loss_G: 0.7862738370895386\n",
      "Epoch [9/10] Batch [2500/20655] Loss_D_A: 0.7837289571762085 Loss_D_B: 0.22292321920394897 Loss_G: 0.9502975940704346\n",
      "Epoch [9/10] Batch [2600/20655] Loss_D_A: 0.2554372549057007 Loss_D_B: 0.2654383182525635 Loss_G: 0.5186600089073181\n",
      "Epoch [9/10] Batch [2700/20655] Loss_D_A: 0.16936838626861572 Loss_D_B: 0.365580677986145 Loss_G: 0.014886021614074707\n",
      "Epoch [9/10] Batch [2800/20655] Loss_D_A: 0.25310492515563965 Loss_D_B: 0.9807685017585754 Loss_G: 0.1571456789970398\n",
      "Epoch [9/10] Batch [2900/20655] Loss_D_A: 0.02176612615585327 Loss_D_B: 0.6776291131973267 Loss_G: 0.5785021781921387\n",
      "Epoch [9/10] Batch [3000/20655] Loss_D_A: 0.4503404498100281 Loss_D_B: 0.5206909775733948 Loss_G: 0.6312440037727356\n",
      "Epoch [9/10] Batch [3100/20655] Loss_D_A: 0.6664344072341919 Loss_D_B: 0.5435672402381897 Loss_G: 0.18775367736816406\n",
      "Epoch [9/10] Batch [3200/20655] Loss_D_A: 0.8203383684158325 Loss_D_B: 0.9018040895462036 Loss_G: 0.7393292784690857\n",
      "Epoch [9/10] Batch [3300/20655] Loss_D_A: 0.6983129382133484 Loss_D_B: 0.08434557914733887 Loss_G: 0.008262813091278076\n",
      "Epoch [9/10] Batch [3400/20655] Loss_D_A: 0.767656683921814 Loss_D_B: 0.5615198016166687 Loss_G: 0.1900954246520996\n",
      "Epoch [9/10] Batch [3500/20655] Loss_D_A: 0.12106871604919434 Loss_D_B: 0.13442379236221313 Loss_G: 0.981759250164032\n",
      "Epoch [9/10] Batch [3600/20655] Loss_D_A: 0.02620905637741089 Loss_D_B: 0.31452441215515137 Loss_G: 0.2601567506790161\n",
      "Epoch [9/10] Batch [3700/20655] Loss_D_A: 0.678573727607727 Loss_D_B: 0.29005682468414307 Loss_G: 0.5778934359550476\n",
      "Epoch [9/10] Batch [3800/20655] Loss_D_A: 0.9943140149116516 Loss_D_B: 0.14460569620132446 Loss_G: 0.3570399880409241\n",
      "Epoch [9/10] Batch [3900/20655] Loss_D_A: 0.5868865251541138 Loss_D_B: 0.535259485244751 Loss_G: 0.5394958853721619\n",
      "Epoch [9/10] Batch [4000/20655] Loss_D_A: 0.39994901418685913 Loss_D_B: 0.5586389899253845 Loss_G: 0.3995015025138855\n",
      "Epoch [9/10] Batch [4100/20655] Loss_D_A: 0.017401278018951416 Loss_D_B: 0.8933718800544739 Loss_G: 0.7410933971405029\n",
      "Epoch [9/10] Batch [4200/20655] Loss_D_A: 0.04996603727340698 Loss_D_B: 0.41749101877212524 Loss_G: 0.47867435216903687\n",
      "Epoch [9/10] Batch [4300/20655] Loss_D_A: 0.6330357193946838 Loss_D_B: 0.5709950923919678 Loss_G: 0.2613990306854248\n",
      "Epoch [9/10] Batch [4400/20655] Loss_D_A: 0.1524113416671753 Loss_D_B: 0.5767635107040405 Loss_G: 0.2966705560684204\n",
      "Epoch [9/10] Batch [4500/20655] Loss_D_A: 0.5518786907196045 Loss_D_B: 0.3527475595474243 Loss_G: 0.1392032504081726\n",
      "Epoch [9/10] Batch [4600/20655] Loss_D_A: 0.2754351496696472 Loss_D_B: 0.31500524282455444 Loss_G: 0.1914430856704712\n",
      "Epoch [9/10] Batch [4700/20655] Loss_D_A: 0.34368807077407837 Loss_D_B: 0.13267117738723755 Loss_G: 0.8439589738845825\n",
      "Epoch [9/10] Batch [4800/20655] Loss_D_A: 0.8367913365364075 Loss_D_B: 0.6499998569488525 Loss_G: 0.3024306297302246\n",
      "Epoch [9/10] Batch [4900/20655] Loss_D_A: 0.2857380509376526 Loss_D_B: 0.24103158712387085 Loss_G: 0.2063700556755066\n",
      "Epoch [9/10] Batch [5000/20655] Loss_D_A: 0.23657619953155518 Loss_D_B: 0.8424749970436096 Loss_G: 0.16483807563781738\n",
      "Epoch [9/10] Batch [5100/20655] Loss_D_A: 0.057824790477752686 Loss_D_B: 0.9231328964233398 Loss_G: 0.27548694610595703\n",
      "Epoch [9/10] Batch [5200/20655] Loss_D_A: 0.12108975648880005 Loss_D_B: 0.10473644733428955 Loss_G: 0.7028931379318237\n",
      "Epoch [9/10] Batch [5300/20655] Loss_D_A: 0.42441606521606445 Loss_D_B: 0.2815445065498352 Loss_G: 0.06337743997573853\n",
      "Epoch [9/10] Batch [5400/20655] Loss_D_A: 0.7865579128265381 Loss_D_B: 0.7986083626747131 Loss_G: 0.4582052230834961\n",
      "Epoch [9/10] Batch [5500/20655] Loss_D_A: 0.6434319615364075 Loss_D_B: 0.29967039823532104 Loss_G: 0.38514840602874756\n",
      "Epoch [9/10] Batch [5600/20655] Loss_D_A: 0.47554832696914673 Loss_D_B: 0.862168550491333 Loss_G: 0.45342642068862915\n",
      "Epoch [9/10] Batch [5700/20655] Loss_D_A: 0.27499842643737793 Loss_D_B: 0.8372431397438049 Loss_G: 0.6521466970443726\n",
      "Epoch [9/10] Batch [5800/20655] Loss_D_A: 0.1062275767326355 Loss_D_B: 0.09094470739364624 Loss_G: 0.7068736553192139\n",
      "Epoch [9/10] Batch [5900/20655] Loss_D_A: 0.4884207844734192 Loss_D_B: 0.474342405796051 Loss_G: 0.23225992918014526\n",
      "Epoch [9/10] Batch [6000/20655] Loss_D_A: 0.8673468828201294 Loss_D_B: 0.8446939587593079 Loss_G: 0.8417125344276428\n",
      "Epoch [9/10] Batch [6100/20655] Loss_D_A: 0.801270067691803 Loss_D_B: 0.850229799747467 Loss_G: 0.34786027669906616\n",
      "Epoch [9/10] Batch [6200/20655] Loss_D_A: 0.5293347835540771 Loss_D_B: 0.3919776678085327 Loss_G: 0.9308844804763794\n",
      "Epoch [9/10] Batch [6300/20655] Loss_D_A: 0.03527557849884033 Loss_D_B: 0.005005359649658203 Loss_G: 0.8787055611610413\n",
      "Epoch [9/10] Batch [6400/20655] Loss_D_A: 0.889928936958313 Loss_D_B: 0.6391448974609375 Loss_G: 0.9768072366714478\n",
      "Epoch [9/10] Batch [6500/20655] Loss_D_A: 0.6145878434181213 Loss_D_B: 0.957793116569519 Loss_G: 0.83502596616745\n",
      "Epoch [9/10] Batch [6600/20655] Loss_D_A: 0.14244264364242554 Loss_D_B: 0.974355936050415 Loss_G: 0.9415205717086792\n",
      "Epoch [9/10] Batch [6700/20655] Loss_D_A: 0.5440809726715088 Loss_D_B: 0.6248397827148438 Loss_G: 0.1356905698776245\n",
      "Epoch [9/10] Batch [6800/20655] Loss_D_A: 0.5808272361755371 Loss_D_B: 0.5509031414985657 Loss_G: 0.7436848878860474\n",
      "Epoch [9/10] Batch [6900/20655] Loss_D_A: 0.45326530933380127 Loss_D_B: 0.1965985894203186 Loss_G: 0.14232122898101807\n",
      "Epoch [9/10] Batch [7000/20655] Loss_D_A: 0.9129249453544617 Loss_D_B: 0.32894861698150635 Loss_G: 0.14903831481933594\n",
      "Epoch [9/10] Batch [7100/20655] Loss_D_A: 0.03165632486343384 Loss_D_B: 0.2609395980834961 Loss_G: 0.3612188696861267\n",
      "Epoch [9/10] Batch [7200/20655] Loss_D_A: 0.7398810982704163 Loss_D_B: 0.5223881006240845 Loss_G: 0.0016930699348449707\n",
      "Epoch [9/10] Batch [7300/20655] Loss_D_A: 0.25522005558013916 Loss_D_B: 0.4230390787124634 Loss_G: 0.537061870098114\n",
      "Epoch [9/10] Batch [7400/20655] Loss_D_A: 0.57177734375 Loss_D_B: 0.9900499582290649 Loss_G: 0.6158953309059143\n",
      "Epoch [9/10] Batch [7500/20655] Loss_D_A: 0.3088533878326416 Loss_D_B: 0.7506536245346069 Loss_G: 0.42795056104660034\n",
      "Epoch [9/10] Batch [7600/20655] Loss_D_A: 0.15033328533172607 Loss_D_B: 0.7138859629631042 Loss_G: 0.03927958011627197\n",
      "Epoch [9/10] Batch [7700/20655] Loss_D_A: 0.4248644709587097 Loss_D_B: 0.5025585293769836 Loss_G: 0.3659883737564087\n",
      "Epoch [9/10] Batch [7800/20655] Loss_D_A: 0.2076985239982605 Loss_D_B: 0.788001537322998 Loss_G: 0.5643122792243958\n",
      "Epoch [9/10] Batch [7900/20655] Loss_D_A: 0.0446547269821167 Loss_D_B: 0.7681980729103088 Loss_G: 0.5986536741256714\n",
      "Epoch [9/10] Batch [8000/20655] Loss_D_A: 0.1600777506828308 Loss_D_B: 0.9067688584327698 Loss_G: 0.6707516312599182\n",
      "Epoch [9/10] Batch [8100/20655] Loss_D_A: 0.500758945941925 Loss_D_B: 0.34717315435409546 Loss_G: 0.5145904421806335\n",
      "Epoch [9/10] Batch [8200/20655] Loss_D_A: 0.32796406745910645 Loss_D_B: 0.4790653586387634 Loss_G: 0.41851526498794556\n",
      "Epoch [9/10] Batch [8300/20655] Loss_D_A: 0.518115758895874 Loss_D_B: 0.5522739887237549 Loss_G: 0.6470070481300354\n",
      "Epoch [9/10] Batch [8400/20655] Loss_D_A: 0.4537610411643982 Loss_D_B: 0.5341042876243591 Loss_G: 0.5617157220840454\n",
      "Epoch [9/10] Batch [8500/20655] Loss_D_A: 0.5859985947608948 Loss_D_B: 0.7881490588188171 Loss_G: 0.9046368598937988\n",
      "Epoch [9/10] Batch [8600/20655] Loss_D_A: 0.3408505320549011 Loss_D_B: 0.10280114412307739 Loss_G: 0.03732705116271973\n",
      "Epoch [9/10] Batch [8700/20655] Loss_D_A: 0.29202842712402344 Loss_D_B: 0.14875447750091553 Loss_G: 0.5852407217025757\n",
      "Epoch [9/10] Batch [8800/20655] Loss_D_A: 0.9829883575439453 Loss_D_B: 0.9831004738807678 Loss_G: 0.5936871767044067\n",
      "Epoch [9/10] Batch [8900/20655] Loss_D_A: 0.005544602870941162 Loss_D_B: 0.547641396522522 Loss_G: 0.14013004302978516\n",
      "Epoch [9/10] Batch [9000/20655] Loss_D_A: 0.1245502233505249 Loss_D_B: 0.0023734569549560547 Loss_G: 0.6170093417167664\n",
      "Epoch [9/10] Batch [9100/20655] Loss_D_A: 0.3391108512878418 Loss_D_B: 0.5224043726921082 Loss_G: 0.6208863854408264\n",
      "Epoch [9/10] Batch [9200/20655] Loss_D_A: 0.8841199278831482 Loss_D_B: 0.06894296407699585 Loss_G: 0.46907204389572144\n",
      "Epoch [9/10] Batch [9300/20655] Loss_D_A: 0.32482439279556274 Loss_D_B: 0.6755756139755249 Loss_G: 0.6648561358451843\n",
      "Epoch [9/10] Batch [9400/20655] Loss_D_A: 0.7282541394233704 Loss_D_B: 0.26440703868865967 Loss_G: 0.8777029514312744\n",
      "Epoch [9/10] Batch [9500/20655] Loss_D_A: 0.5355821251869202 Loss_D_B: 0.8940415382385254 Loss_G: 0.29286712408065796\n",
      "Epoch [9/10] Batch [9600/20655] Loss_D_A: 0.34165775775909424 Loss_D_B: 0.909375786781311 Loss_G: 0.7984896898269653\n",
      "Epoch [9/10] Batch [9700/20655] Loss_D_A: 0.2797028422355652 Loss_D_B: 0.2715734839439392 Loss_G: 0.476043164730072\n",
      "Epoch [9/10] Batch [9800/20655] Loss_D_A: 0.6785569787025452 Loss_D_B: 0.3953855037689209 Loss_G: 0.298483669757843\n",
      "Epoch [9/10] Batch [9900/20655] Loss_D_A: 0.932535707950592 Loss_D_B: 0.6078640222549438 Loss_G: 0.6358175873756409\n",
      "Epoch [9/10] Batch [10000/20655] Loss_D_A: 0.7881051301956177 Loss_D_B: 0.9461864233016968 Loss_G: 0.6276617050170898\n",
      "Epoch [9/10] Batch [10100/20655] Loss_D_A: 0.4093537926673889 Loss_D_B: 0.944878101348877 Loss_G: 0.7186533212661743\n",
      "Epoch [9/10] Batch [10200/20655] Loss_D_A: 0.35288769006729126 Loss_D_B: 0.03379899263381958 Loss_G: 0.4188256859779358\n",
      "Epoch [9/10] Batch [10300/20655] Loss_D_A: 0.8689974546432495 Loss_D_B: 0.7001699209213257 Loss_G: 0.03819012641906738\n",
      "Epoch [9/10] Batch [10400/20655] Loss_D_A: 0.06115347146987915 Loss_D_B: 0.9159594774246216 Loss_G: 0.5577399134635925\n",
      "Epoch [9/10] Batch [10500/20655] Loss_D_A: 0.8841148614883423 Loss_D_B: 0.8338006734848022 Loss_G: 0.5086337924003601\n",
      "Epoch [9/10] Batch [10600/20655] Loss_D_A: 0.8324114084243774 Loss_D_B: 0.35558080673217773 Loss_G: 0.02614116668701172\n",
      "Epoch [9/10] Batch [10700/20655] Loss_D_A: 0.6933946013450623 Loss_D_B: 0.3787456750869751 Loss_G: 0.2658994793891907\n",
      "Epoch [9/10] Batch [10800/20655] Loss_D_A: 0.35044747591018677 Loss_D_B: 0.10161149501800537 Loss_G: 0.6403522491455078\n",
      "Epoch [9/10] Batch [10900/20655] Loss_D_A: 0.0009369850158691406 Loss_D_B: 0.4240451455116272 Loss_G: 0.8468670845031738\n",
      "Epoch [9/10] Batch [11000/20655] Loss_D_A: 0.7953051924705505 Loss_D_B: 0.21737051010131836 Loss_G: 0.02456289529800415\n",
      "Epoch [9/10] Batch [11100/20655] Loss_D_A: 0.0444522500038147 Loss_D_B: 0.2280992865562439 Loss_G: 0.746989905834198\n",
      "Epoch [9/10] Batch [11200/20655] Loss_D_A: 0.8730590343475342 Loss_D_B: 0.2974097728729248 Loss_G: 0.13642984628677368\n",
      "Epoch [9/10] Batch [11300/20655] Loss_D_A: 0.399416983127594 Loss_D_B: 0.9902429580688477 Loss_G: 0.8640453219413757\n",
      "Epoch [9/10] Batch [11400/20655] Loss_D_A: 0.8604989647865295 Loss_D_B: 0.9293729662895203 Loss_G: 0.5036690831184387\n",
      "Epoch [9/10] Batch [11500/20655] Loss_D_A: 0.2547184228897095 Loss_D_B: 0.9258532524108887 Loss_G: 0.16821837425231934\n",
      "Epoch [9/10] Batch [11600/20655] Loss_D_A: 0.7096124291419983 Loss_D_B: 0.28397446870803833 Loss_G: 0.033348143100738525\n",
      "Epoch [9/10] Batch [11700/20655] Loss_D_A: 0.3287670612335205 Loss_D_B: 0.23564380407333374 Loss_G: 0.6970003843307495\n",
      "Epoch [9/10] Batch [11800/20655] Loss_D_A: 0.9580691456794739 Loss_D_B: 0.6075838208198547 Loss_G: 0.6369193196296692\n",
      "Epoch [9/10] Batch [11900/20655] Loss_D_A: 0.25948190689086914 Loss_D_B: 0.45510751008987427 Loss_G: 0.5313176512718201\n",
      "Epoch [9/10] Batch [12000/20655] Loss_D_A: 0.6899147033691406 Loss_D_B: 0.13967466354370117 Loss_G: 0.6972266435623169\n",
      "Epoch [9/10] Batch [12100/20655] Loss_D_A: 0.846400260925293 Loss_D_B: 0.38921886682510376 Loss_G: 0.10715615749359131\n",
      "Epoch [9/10] Batch [12200/20655] Loss_D_A: 0.7252326607704163 Loss_D_B: 0.34339118003845215 Loss_G: 0.8234241008758545\n",
      "Epoch [9/10] Batch [12300/20655] Loss_D_A: 0.3742249608039856 Loss_D_B: 0.18408441543579102 Loss_G: 0.24842989444732666\n",
      "Epoch [9/10] Batch [12400/20655] Loss_D_A: 0.9674802422523499 Loss_D_B: 0.23942095041275024 Loss_G: 0.715782880783081\n",
      "Epoch [9/10] Batch [12500/20655] Loss_D_A: 0.8559097647666931 Loss_D_B: 0.747229278087616 Loss_G: 0.007210850715637207\n",
      "Epoch [9/10] Batch [12600/20655] Loss_D_A: 0.09983360767364502 Loss_D_B: 0.09412521123886108 Loss_G: 0.5968375205993652\n",
      "Epoch [9/10] Batch [12700/20655] Loss_D_A: 0.8417916297912598 Loss_D_B: 0.6405042409896851 Loss_G: 0.9910303354263306\n",
      "Epoch [9/10] Batch [12800/20655] Loss_D_A: 0.4733812212944031 Loss_D_B: 0.6539860367774963 Loss_G: 0.8092824816703796\n",
      "Epoch [9/10] Batch [12900/20655] Loss_D_A: 0.39776909351348877 Loss_D_B: 0.23263680934906006 Loss_G: 0.2558857798576355\n",
      "Epoch [9/10] Batch [13000/20655] Loss_D_A: 0.764893651008606 Loss_D_B: 0.26993125677108765 Loss_G: 0.462558388710022\n",
      "Epoch [9/10] Batch [13100/20655] Loss_D_A: 0.42475825548171997 Loss_D_B: 0.5516346096992493 Loss_G: 0.6033812165260315\n",
      "Epoch [9/10] Batch [13200/20655] Loss_D_A: 0.34178268909454346 Loss_D_B: 0.9299145936965942 Loss_G: 0.4880710244178772\n",
      "Epoch [9/10] Batch [13300/20655] Loss_D_A: 0.8020864725112915 Loss_D_B: 0.6548475027084351 Loss_G: 0.8614977598190308\n",
      "Epoch [9/10] Batch [13400/20655] Loss_D_A: 0.17206817865371704 Loss_D_B: 0.5724560618400574 Loss_G: 0.3187497854232788\n",
      "Epoch [9/10] Batch [13500/20655] Loss_D_A: 0.39199918508529663 Loss_D_B: 0.033860862255096436 Loss_G: 0.01976543664932251\n",
      "Epoch [9/10] Batch [13600/20655] Loss_D_A: 0.6071410775184631 Loss_D_B: 0.270465612411499 Loss_G: 0.5177425742149353\n",
      "Epoch [9/10] Batch [13700/20655] Loss_D_A: 0.09901565313339233 Loss_D_B: 0.1586439609527588 Loss_G: 0.43943917751312256\n",
      "Epoch [9/10] Batch [13800/20655] Loss_D_A: 0.8616979122161865 Loss_D_B: 0.6453750729560852 Loss_G: 0.7243867516517639\n",
      "Epoch [9/10] Batch [13900/20655] Loss_D_A: 0.6243749856948853 Loss_D_B: 0.5262045860290527 Loss_G: 0.14000827074050903\n",
      "Epoch [9/10] Batch [14000/20655] Loss_D_A: 0.5081352591514587 Loss_D_B: 0.9630736708641052 Loss_G: 0.24580109119415283\n",
      "Epoch [9/10] Batch [14100/20655] Loss_D_A: 0.7144157290458679 Loss_D_B: 0.7456335425376892 Loss_G: 0.4716324806213379\n",
      "Epoch [9/10] Batch [14200/20655] Loss_D_A: 0.7083085179328918 Loss_D_B: 0.18648040294647217 Loss_G: 0.35277408361434937\n",
      "Epoch [9/10] Batch [14300/20655] Loss_D_A: 0.8222097754478455 Loss_D_B: 0.5128246545791626 Loss_G: 0.8587473034858704\n",
      "Epoch [9/10] Batch [14400/20655] Loss_D_A: 0.95761638879776 Loss_D_B: 0.8018301129341125 Loss_G: 0.610405445098877\n",
      "Epoch [9/10] Batch [14500/20655] Loss_D_A: 0.0648045539855957 Loss_D_B: 0.29985755681991577 Loss_G: 0.12166815996170044\n",
      "Epoch [9/10] Batch [14600/20655] Loss_D_A: 0.5274423956871033 Loss_D_B: 0.3928341269493103 Loss_G: 0.1336076855659485\n",
      "Epoch [9/10] Batch [14700/20655] Loss_D_A: 0.9399079084396362 Loss_D_B: 0.8545463681221008 Loss_G: 0.4653524160385132\n",
      "Epoch [9/10] Batch [14800/20655] Loss_D_A: 0.7259530425071716 Loss_D_B: 0.991547703742981 Loss_G: 0.3571022152900696\n",
      "Epoch [9/10] Batch [14900/20655] Loss_D_A: 0.10965412855148315 Loss_D_B: 0.7844865322113037 Loss_G: 0.17821985483169556\n",
      "Epoch [9/10] Batch [15000/20655] Loss_D_A: 0.6376519799232483 Loss_D_B: 0.09374916553497314 Loss_G: 0.4320695996284485\n",
      "Epoch [9/10] Batch [15100/20655] Loss_D_A: 0.16181153059005737 Loss_D_B: 0.5791899561882019 Loss_G: 0.07623744010925293\n",
      "Epoch [9/10] Batch [15200/20655] Loss_D_A: 0.6572569608688354 Loss_D_B: 0.8025081157684326 Loss_G: 0.21594059467315674\n",
      "Epoch [9/10] Batch [15300/20655] Loss_D_A: 0.5944918990135193 Loss_D_B: 0.6893407702445984 Loss_G: 0.356295108795166\n",
      "Epoch [9/10] Batch [15400/20655] Loss_D_A: 0.8264334201812744 Loss_D_B: 0.6335892081260681 Loss_G: 0.8172920346260071\n",
      "Epoch [9/10] Batch [15500/20655] Loss_D_A: 0.878875195980072 Loss_D_B: 0.3227803707122803 Loss_G: 0.13619756698608398\n",
      "Epoch [9/10] Batch [15600/20655] Loss_D_A: 0.42170286178588867 Loss_D_B: 0.5099698305130005 Loss_G: 0.8515224456787109\n",
      "Epoch [9/10] Batch [15700/20655] Loss_D_A: 0.9645978212356567 Loss_D_B: 0.5324742794036865 Loss_G: 0.9461398124694824\n",
      "Epoch [9/10] Batch [15800/20655] Loss_D_A: 0.1689968705177307 Loss_D_B: 0.6074023246765137 Loss_G: 0.7404525279998779\n",
      "Epoch [9/10] Batch [15900/20655] Loss_D_A: 0.3735775351524353 Loss_D_B: 0.33735376596450806 Loss_G: 0.22631597518920898\n",
      "Epoch [9/10] Batch [16000/20655] Loss_D_A: 0.4817348122596741 Loss_D_B: 0.0899808406829834 Loss_G: 0.4705089330673218\n",
      "Epoch [9/10] Batch [16100/20655] Loss_D_A: 0.3899970054626465 Loss_D_B: 0.8264479637145996 Loss_G: 0.606078028678894\n",
      "Epoch [9/10] Batch [16200/20655] Loss_D_A: 0.8897749781608582 Loss_D_B: 0.5152145624160767 Loss_G: 0.9698250889778137\n",
      "Epoch [9/10] Batch [16300/20655] Loss_D_A: 0.4126043915748596 Loss_D_B: 0.925294041633606 Loss_G: 0.027969837188720703\n",
      "Epoch [9/10] Batch [16400/20655] Loss_D_A: 0.050647735595703125 Loss_D_B: 0.37955427169799805 Loss_G: 0.2556225061416626\n",
      "Epoch [9/10] Batch [16500/20655] Loss_D_A: 0.2646045684814453 Loss_D_B: 0.6381480097770691 Loss_G: 0.14561086893081665\n",
      "Epoch [9/10] Batch [16600/20655] Loss_D_A: 0.881716251373291 Loss_D_B: 0.2525354027748108 Loss_G: 0.872580349445343\n",
      "Epoch [9/10] Batch [16700/20655] Loss_D_A: 0.12473160028457642 Loss_D_B: 0.025444746017456055 Loss_G: 0.5752133727073669\n",
      "Epoch [9/10] Batch [16800/20655] Loss_D_A: 0.31979554891586304 Loss_D_B: 0.06799334287643433 Loss_G: 0.9781620502471924\n",
      "Epoch [9/10] Batch [16900/20655] Loss_D_A: 0.37169384956359863 Loss_D_B: 0.3150208592414856 Loss_G: 0.7065721154212952\n",
      "Epoch [9/10] Batch [17000/20655] Loss_D_A: 0.19587987661361694 Loss_D_B: 0.21360552310943604 Loss_G: 0.7130993604660034\n",
      "Epoch [9/10] Batch [17100/20655] Loss_D_A: 0.30469000339508057 Loss_D_B: 0.17295581102371216 Loss_G: 0.6166670918464661\n",
      "Epoch [9/10] Batch [17200/20655] Loss_D_A: 0.5054634809494019 Loss_D_B: 0.8487454652786255 Loss_G: 0.34377896785736084\n",
      "Epoch [9/10] Batch [17300/20655] Loss_D_A: 0.7254590392112732 Loss_D_B: 0.15575605630874634 Loss_G: 0.3737573027610779\n",
      "Epoch [9/10] Batch [17400/20655] Loss_D_A: 0.6188033819198608 Loss_D_B: 0.24069052934646606 Loss_G: 0.403192400932312\n",
      "Epoch [9/10] Batch [17500/20655] Loss_D_A: 0.10378086566925049 Loss_D_B: 0.8108806610107422 Loss_G: 0.4411718249320984\n",
      "Epoch [9/10] Batch [17600/20655] Loss_D_A: 0.7252101302146912 Loss_D_B: 0.6025807857513428 Loss_G: 0.38027364015579224\n",
      "Epoch [9/10] Batch [17700/20655] Loss_D_A: 0.832520604133606 Loss_D_B: 0.6268966197967529 Loss_G: 0.8804659843444824\n",
      "Epoch [9/10] Batch [17800/20655] Loss_D_A: 0.45390182733535767 Loss_D_B: 0.526303231716156 Loss_G: 0.917198121547699\n",
      "Epoch [9/10] Batch [17900/20655] Loss_D_A: 0.7007595896720886 Loss_D_B: 0.7300075888633728 Loss_G: 0.5683993697166443\n",
      "Epoch [9/10] Batch [18000/20655] Loss_D_A: 0.9675126671791077 Loss_D_B: 0.666831374168396 Loss_G: 0.08148366212844849\n",
      "Epoch [9/10] Batch [18100/20655] Loss_D_A: 0.390003502368927 Loss_D_B: 0.4571797847747803 Loss_G: 0.6830520033836365\n",
      "Epoch [9/10] Batch [18200/20655] Loss_D_A: 0.821935772895813 Loss_D_B: 0.1509832739830017 Loss_G: 0.7700483798980713\n",
      "Epoch [9/10] Batch [18300/20655] Loss_D_A: 0.587105929851532 Loss_D_B: 0.43769723176956177 Loss_G: 0.6509181261062622\n",
      "Epoch [9/10] Batch [18400/20655] Loss_D_A: 0.4910711646080017 Loss_D_B: 0.05153334140777588 Loss_G: 0.2860943675041199\n",
      "Epoch [9/10] Batch [18500/20655] Loss_D_A: 0.45663416385650635 Loss_D_B: 0.8000432252883911 Loss_G: 0.32829612493515015\n",
      "Epoch [9/10] Batch [18600/20655] Loss_D_A: 0.1440865397453308 Loss_D_B: 0.7333159446716309 Loss_G: 0.8538827300071716\n",
      "Epoch [9/10] Batch [18700/20655] Loss_D_A: 0.3924632668495178 Loss_D_B: 0.25444114208221436 Loss_G: 0.22886282205581665\n",
      "Epoch [9/10] Batch [18800/20655] Loss_D_A: 0.08823508024215698 Loss_D_B: 0.16352415084838867 Loss_G: 0.7077045440673828\n",
      "Epoch [9/10] Batch [18900/20655] Loss_D_A: 0.4408172369003296 Loss_D_B: 0.678404688835144 Loss_G: 0.9481053948402405\n",
      "Epoch [9/10] Batch [19000/20655] Loss_D_A: 0.47962480783462524 Loss_D_B: 0.21883714199066162 Loss_G: 0.14798086881637573\n",
      "Epoch [9/10] Batch [19100/20655] Loss_D_A: 0.8444985747337341 Loss_D_B: 0.6881935000419617 Loss_G: 0.8637382984161377\n",
      "Epoch [9/10] Batch [19200/20655] Loss_D_A: 0.9207785725593567 Loss_D_B: 0.46482741832733154 Loss_G: 0.1793212890625\n",
      "Epoch [9/10] Batch [19300/20655] Loss_D_A: 0.4036661982536316 Loss_D_B: 0.9633052349090576 Loss_G: 0.44588011503219604\n",
      "Epoch [9/10] Batch [19400/20655] Loss_D_A: 0.36103618144989014 Loss_D_B: 0.9418132901191711 Loss_G: 0.7138168215751648\n",
      "Epoch [9/10] Batch [19500/20655] Loss_D_A: 0.5776142477989197 Loss_D_B: 0.7044487595558167 Loss_G: 0.24584341049194336\n",
      "Epoch [9/10] Batch [19600/20655] Loss_D_A: 0.5508477091789246 Loss_D_B: 0.9171533584594727 Loss_G: 0.39469510316848755\n",
      "Epoch [9/10] Batch [19700/20655] Loss_D_A: 0.645119309425354 Loss_D_B: 0.40268248319625854 Loss_G: 0.2412739396095276\n",
      "Epoch [9/10] Batch [19800/20655] Loss_D_A: 0.18077009916305542 Loss_D_B: 0.48127281665802 Loss_G: 0.4735552668571472\n",
      "Epoch [9/10] Batch [19900/20655] Loss_D_A: 0.4059711694717407 Loss_D_B: 0.34853583574295044 Loss_G: 0.5767170190811157\n",
      "Epoch [9/10] Batch [20000/20655] Loss_D_A: 0.6663891673088074 Loss_D_B: 0.6210079193115234 Loss_G: 0.6747834086418152\n",
      "Epoch [9/10] Batch [20100/20655] Loss_D_A: 0.8750645518302917 Loss_D_B: 0.4729578495025635 Loss_G: 0.4054450988769531\n",
      "Epoch [9/10] Batch [20200/20655] Loss_D_A: 0.9893493056297302 Loss_D_B: 0.707685112953186 Loss_G: 0.5070642828941345\n",
      "Epoch [9/10] Batch [20300/20655] Loss_D_A: 0.38618969917297363 Loss_D_B: 0.08735817670822144 Loss_G: 0.7222355604171753\n",
      "Epoch [9/10] Batch [20400/20655] Loss_D_A: 0.0938977599143982 Loss_D_B: 0.4344198703765869 Loss_G: 0.8981983661651611\n",
      "Epoch [9/10] Batch [20500/20655] Loss_D_A: 0.5963252782821655 Loss_D_B: 0.866009533405304 Loss_G: 0.5774486064910889\n",
      "Epoch [9/10] Batch [20600/20655] Loss_D_A: 0.6010239124298096 Loss_D_B: 0.7403450012207031 Loss_G: 0.6441022753715515\n",
      "Epoch [10/10] Batch [0/20655] Loss_D_A: 0.7185193300247192 Loss_D_B: 5.2928924560546875e-05 Loss_G: 0.6681438684463501\n",
      "Epoch [10/10] Batch [0/20655] Loss_D_A: 0.6118133068084717 Loss_D_B: 0.2838956117630005 Loss_G: 0.7008470296859741\n",
      "Epoch [10/10] Batch [100/20655] Loss_D_A: 0.5654808878898621 Loss_D_B: 0.7473573088645935 Loss_G: 0.6869162321090698\n",
      "Epoch [10/10] Batch [200/20655] Loss_D_A: 0.41966283321380615 Loss_D_B: 0.8402213454246521 Loss_G: 0.8573092818260193\n",
      "Epoch [10/10] Batch [300/20655] Loss_D_A: 0.8722388744354248 Loss_D_B: 0.11267799139022827 Loss_G: 0.06439357995986938\n",
      "Epoch [10/10] Batch [400/20655] Loss_D_A: 0.7862420678138733 Loss_D_B: 0.7471579313278198 Loss_G: 0.6764302253723145\n",
      "Epoch [10/10] Batch [500/20655] Loss_D_A: 0.8640351891517639 Loss_D_B: 0.5477339625358582 Loss_G: 0.6452892422676086\n",
      "Epoch [10/10] Batch [600/20655] Loss_D_A: 0.303499698638916 Loss_D_B: 0.9122805595397949 Loss_G: 0.9124268889427185\n",
      "Epoch [10/10] Batch [700/20655] Loss_D_A: 0.026385486125946045 Loss_D_B: 0.49524247646331787 Loss_G: 0.39152228832244873\n",
      "Epoch [10/10] Batch [800/20655] Loss_D_A: 0.025136172771453857 Loss_D_B: 0.03153383731842041 Loss_G: 0.49586158990859985\n",
      "Epoch [10/10] Batch [900/20655] Loss_D_A: 0.37501776218414307 Loss_D_B: 0.16032427549362183 Loss_G: 0.12368994951248169\n",
      "Epoch [10/10] Batch [1000/20655] Loss_D_A: 0.8688138127326965 Loss_D_B: 0.8487192988395691 Loss_G: 0.38155925273895264\n",
      "Epoch [10/10] Batch [1100/20655] Loss_D_A: 0.3154955506324768 Loss_D_B: 0.8258649110794067 Loss_G: 0.4277433156967163\n",
      "Epoch [10/10] Batch [1200/20655] Loss_D_A: 0.1840522289276123 Loss_D_B: 0.7956642508506775 Loss_G: 0.21970319747924805\n",
      "Epoch [10/10] Batch [1300/20655] Loss_D_A: 0.14770501852035522 Loss_D_B: 0.9978418946266174 Loss_G: 0.31801992654800415\n",
      "Epoch [10/10] Batch [1400/20655] Loss_D_A: 0.7476301789283752 Loss_D_B: 0.774263858795166 Loss_G: 0.11580777168273926\n",
      "Epoch [10/10] Batch [1500/20655] Loss_D_A: 0.8620489239692688 Loss_D_B: 0.1299998164176941 Loss_G: 0.4339028596878052\n",
      "Epoch [10/10] Batch [1600/20655] Loss_D_A: 0.6238040328025818 Loss_D_B: 0.9516279697418213 Loss_G: 0.8378045558929443\n",
      "Epoch [10/10] Batch [1700/20655] Loss_D_A: 0.7231225371360779 Loss_D_B: 0.828208863735199 Loss_G: 0.6530972123146057\n",
      "Epoch [10/10] Batch [1800/20655] Loss_D_A: 0.32466018199920654 Loss_D_B: 0.35080796480178833 Loss_G: 0.2660059928894043\n",
      "Epoch [10/10] Batch [1900/20655] Loss_D_A: 0.1823807954788208 Loss_D_B: 0.5356478691101074 Loss_G: 0.9616116881370544\n",
      "Epoch [10/10] Batch [2000/20655] Loss_D_A: 0.0886654257774353 Loss_D_B: 0.023795008659362793 Loss_G: 0.9790909290313721\n",
      "Epoch [10/10] Batch [2100/20655] Loss_D_A: 0.8713118433952332 Loss_D_B: 0.08857536315917969 Loss_G: 0.7220396399497986\n",
      "Epoch [10/10] Batch [2200/20655] Loss_D_A: 0.6403268575668335 Loss_D_B: 0.23151516914367676 Loss_G: 0.9281224608421326\n",
      "Epoch [10/10] Batch [2300/20655] Loss_D_A: 0.3742119073867798 Loss_D_B: 0.476962685585022 Loss_G: 0.7522903084754944\n",
      "Epoch [10/10] Batch [2400/20655] Loss_D_A: 0.4729738235473633 Loss_D_B: 0.4007717967033386 Loss_G: 0.6679366230964661\n",
      "Epoch [10/10] Batch [2500/20655] Loss_D_A: 0.9195055961608887 Loss_D_B: 0.02602517604827881 Loss_G: 0.24066120386123657\n",
      "Epoch [10/10] Batch [2600/20655] Loss_D_A: 0.7510051131248474 Loss_D_B: 0.20439720153808594 Loss_G: 0.6663429737091064\n",
      "Epoch [10/10] Batch [2700/20655] Loss_D_A: 0.2981264591217041 Loss_D_B: 0.6142417788505554 Loss_G: 0.9026579856872559\n",
      "Epoch [10/10] Batch [2800/20655] Loss_D_A: 0.8211150765419006 Loss_D_B: 0.9984503388404846 Loss_G: 0.7765054702758789\n",
      "Epoch [10/10] Batch [2900/20655] Loss_D_A: 0.4852919578552246 Loss_D_B: 0.13609832525253296 Loss_G: 0.2218065857887268\n",
      "Epoch [10/10] Batch [3000/20655] Loss_D_A: 0.5949358344078064 Loss_D_B: 0.12993663549423218 Loss_G: 0.9437752366065979\n",
      "Epoch [10/10] Batch [3100/20655] Loss_D_A: 0.9368022084236145 Loss_D_B: 0.9616711139678955 Loss_G: 0.16102248430252075\n",
      "Epoch [10/10] Batch [3200/20655] Loss_D_A: 0.8492944836616516 Loss_D_B: 0.8668604493141174 Loss_G: 0.6647125482559204\n",
      "Epoch [10/10] Batch [3300/20655] Loss_D_A: 0.32007622718811035 Loss_D_B: 0.16869986057281494 Loss_G: 0.54353266954422\n",
      "Epoch [10/10] Batch [3400/20655] Loss_D_A: 0.14478343725204468 Loss_D_B: 0.1835201382637024 Loss_G: 0.025354504585266113\n",
      "Epoch [10/10] Batch [3500/20655] Loss_D_A: 0.9643979072570801 Loss_D_B: 0.27452051639556885 Loss_G: 0.2898235321044922\n",
      "Epoch [10/10] Batch [3600/20655] Loss_D_A: 0.6052706241607666 Loss_D_B: 0.9467315673828125 Loss_G: 0.7344166040420532\n",
      "Epoch [10/10] Batch [3700/20655] Loss_D_A: 0.39590638875961304 Loss_D_B: 0.17631793022155762 Loss_G: 0.385616660118103\n",
      "Epoch [10/10] Batch [3800/20655] Loss_D_A: 0.6235600113868713 Loss_D_B: 0.32050877809524536 Loss_G: 0.6187325716018677\n",
      "Epoch [10/10] Batch [3900/20655] Loss_D_A: 0.9543868899345398 Loss_D_B: 0.6391662955284119 Loss_G: 0.6903822422027588\n",
      "Epoch [10/10] Batch [4000/20655] Loss_D_A: 0.5654879808425903 Loss_D_B: 0.6358857750892639 Loss_G: 0.7170960903167725\n",
      "Epoch [10/10] Batch [4100/20655] Loss_D_A: 0.3136395215988159 Loss_D_B: 0.08393305540084839 Loss_G: 0.1063230037689209\n",
      "Epoch [10/10] Batch [4200/20655] Loss_D_A: 0.8961923718452454 Loss_D_B: 0.24628031253814697 Loss_G: 0.5571803450584412\n",
      "Epoch [10/10] Batch [4300/20655] Loss_D_A: 0.5781406164169312 Loss_D_B: 0.5777490735054016 Loss_G: 0.9826489090919495\n",
      "Epoch [10/10] Batch [4400/20655] Loss_D_A: 0.3280050754547119 Loss_D_B: 0.013344049453735352 Loss_G: 0.6461430191993713\n",
      "Epoch [10/10] Batch [4500/20655] Loss_D_A: 0.12862586975097656 Loss_D_B: 0.7326129674911499 Loss_G: 0.7846341133117676\n",
      "Epoch [10/10] Batch [4600/20655] Loss_D_A: 0.4725046753883362 Loss_D_B: 0.03451424837112427 Loss_G: 0.03294438123703003\n",
      "Epoch [10/10] Batch [4700/20655] Loss_D_A: 0.5399090051651001 Loss_D_B: 0.7410515546798706 Loss_G: 0.353082537651062\n",
      "Epoch [10/10] Batch [4800/20655] Loss_D_A: 0.7713490724563599 Loss_D_B: 0.9559780955314636 Loss_G: 0.7151101231575012\n",
      "Epoch [10/10] Batch [4900/20655] Loss_D_A: 0.9105739593505859 Loss_D_B: 0.4073951840400696 Loss_G: 0.36238008737564087\n",
      "Epoch [10/10] Batch [5000/20655] Loss_D_A: 0.28284621238708496 Loss_D_B: 0.7144675850868225 Loss_G: 0.9317241311073303\n",
      "Epoch [10/10] Batch [5100/20655] Loss_D_A: 0.13167214393615723 Loss_D_B: 0.77546226978302 Loss_G: 0.29772675037384033\n",
      "Epoch [10/10] Batch [5200/20655] Loss_D_A: 0.32505321502685547 Loss_D_B: 0.14099198579788208 Loss_G: 0.25698018074035645\n",
      "Epoch [10/10] Batch [5300/20655] Loss_D_A: 0.2085362672805786 Loss_D_B: 0.7821961641311646 Loss_G: 0.4885404706001282\n",
      "Epoch [10/10] Batch [5400/20655] Loss_D_A: 0.3643839359283447 Loss_D_B: 0.1437385082244873 Loss_G: 0.6707550883293152\n",
      "Epoch [10/10] Batch [5500/20655] Loss_D_A: 0.4908933639526367 Loss_D_B: 0.054179489612579346 Loss_G: 0.12569475173950195\n",
      "Epoch [10/10] Batch [5600/20655] Loss_D_A: 0.8848505020141602 Loss_D_B: 0.0737946629524231 Loss_G: 0.22765284776687622\n",
      "Epoch [10/10] Batch [5700/20655] Loss_D_A: 0.6962823867797852 Loss_D_B: 0.48034268617630005 Loss_G: 0.06767082214355469\n",
      "Epoch [10/10] Batch [5800/20655] Loss_D_A: 0.36079466342926025 Loss_D_B: 0.8325529098510742 Loss_G: 0.1328989863395691\n",
      "Epoch [10/10] Batch [5900/20655] Loss_D_A: 0.4206603169441223 Loss_D_B: 0.12514400482177734 Loss_G: 0.27231818437576294\n",
      "Epoch [10/10] Batch [6000/20655] Loss_D_A: 0.8553136587142944 Loss_D_B: 0.2960541248321533 Loss_G: 0.6783514618873596\n",
      "Epoch [10/10] Batch [6100/20655] Loss_D_A: 0.2515868544578552 Loss_D_B: 0.4171891212463379 Loss_G: 0.12171638011932373\n",
      "Epoch [10/10] Batch [6200/20655] Loss_D_A: 0.7134178876876831 Loss_D_B: 0.8836743831634521 Loss_G: 0.1967601776123047\n",
      "Epoch [10/10] Batch [6300/20655] Loss_D_A: 0.9529256820678711 Loss_D_B: 0.8709170818328857 Loss_G: 0.8361383676528931\n",
      "Epoch [10/10] Batch [6400/20655] Loss_D_A: 0.5732018947601318 Loss_D_B: 0.18131279945373535 Loss_G: 0.5047698020935059\n",
      "Epoch [10/10] Batch [6500/20655] Loss_D_A: 0.5622926354408264 Loss_D_B: 0.4889010190963745 Loss_G: 0.2500651478767395\n",
      "Epoch [10/10] Batch [6600/20655] Loss_D_A: 0.48199379444122314 Loss_D_B: 0.3761098384857178 Loss_G: 0.19826054573059082\n",
      "Epoch [10/10] Batch [6700/20655] Loss_D_A: 0.5113174319267273 Loss_D_B: 0.1862693428993225 Loss_G: 0.9667736887931824\n",
      "Epoch [10/10] Batch [6800/20655] Loss_D_A: 0.5099067687988281 Loss_D_B: 0.3125675320625305 Loss_G: 0.4135417938232422\n",
      "Epoch [10/10] Batch [6900/20655] Loss_D_A: 0.12163335084915161 Loss_D_B: 0.2862943410873413 Loss_G: 0.284130334854126\n",
      "Epoch [10/10] Batch [7000/20655] Loss_D_A: 0.011794984340667725 Loss_D_B: 0.39657139778137207 Loss_G: 0.8139609694480896\n",
      "Epoch [10/10] Batch [7100/20655] Loss_D_A: 0.87016761302948 Loss_D_B: 0.3875235915184021 Loss_G: 0.738361656665802\n",
      "Epoch [10/10] Batch [7200/20655] Loss_D_A: 0.46025311946868896 Loss_D_B: 0.8766005039215088 Loss_G: 0.29165470600128174\n",
      "Epoch [10/10] Batch [7300/20655] Loss_D_A: 0.8166918754577637 Loss_D_B: 0.2314552664756775 Loss_G: 0.14450198411941528\n",
      "Epoch [10/10] Batch [7400/20655] Loss_D_A: 0.5715028643608093 Loss_D_B: 0.4078094959259033 Loss_G: 0.7301421761512756\n",
      "Epoch [10/10] Batch [7500/20655] Loss_D_A: 0.530829131603241 Loss_D_B: 0.16353392601013184 Loss_G: 0.6766679286956787\n",
      "Epoch [10/10] Batch [7600/20655] Loss_D_A: 0.5417531132698059 Loss_D_B: 0.3062957525253296 Loss_G: 0.9595127105712891\n",
      "Epoch [10/10] Batch [7700/20655] Loss_D_A: 0.5417959094047546 Loss_D_B: 0.13581794500350952 Loss_G: 0.15927064418792725\n",
      "Epoch [10/10] Batch [7800/20655] Loss_D_A: 0.6876401305198669 Loss_D_B: 0.5272541642189026 Loss_G: 0.9735677242279053\n",
      "Epoch [10/10] Batch [7900/20655] Loss_D_A: 0.26006656885147095 Loss_D_B: 0.4260222315788269 Loss_G: 0.33848845958709717\n",
      "Epoch [10/10] Batch [8000/20655] Loss_D_A: 0.7297811508178711 Loss_D_B: 0.7887540459632874 Loss_G: 0.6512993574142456\n",
      "Epoch [10/10] Batch [8100/20655] Loss_D_A: 0.5032486915588379 Loss_D_B: 0.9263672232627869 Loss_G: 0.3488619327545166\n",
      "Epoch [10/10] Batch [8200/20655] Loss_D_A: 0.4203339219093323 Loss_D_B: 0.6647711396217346 Loss_G: 0.48775607347488403\n",
      "Epoch [10/10] Batch [8300/20655] Loss_D_A: 0.013590335845947266 Loss_D_B: 0.2903311252593994 Loss_G: 0.7374171614646912\n",
      "Epoch [10/10] Batch [8400/20655] Loss_D_A: 0.17876243591308594 Loss_D_B: 0.8484988212585449 Loss_G: 0.6860158443450928\n",
      "Epoch [10/10] Batch [8500/20655] Loss_D_A: 0.5687686204910278 Loss_D_B: 0.2576103210449219 Loss_G: 0.8206148147583008\n",
      "Epoch [10/10] Batch [8600/20655] Loss_D_A: 0.019816994667053223 Loss_D_B: 0.007048130035400391 Loss_G: 0.19704699516296387\n",
      "Epoch [10/10] Batch [8700/20655] Loss_D_A: 0.7557546496391296 Loss_D_B: 0.2730221748352051 Loss_G: 0.4155530333518982\n",
      "Epoch [10/10] Batch [8800/20655] Loss_D_A: 0.013703703880310059 Loss_D_B: 0.8652781844139099 Loss_G: 0.2591428756713867\n",
      "Epoch [10/10] Batch [8900/20655] Loss_D_A: 0.67606121301651 Loss_D_B: 0.4225114583969116 Loss_G: 0.30821144580841064\n",
      "Epoch [10/10] Batch [9000/20655] Loss_D_A: 0.639278769493103 Loss_D_B: 0.2943260669708252 Loss_G: 0.7522560954093933\n",
      "Epoch [10/10] Batch [9100/20655] Loss_D_A: 0.2072349190711975 Loss_D_B: 0.8959729671478271 Loss_G: 0.013962030410766602\n",
      "Epoch [10/10] Batch [9200/20655] Loss_D_A: 0.5886294841766357 Loss_D_B: 0.9863418936729431 Loss_G: 0.7295777797698975\n",
      "Epoch [10/10] Batch [9300/20655] Loss_D_A: 0.39132779836654663 Loss_D_B: 0.46305131912231445 Loss_G: 0.7919324040412903\n",
      "Epoch [10/10] Batch [9400/20655] Loss_D_A: 0.9409536123275757 Loss_D_B: 0.012222826480865479 Loss_G: 0.8716030716896057\n",
      "Epoch [10/10] Batch [9500/20655] Loss_D_A: 0.9142793416976929 Loss_D_B: 0.50882488489151 Loss_G: 0.9775611758232117\n",
      "Epoch [10/10] Batch [9600/20655] Loss_D_A: 0.9978388547897339 Loss_D_B: 0.5538457036018372 Loss_G: 0.14929944276809692\n",
      "Epoch [10/10] Batch [9700/20655] Loss_D_A: 0.37426918745040894 Loss_D_B: 0.6189422607421875 Loss_G: 0.8613947033882141\n",
      "Epoch [10/10] Batch [9800/20655] Loss_D_A: 0.8543307781219482 Loss_D_B: 0.4943608045578003 Loss_G: 0.514312744140625\n",
      "Epoch [10/10] Batch [9900/20655] Loss_D_A: 0.38339340686798096 Loss_D_B: 0.929802656173706 Loss_G: 0.7819908857345581\n",
      "Epoch [10/10] Batch [10000/20655] Loss_D_A: 0.8164498209953308 Loss_D_B: 0.4408119320869446 Loss_G: 0.5553756952285767\n",
      "Epoch [10/10] Batch [10100/20655] Loss_D_A: 0.5034314393997192 Loss_D_B: 0.3936220407485962 Loss_G: 0.5146951079368591\n",
      "Epoch [10/10] Batch [10200/20655] Loss_D_A: 0.831063985824585 Loss_D_B: 0.19212090969085693 Loss_G: 0.4861071705818176\n",
      "Epoch [10/10] Batch [10300/20655] Loss_D_A: 0.20988065004348755 Loss_D_B: 0.16470342874526978 Loss_G: 0.22847765684127808\n",
      "Epoch [10/10] Batch [10400/20655] Loss_D_A: 0.2273535132408142 Loss_D_B: 0.8537114858627319 Loss_G: 0.03289681673049927\n",
      "Epoch [10/10] Batch [10500/20655] Loss_D_A: 0.9976027011871338 Loss_D_B: 0.47766798734664917 Loss_G: 0.8323822021484375\n",
      "Epoch [10/10] Batch [10600/20655] Loss_D_A: 0.7169503569602966 Loss_D_B: 0.20511478185653687 Loss_G: 0.5172136425971985\n",
      "Epoch [10/10] Batch [10700/20655] Loss_D_A: 0.844022810459137 Loss_D_B: 0.6968749165534973 Loss_G: 0.8625561594963074\n",
      "Epoch [10/10] Batch [10800/20655] Loss_D_A: 0.6143010258674622 Loss_D_B: 0.5502989292144775 Loss_G: 0.7337501645088196\n",
      "Epoch [10/10] Batch [10900/20655] Loss_D_A: 0.2604767680168152 Loss_D_B: 0.7156641483306885 Loss_G: 0.9488343000411987\n",
      "Epoch [10/10] Batch [11000/20655] Loss_D_A: 0.33893710374832153 Loss_D_B: 0.7847352027893066 Loss_G: 0.40468740463256836\n",
      "Epoch [10/10] Batch [11100/20655] Loss_D_A: 0.30937063694000244 Loss_D_B: 0.960809051990509 Loss_G: 0.032878577709198\n",
      "Epoch [10/10] Batch [11200/20655] Loss_D_A: 0.5429317355155945 Loss_D_B: 0.11597591638565063 Loss_G: 0.9260764122009277\n",
      "Epoch [10/10] Batch [11300/20655] Loss_D_A: 0.5536302924156189 Loss_D_B: 0.6395843625068665 Loss_G: 0.07303959131240845\n",
      "Epoch [10/10] Batch [11400/20655] Loss_D_A: 0.6117913126945496 Loss_D_B: 0.31926798820495605 Loss_G: 0.6052161455154419\n",
      "Epoch [10/10] Batch [11500/20655] Loss_D_A: 0.36200445890426636 Loss_D_B: 0.1900002360343933 Loss_G: 0.981428325176239\n",
      "Epoch [10/10] Batch [11600/20655] Loss_D_A: 0.1870441436767578 Loss_D_B: 0.523420512676239 Loss_G: 0.8852635025978088\n",
      "Epoch [10/10] Batch [11700/20655] Loss_D_A: 0.36114150285720825 Loss_D_B: 0.2981875538825989 Loss_G: 0.6983354687690735\n",
      "Epoch [10/10] Batch [11800/20655] Loss_D_A: 0.042926669120788574 Loss_D_B: 0.6153014302253723 Loss_G: 0.05864083766937256\n",
      "Epoch [10/10] Batch [11900/20655] Loss_D_A: 0.969458818435669 Loss_D_B: 0.9528492093086243 Loss_G: 0.9589152932167053\n",
      "Epoch [10/10] Batch [12000/20655] Loss_D_A: 0.5082462430000305 Loss_D_B: 0.0954781174659729 Loss_G: 0.36736947298049927\n",
      "Epoch [10/10] Batch [12100/20655] Loss_D_A: 0.4990905523300171 Loss_D_B: 0.6920924186706543 Loss_G: 0.2890474200248718\n",
      "Epoch [10/10] Batch [12200/20655] Loss_D_A: 0.19938218593597412 Loss_D_B: 0.2283998727798462 Loss_G: 0.85296630859375\n",
      "Epoch [10/10] Batch [12300/20655] Loss_D_A: 0.1547827124595642 Loss_D_B: 0.7443763613700867 Loss_G: 0.04886782169342041\n",
      "Epoch [10/10] Batch [12400/20655] Loss_D_A: 0.3940521478652954 Loss_D_B: 0.883092999458313 Loss_G: 0.3866662383079529\n",
      "Epoch [10/10] Batch [12500/20655] Loss_D_A: 0.2877040505409241 Loss_D_B: 0.8377915620803833 Loss_G: 0.5655426383018494\n",
      "Epoch [10/10] Batch [12600/20655] Loss_D_A: 0.4291229248046875 Loss_D_B: 0.08038455247879028 Loss_G: 0.5622658729553223\n",
      "Epoch [10/10] Batch [12700/20655] Loss_D_A: 0.25484079122543335 Loss_D_B: 0.5862414240837097 Loss_G: 0.8598311543464661\n",
      "Epoch [10/10] Batch [12800/20655] Loss_D_A: 0.17505353689193726 Loss_D_B: 0.29430025815963745 Loss_G: 0.7718903422355652\n",
      "Epoch [10/10] Batch [12900/20655] Loss_D_A: 0.4239896535873413 Loss_D_B: 0.06942754983901978 Loss_G: 0.8257350325584412\n",
      "Epoch [10/10] Batch [13000/20655] Loss_D_A: 0.07544994354248047 Loss_D_B: 0.7950896620750427 Loss_G: 0.05602461099624634\n",
      "Epoch [10/10] Batch [13100/20655] Loss_D_A: 0.7058351039886475 Loss_D_B: 0.3372781276702881 Loss_G: 0.6694548726081848\n",
      "Epoch [10/10] Batch [13200/20655] Loss_D_A: 0.14550864696502686 Loss_D_B: 0.014050960540771484 Loss_G: 0.3703177571296692\n",
      "Epoch [10/10] Batch [13300/20655] Loss_D_A: 0.973429799079895 Loss_D_B: 0.3210819363594055 Loss_G: 0.5320706963539124\n",
      "Epoch [10/10] Batch [13400/20655] Loss_D_A: 0.8948792815208435 Loss_D_B: 0.9019923806190491 Loss_G: 0.9225168824195862\n",
      "Epoch [10/10] Batch [13500/20655] Loss_D_A: 0.5016508102416992 Loss_D_B: 0.44365447759628296 Loss_G: 0.5983887910842896\n",
      "Epoch [10/10] Batch [13600/20655] Loss_D_A: 0.45494282245635986 Loss_D_B: 0.6792669892311096 Loss_G: 0.12387305498123169\n",
      "Epoch [10/10] Batch [13700/20655] Loss_D_A: 0.12819260358810425 Loss_D_B: 0.3680006265640259 Loss_G: 0.45473700761795044\n",
      "Epoch [10/10] Batch [13800/20655] Loss_D_A: 0.7156529426574707 Loss_D_B: 0.41654878854751587 Loss_G: 0.8090074062347412\n",
      "Epoch [10/10] Batch [13900/20655] Loss_D_A: 0.11072933673858643 Loss_D_B: 0.6115921139717102 Loss_G: 0.8911190629005432\n",
      "Epoch [10/10] Batch [14000/20655] Loss_D_A: 0.9687647819519043 Loss_D_B: 0.7512511610984802 Loss_G: 0.9248493909835815\n",
      "Epoch [10/10] Batch [14100/20655] Loss_D_A: 0.7541173100471497 Loss_D_B: 0.521452009677887 Loss_G: 0.8052287697792053\n",
      "Epoch [10/10] Batch [14200/20655] Loss_D_A: 0.530444324016571 Loss_D_B: 0.17384517192840576 Loss_G: 0.8060516119003296\n",
      "Epoch [10/10] Batch [14300/20655] Loss_D_A: 0.7714545726776123 Loss_D_B: 0.10310715436935425 Loss_G: 0.4631463289260864\n",
      "Epoch [10/10] Batch [14400/20655] Loss_D_A: 0.2252560257911682 Loss_D_B: 0.4911302924156189 Loss_G: 0.3081168532371521\n",
      "Epoch [10/10] Batch [14500/20655] Loss_D_A: 0.6456237435340881 Loss_D_B: 0.08654206991195679 Loss_G: 0.18856924772262573\n",
      "Epoch [10/10] Batch [14600/20655] Loss_D_A: 0.5226062536239624 Loss_D_B: 0.27791154384613037 Loss_G: 0.3937123417854309\n",
      "Epoch [10/10] Batch [14700/20655] Loss_D_A: 0.9354320168495178 Loss_D_B: 0.6023760437965393 Loss_G: 0.167341947555542\n",
      "Epoch [10/10] Batch [14800/20655] Loss_D_A: 0.4725165367126465 Loss_D_B: 0.23226267099380493 Loss_G: 0.6300846934318542\n",
      "Epoch [10/10] Batch [14900/20655] Loss_D_A: 0.21427124738693237 Loss_D_B: 0.9685741662979126 Loss_G: 0.5338128805160522\n",
      "Epoch [10/10] Batch [15000/20655] Loss_D_A: 0.8774207234382629 Loss_D_B: 0.6033883094787598 Loss_G: 0.8213087916374207\n",
      "Epoch [10/10] Batch [15100/20655] Loss_D_A: 0.6293196082115173 Loss_D_B: 0.019687533378601074 Loss_G: 0.8055096864700317\n",
      "Epoch [10/10] Batch [15200/20655] Loss_D_A: 0.9999212026596069 Loss_D_B: 0.7935594320297241 Loss_G: 0.36101555824279785\n",
      "Epoch [10/10] Batch [15300/20655] Loss_D_A: 0.7241770625114441 Loss_D_B: 0.1909368634223938 Loss_G: 0.9866343140602112\n",
      "Epoch [10/10] Batch [15400/20655] Loss_D_A: 0.6766428351402283 Loss_D_B: 0.7781238555908203 Loss_G: 0.17783111333847046\n",
      "Epoch [10/10] Batch [15500/20655] Loss_D_A: 0.6134836077690125 Loss_D_B: 0.48129987716674805 Loss_G: 0.6133009195327759\n",
      "Epoch [10/10] Batch [15600/20655] Loss_D_A: 0.7631687521934509 Loss_D_B: 0.6691388487815857 Loss_G: 0.40385085344314575\n",
      "Epoch [10/10] Batch [15700/20655] Loss_D_A: 0.9198176860809326 Loss_D_B: 0.45897096395492554 Loss_G: 0.3177827000617981\n",
      "Epoch [10/10] Batch [15800/20655] Loss_D_A: 0.8676102757453918 Loss_D_B: 0.8680049777030945 Loss_G: 0.6342674493789673\n",
      "Epoch [10/10] Batch [15900/20655] Loss_D_A: 0.9369813203811646 Loss_D_B: 0.5514317750930786 Loss_G: 0.23881065845489502\n",
      "Epoch [10/10] Batch [16000/20655] Loss_D_A: 0.7996827363967896 Loss_D_B: 0.8547336459159851 Loss_G: 0.10630804300308228\n",
      "Epoch [10/10] Batch [16100/20655] Loss_D_A: 0.34831321239471436 Loss_D_B: 0.33446377515792847 Loss_G: 0.5739625096321106\n",
      "Epoch [10/10] Batch [16200/20655] Loss_D_A: 0.8408704400062561 Loss_D_B: 0.3568613529205322 Loss_G: 0.00170135498046875\n",
      "Epoch [10/10] Batch [16300/20655] Loss_D_A: 0.9076932668685913 Loss_D_B: 0.504093587398529 Loss_G: 0.33087366819381714\n",
      "Epoch [10/10] Batch [16400/20655] Loss_D_A: 0.8355712890625 Loss_D_B: 0.9450923800468445 Loss_G: 0.791745126247406\n",
      "Epoch [10/10] Batch [16500/20655] Loss_D_A: 0.7502535581588745 Loss_D_B: 0.5332583785057068 Loss_G: 0.8662902116775513\n",
      "Epoch [10/10] Batch [16600/20655] Loss_D_A: 0.032711029052734375 Loss_D_B: 0.204046368598938 Loss_G: 0.330494225025177\n",
      "Epoch [10/10] Batch [16700/20655] Loss_D_A: 0.5867729783058167 Loss_D_B: 0.6213561296463013 Loss_G: 0.2789769172668457\n",
      "Epoch [10/10] Batch [16800/20655] Loss_D_A: 0.8813230991363525 Loss_D_B: 0.4763370156288147 Loss_G: 0.3113609552383423\n",
      "Epoch [10/10] Batch [16900/20655] Loss_D_A: 0.6005956530570984 Loss_D_B: 0.8009947538375854 Loss_G: 0.16848307847976685\n",
      "Epoch [10/10] Batch [17000/20655] Loss_D_A: 0.6759831309318542 Loss_D_B: 0.7448819279670715 Loss_G: 0.41628575325012207\n",
      "Epoch [10/10] Batch [17100/20655] Loss_D_A: 0.6749414801597595 Loss_D_B: 0.6233821511268616 Loss_G: 0.16611170768737793\n",
      "Epoch [10/10] Batch [17200/20655] Loss_D_A: 0.17956924438476562 Loss_D_B: 0.3431490659713745 Loss_G: 0.7600299119949341\n",
      "Epoch [10/10] Batch [17300/20655] Loss_D_A: 0.08859986066818237 Loss_D_B: 0.4128432273864746 Loss_G: 0.3164149522781372\n",
      "Epoch [10/10] Batch [17400/20655] Loss_D_A: 0.8864631652832031 Loss_D_B: 0.8292176723480225 Loss_G: 0.33752989768981934\n",
      "Epoch [10/10] Batch [17500/20655] Loss_D_A: 0.9143581986427307 Loss_D_B: 0.6633252501487732 Loss_G: 0.8375847339630127\n",
      "Epoch [10/10] Batch [17600/20655] Loss_D_A: 0.6947128772735596 Loss_D_B: 0.3672364354133606 Loss_G: 0.34742724895477295\n",
      "Epoch [10/10] Batch [17700/20655] Loss_D_A: 0.34842032194137573 Loss_D_B: 0.7459886074066162 Loss_G: 0.0237463116645813\n",
      "Epoch [10/10] Batch [17800/20655] Loss_D_A: 0.7044482231140137 Loss_D_B: 0.05584132671356201 Loss_G: 0.14141088724136353\n",
      "Epoch [10/10] Batch [17900/20655] Loss_D_A: 0.9816716909408569 Loss_D_B: 0.21256566047668457 Loss_G: 0.8266112804412842\n",
      "Epoch [10/10] Batch [18000/20655] Loss_D_A: 0.18143188953399658 Loss_D_B: 0.7653543949127197 Loss_G: 0.6113754510879517\n",
      "Epoch [10/10] Batch [18100/20655] Loss_D_A: 0.17055338621139526 Loss_D_B: 0.438870370388031 Loss_G: 0.8458856344223022\n",
      "Epoch [10/10] Batch [18200/20655] Loss_D_A: 0.22900909185409546 Loss_D_B: 0.9776989817619324 Loss_G: 0.5719659924507141\n",
      "Epoch [10/10] Batch [18300/20655] Loss_D_A: 0.6933026909828186 Loss_D_B: 0.5299060940742493 Loss_G: 0.6120010018348694\n",
      "Epoch [10/10] Batch [18400/20655] Loss_D_A: 0.20721405744552612 Loss_D_B: 0.7082538604736328 Loss_G: 0.6499672532081604\n",
      "Epoch [10/10] Batch [18500/20655] Loss_D_A: 0.7235201597213745 Loss_D_B: 0.2146766185760498 Loss_G: 0.4801309108734131\n",
      "Epoch [10/10] Batch [18600/20655] Loss_D_A: 0.7086610794067383 Loss_D_B: 0.3212159276008606 Loss_G: 0.9850302934646606\n",
      "Epoch [10/10] Batch [18700/20655] Loss_D_A: 0.5514975190162659 Loss_D_B: 0.9043930172920227 Loss_G: 0.8812639713287354\n",
      "Epoch [10/10] Batch [18800/20655] Loss_D_A: 0.8190819025039673 Loss_D_B: 0.536777138710022 Loss_G: 0.18691003322601318\n",
      "Epoch [10/10] Batch [18900/20655] Loss_D_A: 0.07718789577484131 Loss_D_B: 0.9930627942085266 Loss_G: 0.1237635612487793\n",
      "Epoch [10/10] Batch [19000/20655] Loss_D_A: 0.599166750907898 Loss_D_B: 0.36760473251342773 Loss_G: 0.8502280116081238\n",
      "Epoch [10/10] Batch [19100/20655] Loss_D_A: 0.8166547417640686 Loss_D_B: 0.9468493461608887 Loss_G: 0.6364648342132568\n",
      "Epoch [10/10] Batch [19200/20655] Loss_D_A: 0.9745673537254333 Loss_D_B: 0.39937394857406616 Loss_G: 0.23097693920135498\n",
      "Epoch [10/10] Batch [19300/20655] Loss_D_A: 0.3693426251411438 Loss_D_B: 0.6142435669898987 Loss_G: 0.408366322517395\n",
      "Epoch [10/10] Batch [19400/20655] Loss_D_A: 0.07972830533981323 Loss_D_B: 0.001183152198791504 Loss_G: 0.5338281989097595\n",
      "Epoch [10/10] Batch [19500/20655] Loss_D_A: 0.7493903040885925 Loss_D_B: 0.10749328136444092 Loss_G: 0.9907999634742737\n",
      "Epoch [10/10] Batch [19600/20655] Loss_D_A: 0.40024495124816895 Loss_D_B: 0.46959662437438965 Loss_G: 0.1015821099281311\n",
      "Epoch [10/10] Batch [19700/20655] Loss_D_A: 0.6765449047088623 Loss_D_B: 0.9729418754577637 Loss_G: 0.5690258145332336\n",
      "Epoch [10/10] Batch [19800/20655] Loss_D_A: 0.07608598470687866 Loss_D_B: 0.7100374102592468 Loss_G: 0.6426302790641785\n",
      "Epoch [10/10] Batch [19900/20655] Loss_D_A: 0.6256815791130066 Loss_D_B: 0.9874902367591858 Loss_G: 0.29827386140823364\n",
      "Epoch [10/10] Batch [20000/20655] Loss_D_A: 0.4354305863380432 Loss_D_B: 0.03853034973144531 Loss_G: 0.10187560319900513\n",
      "Epoch [10/10] Batch [20100/20655] Loss_D_A: 0.2303295135498047 Loss_D_B: 0.4892706871032715 Loss_G: 0.6879812479019165\n",
      "Epoch [10/10] Batch [20200/20655] Loss_D_A: 0.30314046144485474 Loss_D_B: 0.968008816242218 Loss_G: 0.08068859577178955\n",
      "Epoch [10/10] Batch [20300/20655] Loss_D_A: 0.8040509819984436 Loss_D_B: 0.11732912063598633 Loss_G: 0.8518155813217163\n",
      "Epoch [10/10] Batch [20400/20655] Loss_D_A: 0.3037157654762268 Loss_D_B: 0.07943284511566162 Loss_G: 0.06346303224563599\n",
      "Epoch [10/10] Batch [20500/20655] Loss_D_A: 0.5897753238677979 Loss_D_B: 0.22859656810760498 Loss_G: 0.9711256623268127\n",
      "Epoch [10/10] Batch [20600/20655] Loss_D_A: 0.375907301902771 Loss_D_B: 0.6987228989601135 Loss_G: 0.06375133991241455\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        real_A = batch['photo'].to(device)\n",
    "        real_B = batch['sketch'].to(device)\n",
    "\n",
    "        # Generators forward\n",
    "        fake_B = G_A2B(real_A)\n",
    "        cycle_A = G_B2A(fake_B)\n",
    "        fake_A = G_B2A(real_B)\n",
    "        cycle_B = G_A2B(fake_A)\n",
    "\n",
    "        # Identity loss\n",
    "        loss_identity_B = criterion_identity(G_A2B(real_B), real_B) * 5.0\n",
    "        loss_identity_A = criterion_identity(G_B2A(real_A), real_A) * 5.0\n",
    "\n",
    "        # GAN loss\n",
    "        loss_GAN_A2B = criterion_GAN(D_B(fake_B), torch.ones_like(D_B(fake_B)).to(device))\n",
    "        loss_GAN_B2A = criterion_GAN(D_A(fake_A), torch.ones_like(D_A(fake_A)).to(device))\n",
    "\n",
    "        # Cycle consistency loss\n",
    "        loss_cycle_A = criterion_cycle(cycle_A, real_A) * 10.0\n",
    "        loss_cycle_B = criterion_cycle(cycle_B, real_B) * 10.0\n",
    "\n",
    "        # Total generators loss\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + loss_cycle_A + loss_cycle_B + loss_identity_A + loss_identity_B\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminator A\n",
    "        loss_D_A_total = (criterion_GAN(D_A(real_A), torch.ones_like(D_A(real_A)).to(device)) +\n",
    "                          criterion_GAN(D_A(fake_A_buffer.push_and_pop(fake_A).detach()), torch.zeros_like(D_A(fake_A).detach()).to(device))) * 0.5\n",
    "        optimizer_D_A.zero_grad()\n",
    "        loss_D_A_total.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        # Discriminator B\n",
    "        loss_D_B_total = (criterion_GAN(D_B(real_B), torch.ones_like(D_B(real_B)).to(device)) +\n",
    "                          criterion_GAN(D_B(fake_B_buffer.push_and_pop(fake_B).detach()), torch.zeros_like(D_B(fake_B).detach()).to(device))) * 0.5\n",
    "        optimizer_D_B.zero_grad()\n",
    "        loss_D_B_total.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{n_epochs}] Batch [{i}/{len(train_loader)}] \"\n",
    "                  f\"Loss_D_A: {loss_D_A_total.item():.4f} Loss_D_B: {loss_D_B_total.item():.4f} \"\n",
    "                  f\"Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "    save_checkpoint(epoch)\n",
    "\n",
    "    os.makedirs('samples', exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        sample_real_A = next(iter(train_loader))['photo'].to(device)\n",
    "        sample_real_B = next(iter(train_loader))['sketch'].to(device)\n",
    "        fake_B = G_A2B(sample_real_A)\n",
    "        save_image(fake_B, f'samples/fake_B_epoch_{epoch}.png', normalize=True)\n",
    "        fake_A = G_B2A(sample_real_B)\n",
    "        save_image(fake_A, f'samples/fake_A_epoch_{epoch}.png', normalize=True)\n",
    "\n",
    "def test():\n",
    "    G_A2B.eval()\n",
    "    G_B2A.eval()\n",
    "    os.makedirs('test_results', exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            real_A = batch['photo'].to(device)\n",
    "            real_B = batch['sketch'].to(device)\n",
    "\n",
    "            fake_B = G_A2B(real_A)\n",
    "            fake_A = G_B2A(real_B)\n",
    "\n",
    "            save_image(fake_B, f'test_results/fake_B_{i}.png', normalize=True)\n",
    "            save_image(fake_A, f'test_results/fake_A_{i}.png', normalize=True)\n",
    "    print(\"Testing completed. Generated images are saved in 'test_results' folder.\")\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bec2b9",
   "metadata": {
    "id": "d5bec2b9"
   },
   "source": [
    "# residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333dc34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7333dc34",
    "outputId": "859663f2-40a7-4109-986c-34b551b54293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Collecting fastapi<1.0 (from gradio)\n",
      "  Downloading fastapi-0.115.2-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.0 (from gradio)\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub>=0.25.1 (from gradio)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
      "Collecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n",
      "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio)\n",
      "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Collecting starlette<0.41.0,>=0.37.2 (from fastapi<1.0->gradio)\n",
      "  Downloading starlette-0.40.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.1.0-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.2-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m436.6/436.6 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
      "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.40.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, gradio-client, gradio\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.1\n",
      "    Uninstalling MarkupSafe-3.0.1:\n",
      "      Successfully uninstalled MarkupSafe-3.0.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.24.7\n",
      "    Uninstalling huggingface-hub-0.24.7:\n",
      "      Successfully uninstalled huggingface-hub-0.24.7\n",
      "Successfully installed aiofiles-23.2.1 fastapi-0.115.2 ffmpy-0.4.0 gradio-5.1.0 gradio-client-1.4.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.25.2 markupsafe-2.1.5 orjson-3.10.7 pydub-0.25.1 python-multipart-0.0.12 ruff-0.6.9 semantic-version-2.10.0 starlette-0.40.0 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W_7x1XtDzJq9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "W_7x1XtDzJq9",
    "outputId": "e221f15c-78fa-4934-c655-2decad82122e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://bd87ae122d0e959083.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://bd87ae122d0e959083.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from google.colab import drive\n",
    "from PIL import Image\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Path to save images\n",
    "drive_folder = '/content/drive/MyDrive/person-face-sketches/test'\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(drive_folder):\n",
    "    os.makedirs(drive_folder)\n",
    "\n",
    "# Function to save the uploaded image\n",
    "def save_image(image):\n",
    "    if image:\n",
    "        img = Image.fromarray(image)\n",
    "        file_path = os.path.join(drive_folder, 'uploaded_image.png')\n",
    "        img.save(file_path)\n",
    "        return f\"Image saved to {file_path}\"\n",
    "    return \"No image received.\"\n",
    "\n",
    "# Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=save_image,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload or Take Photo\"),\n",
    "    outputs=\"text\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0u2JwP564WHo",
   "metadata": {
    "id": "0u2JwP564WHo"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4NdraM4z4Pm4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NdraM4z4Pm4",
    "outputId": "7cf2ae24-bced-407e-b256-b22f927a0ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 170498071/170498071 [00:04<00:00, 42478080.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "z_dim = 100\n",
    "img_size = 32\n",
    "channels_img = 3\n",
    "num_epochs = 15\n",
    "\n",
    "# Load CIFAR-10 dataset and filter only 'cat' (class 3) and 'dog' (class 5)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "indices = [i for i, label in enumerate(cifar10.targets) if label in [3, 5]]  # Cats (3), Dogs (5)\n",
    "subset_cifar10 = Subset(cifar10, indices)\n",
    "\n",
    "dataloader = DataLoader(subset_cifar10, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "C1ayyaVh4Zhp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1ayyaVh4Zhp",
    "outputId": "12578df0-b25a-4414-a331-9915feef5b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(z_dim, 512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, img_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels, stride=2):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            conv_block(img_channels, 64, stride=2),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 256, stride=2),\n",
    "            conv_block(256, 512, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2 * 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        feat1 = self.feature_extractor(img1)\n",
    "        feat2 = self.feature_extractor(img2)\n",
    "        combined = torch.cat((feat1, feat2), dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen = Generator(z_dim, channels_img).to(device)\n",
    "disc = Discriminator(channels_img).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "CGAG21jL6q-r",
   "metadata": {
    "id": "CGAG21jL6q-r"
   },
   "outputs": [],
   "source": [
    "# Loss function (Binary Cross-Entropy)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "FI1Vw-5_6-P6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FI1Vw-5_6-P6",
    "outputId": "be19a5f9-06c5-42df-bd8a-02593ab7def8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/15], Batch [0/79]\n",
      "  Loss D: 0.7192784547805786, Loss G: 2.7665815353393555\n",
      "  Real Similarity Score: 0.5680, Fake Similarity Score: 0.0679\n",
      "Epoch [2/15], Batch [0/79]\n",
      "  Loss D: 0.007846756838262081, Loss G: 7.438920974731445\n",
      "  Real Similarity Score: 0.9852, Fake Similarity Score: 0.0008\n",
      "Epoch [3/15], Batch [0/79]\n",
      "  Loss D: 0.1496402621269226, Loss G: 4.956753253936768\n",
      "  Real Similarity Score: 0.9857, Fake Similarity Score: 0.0331\n",
      "Epoch [4/15], Batch [0/79]\n",
      "  Loss D: 0.21915686130523682, Loss G: 3.6877284049987793\n",
      "  Real Similarity Score: 0.8542, Fake Similarity Score: 0.0481\n",
      "Epoch [5/15], Batch [0/79]\n",
      "  Loss D: 0.8029453754425049, Loss G: 4.265120983123779\n",
      "  Real Similarity Score: 0.7195, Fake Similarity Score: 0.0620\n",
      "Epoch [6/15], Batch [0/79]\n",
      "  Loss D: 0.26160210371017456, Loss G: 4.040586948394775\n",
      "  Real Similarity Score: 0.8261, Fake Similarity Score: 0.0679\n",
      "Epoch [7/15], Batch [0/79]\n",
      "  Loss D: 0.12147576361894608, Loss G: 4.604707717895508\n",
      "  Real Similarity Score: 0.8393, Fake Similarity Score: 0.0488\n",
      "Epoch [8/15], Batch [0/79]\n",
      "  Loss D: 0.2712901830673218, Loss G: 5.977598190307617\n",
      "  Real Similarity Score: 0.9560, Fake Similarity Score: 0.0125\n",
      "Epoch [9/15], Batch [0/79]\n",
      "  Loss D: 0.11854033172130585, Loss G: 6.168966293334961\n",
      "  Real Similarity Score: 0.8348, Fake Similarity Score: 0.0092\n",
      "Epoch [10/15], Batch [0/79]\n",
      "  Loss D: 0.16680407524108887, Loss G: 5.887812614440918\n",
      "  Real Similarity Score: 0.8226, Fake Similarity Score: 0.0168\n",
      "Epoch [11/15], Batch [0/79]\n",
      "  Loss D: 0.3696248531341553, Loss G: 5.838634014129639\n",
      "  Real Similarity Score: 0.8001, Fake Similarity Score: 0.0297\n",
      "Epoch [12/15], Batch [0/79]\n",
      "  Loss D: 0.04780542105436325, Loss G: 6.31052303314209\n",
      "  Real Similarity Score: 0.9419, Fake Similarity Score: 0.0050\n",
      "Epoch [13/15], Batch [0/79]\n",
      "  Loss D: 0.04633597284555435, Loss G: 7.935952186584473\n",
      "  Real Similarity Score: 0.9138, Fake Similarity Score: 0.0015\n",
      "Epoch [14/15], Batch [0/79]\n",
      "  Loss D: 0.04053880274295807, Loss G: 7.950383186340332\n",
      "  Real Similarity Score: 0.9269, Fake Similarity Score: 0.0051\n",
      "Epoch [15/15], Batch [0/79]\n",
      "  Loss D: 0.1468873918056488, Loss G: 6.591805458068848\n",
      "  Real Similarity Score: 0.8907, Fake Similarity Score: 0.0091\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train():\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (real_imgs, _) in enumerate(dataloader):\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            batch_size = real_imgs.size(0)\n",
    "\n",
    "            # Labels for real and fake\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "            fake_imgs = gen(z)\n",
    "\n",
    "            real_imgs_sampled = real_imgs[:batch_size]  # Subset of real images for comparison\n",
    "            fake_imgs_sampled = fake_imgs[:batch_size]  # Subset of generated images\n",
    "\n",
    "            real_similarity = disc(real_imgs_sampled, real_imgs_sampled)\n",
    "            fake_similarity = disc(fake_imgs_sampled, real_imgs_sampled)\n",
    "\n",
    "            loss_disc_real = criterion(real_similarity, real_labels)\n",
    "            loss_disc_fake = criterion(fake_similarity, fake_labels)\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "\n",
    "            optimizer_disc.zero_grad()\n",
    "            loss_disc.backward()\n",
    "            optimizer_disc.step()\n",
    "\n",
    "            # Train Generator\n",
    "            z = torch.randn(batch_size, z_dim, 1, 1).to(device)\n",
    "            fake_imgs = gen(z)\n",
    "            fake_similarity = disc(fake_imgs, real_imgs_sampled)\n",
    "            loss_gen = criterion(fake_similarity, real_labels)  # We want fake to be similar to real\n",
    "\n",
    "            optimizer_gen.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            # Print losses and similarity scores\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(dataloader)}]\")\n",
    "                print(f\"  Loss D: {loss_disc.item()}, Loss G: {loss_gen.item()}\")\n",
    "                print(f\"  Real Similarity Score: {real_similarity.mean().item():.4f}, Fake Similarity Score: {fake_similarity.mean().item():.4f}\")\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkjgs0hU_jdr",
   "metadata": {
    "id": "mkjgs0hU_jdr"
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "E34qgtxy_k3t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E34qgtxy_k3t",
    "outputId": "97b634ea-da9a-4deb-eb6e-4386451c5eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/5], Batch [0/63], Loss_D: 0.7138, Loss_G: 0.7126\n",
      "Epoch [2/5], Batch [0/63], Loss_D: 0.6436, Loss_G: 0.9134\n",
      "Epoch [3/5], Batch [0/63], Loss_D: 0.5922, Loss_G: 0.9112\n",
      "Epoch [4/5], Batch [0/63], Loss_D: 0.5229, Loss_G: 0.9925\n",
      "Epoch [5/5], Batch [0/63], Loss_D: 0.5257, Loss_G: 0.9960\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Define the paths\n",
    "train_folder = 'drive/MyDrive/person-face-sketches/train'\n",
    "train_sketches_folder = os.path.join(train_folder, 'sketches')\n",
    "train_photos_folder = os.path.join(train_folder, 'photos')\n",
    "\n",
    "# Create sample output directory\n",
    "sample_folder = 'samples'\n",
    "os.makedirs(sample_folder, exist_ok=True)\n",
    "\n",
    "# Custom Dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, sketches_dir, photos_dir, transform=None, max_images=500):\n",
    "        self.sketches_dir = Path(sketches_dir)\n",
    "        self.photos_dir = Path(photos_dir)\n",
    "        self.sketches = sorted(os.listdir(sketches_dir))[:max_images]\n",
    "        self.photos = sorted(os.listdir(photos_dir))[:max_images]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.sketches), len(self.photos))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch_path = self.sketches_dir / self.sketches[idx % len(self.sketches)]\n",
    "        photo_path = self.photos_dir / self.photos[idx % len(self.photos)]\n",
    "        sketch = Image.open(sketch_path).convert('RGB')\n",
    "        photo = Image.open(photo_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            sketch = self.transform(sketch)\n",
    "            photo = self.transform(photo)\n",
    "\n",
    "        return {'sketch': sketch, 'photo': photo}\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "batch_size = 8  # Increased batch size for faster training\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_dataset = ImageDataset(train_sketches_folder, train_photos_folder, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x_cond = torch.cat((x, condition), 1)\n",
    "        return self.main(x_cond)\n",
    "\n",
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=1)  # No sigmoid here\n",
    "        )\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        x_cond = torch.cat((x, condition), 1)\n",
    "        return self.main(x_cond)\n",
    "\n",
    "# Initialize models\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for stable gradients\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        real_sketch = batch['sketch'].to(device)\n",
    "        real_photo = batch['photo'].to(device)\n",
    "\n",
    "        # Forward pass through D to determine the size of the output\n",
    "        output_size = D(real_photo, real_sketch).size()\n",
    "\n",
    "        # Create labels to match the output size\n",
    "        real_labels = torch.ones(output_size, device=device)\n",
    "        fake_labels = torch.zeros(output_size, device=device)\n",
    "\n",
    "        # Train Discriminator with real data\n",
    "        D.zero_grad()\n",
    "        output_real = D(real_photo, real_sketch)\n",
    "        loss_D_real = criterion(output_real, real_labels)\n",
    "\n",
    "        # Train Discriminator with fake data\n",
    "        fake_photo = G(real_sketch, real_sketch)\n",
    "        output_fake = D(fake_photo.detach(), real_sketch)\n",
    "        loss_D_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        # Total Discriminator loss\n",
    "        loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        G.zero_grad()\n",
    "        output_fake = D(fake_photo, real_sketch)\n",
    "        loss_G = criterion(output_fake, real_labels)  # Generator wants output to be classified as real\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{n_epochs}], Batch [{i}/{len(train_loader)}], \"\n",
    "                  f\"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # Save sample images after each epoch\n",
    "    with torch.no_grad():\n",
    "        fake_sample = G(real_sketch, real_sketch)\n",
    "        save_image(fake_sample, f'{sample_folder}/fake_epoch_{epoch+1}.png', normalize=True)\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928UP7vyZn6f",
   "metadata": {
    "id": "928UP7vyZn6f"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dP2DI5tBI19",
   "metadata": {
    "id": "5dP2DI5tBI19"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir = r\"Signatures\"\n",
    "IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS = 64, 64, 1\n",
    "\n",
    "def load_images(data_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for folder in os.listdir(data_dir):\n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT)).astype(\"float32\") / 255.0\n",
    "                img = np.expand_dims(img, axis=-1)\n",
    "                images.append(img)\n",
    "                labels.append(folder)\n",
    "    return np.array(images)\n",
    "\n",
    "images = load_images(data_dir)\n",
    "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1, shear_range=0.1, fill_mode='nearest')\n",
    "datagen.fit(images)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_sample_images(images, num_samples=8):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_samples):\n",
    "        ax = plt.subplot(2, 4, i + 1)\n",
    "        plt.imshow(images[i].reshape(IMG_WIDTH, IMG_HEIGHT), cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(images)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, backend as K\n",
    "\n",
    "LATENT_DIM = 16\n",
    "\n",
    "def build_encoder(input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS), latent_dim=LATENT_DIM):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], K.int_shape(z_mean)[1]))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "    return Model(inputs, [z_mean, z_log_var, z])\n",
    "\n",
    "def build_decoder(latent_dim=LATENT_DIM):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(16 * 16 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((16, 16, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    outputs = layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    return Model(latent_inputs, outputs)\n",
    "\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder, beta=0.001):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def vae_loss(self, inputs, outputs):\n",
    "        reconstruction_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(inputs, outputs))\n",
    "        z_mean, z_log_var, _ = self.encoder(inputs)\n",
    "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return tf.reduce_mean(reconstruction_loss + self.beta * kl_loss)\n",
    "\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae.vae_loss)\n",
    "BATCH_SIZE, EPOCHS = 32, 10\n",
    "\n",
    "def create_tf_dataset(images, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, images)).shuffle(len(images)).batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_tf_dataset(images, BATCH_SIZE)\n",
    "steps_per_epoch = len(images) // BATCH_SIZE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "vae.compile(optimizer=optimizer, loss=vae.vae_loss)\n",
    "\n",
    "vae.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "encoder.save(\"vae_encoder.h5\")\n",
    "decoder.save(\"vae_decoder.h5\")\n",
    "vae.save(\"vae_model.h5\")\n",
    "\n",
    "def train_gan(generator, discriminator, gan, dataset, latent_dim, epochs=10000, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "        generated_images = generator.predict(noise)\n",
    "        idx = np.random.randint(0, dataset.shape[0], batch_size)\n",
    "        real_images = dataset[idx]\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "        d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        g_loss = gan.train_on_batch(noise, real_labels)\n",
    "\n",
    "signatures_dataset = load_images(data_dir).astype('float32') / 255.0\n",
    "train_gan(generator, discriminator, gan, signatures_dataset, latent_dim=100, epochs=150, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PBv9x7EWZhxF",
   "metadata": {
    "id": "PBv9x7EWZhxF"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(16 * 16 * 64, activation=\"relu\", input_dim=latent_dim),\n",
    "        layers.Reshape((16, 16, 64)),\n",
    "        layers.Conv2DTranspose(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\"),\n",
    "        layers.Conv2DTranspose(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\"),\n",
    "        layers.Conv2DTranspose(1, (3, 3), activation=\"sigmoid\", padding=\"same\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", strides=2, padding=\"same\", input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\", strides=2, padding=\"same\"),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = layers.Input(shape=(100,))\n",
    "    generated_image = generator(gan_input)\n",
    "    gan_output = discriminator(generated_image)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    return gan\n",
    "\n",
    "# Build generator and discriminator\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5),\n",
    "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Build and compile GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "\n",
    "# Train GAN\n",
    "train_gan(generator, discriminator, gan, signatures_dataset, latent_dim=latent_dim, epochs=150, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r2MJ6_a1aVKo",
   "metadata": {
    "id": "r2MJ6_a1aVKo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
